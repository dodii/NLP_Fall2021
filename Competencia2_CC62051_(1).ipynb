{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Competencia2_CC62051.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6u3S2l-NIvKT",
        "S0Sgpfl_lgMk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0tyIsliieNr"
      },
      "source": [
        "# **Competencia 2 - CC6205 Natural Language Processing üìö**\n",
        "\n",
        "**Integrantes:** Vicente Ardiles, Rodrigo Oportot\n",
        "\n",
        "**Usuario del equipo en CodaLab:** NLPachi\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** 12 de Julio.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:** 40 horas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQlYlmGaSFH"
      },
      "source": [
        "## Introducci√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVL0-01AUOzL"
      },
      "source": [
        "La competencia tiene como objetivo resolver una task de NER sobre un corpus perteneciente a la lista de espera NO GES en Chile. La task en cuesti√≥n se aborda como un problema de Sequence Labeling. Tal como se explica en el enunciado de la competencia, se tiene lo siguiente:\n",
        "\n",
        "- Dada una secuencia de tokens (oraci√≥n), sequence labeling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia. En pocas palabras, dada una lista de tokens, se espera encontrar la mejor secuencia de etiquetas asociadas a esa lista.\n",
        "\n",
        "NER es un ejemplo de un problema de Sequence Labeling. Primero es necesario definir los siguientes conceptos, antes de formalizar la task en cuesti√≥n:\n",
        "\n",
        "- *Token*: secuencia de caracteres, pudiendo ser una palabra, un n√∫mero o un s√≠mbolo.\n",
        "\n",
        "- *Entidad*: un trozo de texto (uno o m√°s tokens) asociado a una categor√≠a predefinida.\n",
        "\n",
        "- *L√≠mites de una entidad*: √≠ndices de inicio y f√≠n de los tokens dentro de una entidad.\n",
        "\n",
        "- *Tipo de entidad*: categor√≠a predefinida asociada a la entidad.\n",
        "\n",
        "Ahora se puede definir formalmente una entidad como una tupla: $(s, e, t)$, donde $s, e$ son los l√≠mites de la entidad (√≠ndices de los tokens de inicio y fin, respectivamente) y e corresponde al tipo de entidad o categor√≠a.\n",
        "\n",
        "El corpus est√° constituido originalmente por 7 tipos de entidades, pero por simplicidad de la competencia se trabajar√° con las siguientes:\n",
        "\n",
        "- **Disease**\n",
        "- **Body_Part**\n",
        "- **Medication** \n",
        "- **Procedures** \n",
        "- **Family_Member**\n",
        "\n",
        "La formalizaci√≥n del problema se construye tal como sigue:\n",
        "\n",
        "**Formato ConLL**\n",
        "\n",
        "Los archivos vienen en un formato est√°ndar llamado ConLL. Es un archivo de texto que cumple las siguientes propiedades:\n",
        "\n",
        "- Un salto de linea corresponde a la separaci√≥n entre oraciones. Permite pasar una lista de oraciones como batches a las redes neuronales.\n",
        "\n",
        "- La primera columna del archivo contiene todos los tokens de la partici√≥n.\n",
        "\n",
        "- La segunda columna del archivo contiene el tipo de entidad asociado al token de la primera columna.\n",
        "\n",
        "- Los tipos de entidades siguen un formato denominado *IOB2*. Si un tipo de entidad comienza con el prefijo \"B-\" (Beginning) significa que es el token de inicio de una entidad, si comienza con \"I-\" (Inside) es un token distinto al de inicio y si un token est√° asociado a la categor√≠a O (Outside) significa que no pertenece a ninguna entidad.\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "```\n",
        "PACIENTE O\n",
        "PRESENTA O\n",
        "FRACTURA B-Disease\n",
        "CORONARIA I-Disease\n",
        "COMPLICADA I-Disease\n",
        "EN O\n",
        "PIE B-Body_Part\n",
        "IZQUIERDO I-Body_Part\n",
        ". O\n",
        "SE O\n",
        "REALIZA O\n",
        "INSTRUMENTACION B-Procedure\n",
        "INTRACONDUCTO I-Procedure\n",
        ". O\n",
        "```\n",
        "\n",
        "Por la definici√≥n se tienen las siguientes tres entidades (enumerando desde 0): \n",
        "\n",
        "- $(2, 4, Disease)$\n",
        "- $(6, 7, Body Part)$\n",
        "- $(11, 12, Procedure)$\n",
        "\n",
        "El rendimiento de los modelos es sometido bajo **m√©tricas estrictas**. Solamente se considera correcta una predicci√≥n si, al compararla con las entidades reales, **coinciden tanto los l√≠mites de la entidad como el tipo.** \n",
        "\n",
        "Tomando el caso anterior, si un modelo es capaz de encontrar la siguiente entidad: $(2, 3, Disease)$, entonces se considera incorrecto ya que pudo predecir dos de los tres tokens de dicha enfermedad. Se busca una m√©trica que sea alta a nivel de entidad y no a nivel de token.\n",
        "\n",
        "Se vislumbra como desaf√≠o el dise√±o de redes neuronales capaces de solucionar la task correctamente. La informaci√≥n es de car√°cter nacional, lo que la hace a√∫n m√°s interesante. El formato del corpus exige predecir a nivel de entidad y no de token, tambi√©n aumentando la dificultad para resolver la task correctamente. Por √∫ltimo, queda se√±alar que la naturaleza de los algoritmos a emplear, al ser no supervisada (Deep Learning), dificulta un poco estas primeras etapas de aprendizaje para los alumnos, por su grado de complejidad. No obstante, se espera aprender mucho al haber finalizado la competencia, acumulando experiencia para los siguientes cursos que los alumnos puedan tomar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbA1EmhCaSFI"
      },
      "source": [
        "## Modelos \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HsvlfPJUSId"
      },
      "source": [
        "En primera instancia, a los siguientes modelos se les fueron alterando los siguientes hiperpar√°metros (en caso de poseerlos) para realizar los experimentos:\n",
        "\n",
        "- INPUT_DIM, tama√±o del vocabulario.\n",
        "- EMBEDDING_WEIGHT, embedding pre entrenados.\n",
        "- EMBEDDING_DIM, dimensi√≥n de los embeddings.\n",
        "- HIDDEN_DIM = 128, dimensi√≥n de la capas.\n",
        "- OUTPUT_DIM, n√∫mero de clases.\n",
        "- N_LAYERS = 3, n√∫mero de capas.\n",
        "- DROPOUT = 0.2, valor del dropout (probabilidad de remover neuronas en las capas para evitar que se co-adapten entre ellas).\n",
        "- BIDIRECTIONAL, booleano para activar bidireccionalidad.\n",
        "- n_epochs, itereaciones para el entrenamiento.\n",
        "\n",
        "Como funci√≥n de loss, se mantuvo constante Cross Entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKTQFSL7tZ06"
      },
      "source": [
        "### Modelo 1: LSTM Bidireccional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYhZ6j9MtcS1"
      },
      "source": [
        "Este modelo es similar al de la baseline, pero se activ√≥ el par√°metro bidireccional. M√°s adelante se fueron ajustado algunos hiperpar√°metros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aEJMvQHtfHP"
      },
      "source": [
        "### Modelo 2: LSTM + Bidireccional + Embedding Clinicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8_w-TQZtgoF"
      },
      "source": [
        "Este modelo es similar al de la baseline, pero se activ√≥ el par√°metro bidireccional. Tambi√©n se le fueron ajustado algunos hiperpar√°metros m√°s adelante. El punto importante fue la adici√≥n de embeddings cl√≠nicos pre-entrenados de la lista de espera chilena (obtenido de https://zenodo.org/record/3924799) a la arquitectura de la RNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_x99c2RtjDh"
      },
      "source": [
        "### Modelo 3: GRU + Bidireccional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5RuHUGctkp0"
      },
      "source": [
        "GRU bidireccional. Como nota aparte, en las primeras instancias del trabajo, fue aquel que entreg√≥ mejores resultados preliminares, por lo que se prefiri√≥ por sobre los LSTM anteriormente mencionados en gran parte de los experimentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hJNaDzwtmQV"
      },
      "source": [
        "### Modelo 4: GRU + Bidireccional + Embedding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB2I5RJ9tnuz"
      },
      "source": [
        "Similar al modelo 3, pero con la adici√≥n de distintos embeddings pre-entrenados:\n",
        "\n",
        " - Los cl√≠nicos de https://zenodo.org/record/3924799 ya mencionados.\n",
        " - FastText embeddings de SUC.\n",
        " - GloveEmbeddings de SBWC.\n",
        " - FastText embeddings SBWC.\n",
        "\n",
        "Se a√±adieron m√°s embeddings en comparaci√≥n al modelo 2 por lo mencionado en el modelo 3. Todos fueron obtenidos de https://github.com/dccuchile/spanish-word-embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVhZ5iaaSFK"
      },
      "source": [
        "## M√©tricas de evaluaci√≥n\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl3GaVMUYA7"
      },
      "source": [
        "- **M√©trica estricta:** Para que una entidad se considere predicha exitosamente por un modelo, deben reconocerse correctamente los l√≠mites y el tipo de entidad en conjunto. Si al menos uno de esos dos aspectos es err√≥neo, toda la entidad estar√° incorrectamente catalogada.\n",
        "- **Precision:** Esta m√©trica indica qu√© tan v√°lidos son los resultados extra√≠dos. Es la fracci√≥n de resultados de una categor√≠a **que realmente lo son** sobre resultados seleccionados en dicha categor√≠a por el algoritmo: $\\frac{verdaderos \\ positivos}{elementos \\ seleccionados \\ como \\ positivos}$. \n",
        "- **Recall:** Esta m√©trica indica qu√© tan completos son los resultados extra√≠dos. De manera sencilla, ser√≠a: $\\frac{verdaderos \\ positivos}{todos \\ los \\ positivos}$.\n",
        "- **Micro F1 score:** Es el promedio arm√≥nico entre precision y recall, por lo tanto: $\\frac{2}{recall^{-1} \\ * \\ precision^{-1}} = \\frac{verdadero \\ positivo}{verdadero \\ positivo \\ + \\ 1/2 \\ * \\ (falso \\ positivo \\ + \\ falso \\ negativo)}$.\n",
        "\n",
        "    Macro F1 computa las m√©tricas de F1 para todas las clases de manera independiento para luego promediarlas , mientras que micro F1 suma las contribuciones de todas las clases (los verdaderos positivos) para calcular dicho promedio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27WffRVUj4v"
      },
      "source": [
        "## Dise√±o experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwfIfQpT8-A6"
      },
      "source": [
        "### Experimentos preliminares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O228DbeUmE7"
      },
      "source": [
        "En un principio, se compararon los tres primeros modelos propuestos con los par√°metros predeterminados (baseline), observando que el modelo 3 arrojaba mejores resultados de manera notoria en comparaci√≥n a los otros dos modelos, haciendo referencia a las m√©tricas estrictas como criterios de evaluaci√≥n. Por esta raz√≥n, se prefiri√≥ continuar con √©l. A partir de esto, surgi√≥ el modelo 4, explicado m√°s adelante. \n",
        "\n",
        "Cabe se√±alar que el modelo 2, que adoptaba embeddings pre-entrenados,  daba resultados inferiores al modelo 1. Tambi√©n que casi la totalidad de los experimentos incluyeron early stopping para prevenir modelos overfitteados. Esto ser√° analizado m√°s adelante en los resultados y conclusiones.\n",
        "\n",
        "En relaci√≥n a porqu√© no se opt√≥ por refinar los primeros dos modelos para continuar compar√°ndolos, esto es a causa de no comprender en su totalidad la injerencia de los diversos par√°metros e hiperpar√°metros de los modelos de redes neuronales. Si bien se pueden estudiar sus definiciones, conocer a fondo la totalidad de sus repercusiones en su procesamiento al ser estos alterados a√∫n escapa de las habilidades de los estudiantes. Se fueron modificando y comparando los par√°metros de manera muy sencilla, con el fin de aprender lo mejor posible, yendo en desmedro de complejizar las estructuras encargadas de resolver las tareas. Si bien esto puede resultar cuestionable, permiti√≥ acercarse de manera m√°s did√°ctica a la competencia. Esto se pudo notar al superar los resultados de la baseline y aprender a identificar un modelo overfitteado, problema que el grupo pudo sobrellevar de mejor manera al mirar retrospectivamente el trabajo de la competencia 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bt5opE6t2El"
      },
      "source": [
        "### Experimentos principales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7vTO7fyt3aj"
      },
      "source": [
        "Luego de lo anterior, el resto de los experimentos involucraron al modelo 3 y 4, en donde se variaron una diversidad de par√°metros y optimizadores (Adam, AdamW, Adafactor, etc) para realizarlos. Respecto a esto √∫ltimo, finalmente se le dio prioridad a Adam, dado que arroj√≥ mejores resultados que el resto. Resumiendo este apartado, se observ√≥ que el modelo 3 era superior al modelo 4, tanto en loss como m√©tricas estrictas. Por esta raz√≥n, se opt√≥ por continuar con √©ste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PShipID2tvlu"
      },
      "source": [
        "### Experimentos finales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tLvtNtItyWW"
      },
      "source": [
        "Finalizando los dise√±os, se busc√≥ refinar lo m√°s posible el modelo 3, ajustando sus hiperpar√°metros y aspectos asociados de manera exhaustiva. Esto es explicado en la secci√≥n de resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFM-wNt8aSFM"
      },
      "source": [
        "## Experimentos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMgKjfYC_Go-"
      },
      "source": [
        "###  Carga de datos y Preprocesamiento\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librer√≠a [`torchtext`](https://github.com/pytorch/text).\n",
        "En particular usaremos su m√≥dulo `data`, el cual seg√∫n su documentaci√≥n original provee: \n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso ser√° el siguiente: \n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27csY87GaSFO",
        "scrolled": false,
        "outputId": "035674ed-bde0-4aa7-d722-d13a129b97cf"
      },
      "source": [
        "# Instalamos torchtext que nos facilitar√° la vida en el pre-procesamiento del formato ConLL.\n",
        "!pip3 install --upgrade torchtext"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torchtext in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng7wRGEyawjM"
      },
      "source": [
        "import torch\n",
        "from torchtext import data, datasets, legacy\n",
        "\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehSou6rCvwg"
      },
      "source": [
        "#### Obtener datos\n",
        "\n",
        "Descargamos los datos de entrenamiento, validaci√≥n y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbT0g_kC18Jb",
        "outputId": "a0938b54-4671-4c62-a7d5-e29075a879bf"
      },
      "source": [
        "#%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de Validaci√≥n (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¬°¬°SON LOS QUE DEBEN SER PREDICHOS!!"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-13 02:38:29--  https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=a51666ea45f4c403f7b515ef76d51e07d423d671239e052b1da4a189fcaf60af&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-13 02:38:29--  https://github-releases.githubusercontent.com/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=a51666ea45f4c403f7b515ef76d51e07d423d671239e052b1da4a189fcaf60af&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.109.154, 185.199.108.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1607913 (1.5M) [application/octet-stream]\n",
            "Saving to: ‚Äòtrain.txt‚Äô\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>]   1.53M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-13 02:38:29 (57.8 MB/s) - ‚Äòtrain.txt‚Äô saved [1607913/1607913]\n",
            "\n",
            "--2021-07-13 02:38:29--  https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=e37b4bc2483809c26a280d061f6c6925e700fe4395db758c52963da78f2be66b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-13 02:38:29--  https://github-releases.githubusercontent.com/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=e37b4bc2483809c26a280d061f6c6925e700fe4395db758c52963da78f2be66b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.109.154, 185.199.108.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177166 (173K) [application/octet-stream]\n",
            "Saving to: ‚Äòdev.txt‚Äô\n",
            "\n",
            "dev.txt             100%[===================>] 173.01K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-07-13 02:38:29 (44.3 MB/s) - ‚Äòdev.txt‚Äô saved [177166/177166]\n",
            "\n",
            "--2021-07-13 02:38:29--  https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=babfa22b93c05ded61a27daf49715110c664d5182780f591b7a44372033cd4c5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-13 02:38:29--  https://github-releases.githubusercontent.com/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=babfa22b93c05ded61a27daf49715110c664d5182780f591b7a44372033cd4c5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.109.154, 185.199.108.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147052 (144K) [application/octet-stream]\n",
            "Saving to: ‚Äòtest.txt‚Äô\n",
            "\n",
            "test.txt            100%[===================>] 143.61K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-07-13 02:38:29 (31.2 MB/s) - ‚Äòtest.txt‚Äô saved [147052/147052]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKg0h8A1nire"
      },
      "source": [
        "#### Obtener embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8_m6HBJoW7r"
      },
      "source": [
        "import csv\n",
        "import gzip\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import requests"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqj5N_jjQZdq"
      },
      "source": [
        "Embeddings clinicos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKEQH5PDnetF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d58bd8-5819-4eaf-ecf9-409b17318e5d"
      },
      "source": [
        "VEC_FILE = \"cwlce.vec\"\n",
        "\n",
        "# Descargar embeddings de aca\n",
        "# https://zenodo.org/record/3924799/files/cwlce.vec?download=1\n",
        "if not os.path.exists(VEC_FILE):\n",
        "    print(f\"Descargando {VEC_FILE}\")\n",
        "    url = \"https://zenodo.org/record/3924799/files/cwlce.vec?download=1\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    try:\n",
        "        open(VEC_FILE, \"wb\").write(response.content)\n",
        "    except Exception as e:\n",
        "        os.remove(VEC_FILE)\n",
        "        raise e"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Descargando cwlce.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VshE7y-dp8RG"
      },
      "source": [
        "Los siguientes embeddings son bastante pesados asi que estos fueron ejecutados en la medida que se requerian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp25sARIX3R-"
      },
      "source": [
        "Spanish Unannotated Corpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOpY9-pwXTQP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "bfa7f0e9-7506-42c5-c370-47c9a1a3d358"
      },
      "source": [
        "'''\n",
        "VEC2_FILE = \"em1.vec\"\n",
        "\n",
        "# Descargar embeddings de aca\n",
        "# https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1\n",
        "if not os.path.exists(VEC2_FILE):\n",
        "    print(f\"Descargando {VEC2_FILE}\")\n",
        "    url = \"https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    try:\n",
        "        open(VEC2_FILE, \"wb\").write(response.content)\n",
        "    except Exception as e:\n",
        "        os.remove(VEC2_FILE)\n",
        "        raise e\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nVEC2_FILE = \"em1.vec\"\\n\\n# Descargar embeddings de aca\\n# https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1\\nif not os.path.exists(VEC2_FILE):\\n    print(f\"Descargando {VEC2_FILE}\")\\n    url = \"https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1\"\\n    response = requests.get(url, stream=True)\\n    try:\\n        open(VEC2_FILE, \"wb\").write(response.content)\\n    except Exception as e:\\n        os.remove(VEC2_FILE)\\n        raise e\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJxPZB63ena7"
      },
      "source": [
        "FastText embeddings from SBWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRT-61Olenv_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "477c4059-5727-4d11-f2ba-66a8472e2469"
      },
      "source": [
        "'''\n",
        "FASTTEXT_FILE = \"fasttext300d.vec\"\n",
        "\n",
        "# Descargar vectores fasttext de aca\n",
        "# http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.vec.gz\n",
        "if not os.path.exists(FASTTEXT_FILE):\n",
        "    print(f\"Descargando {FASTTEXT_FILE}\")\n",
        "    url = \"http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.vec.gz\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    try:\n",
        "        with gzip.open(response.raw, \"rb\") as f_in:\n",
        "            with open(FASTTEXT_FILE, \"wb\") as f_out:\n",
        "                # Funcion util para copiar de un file-like object a otro\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "    except Exception as e:\n",
        "        os.remove(FASTTEXT_FILE)\n",
        "        raise e\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nFASTTEXT_FILE = \"fasttext300d.vec\"\\n\\n# Descargar vectores fasttext de aca\\n# http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.vec.gz\\nif not os.path.exists(FASTTEXT_FILE):\\n    print(f\"Descargando {FASTTEXT_FILE}\")\\n    url = \"http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.vec.gz\"\\n    response = requests.get(url, stream=True)\\n    try:\\n        with gzip.open(response.raw, \"rb\") as f_in:\\n            with open(FASTTEXT_FILE, \"wb\") as f_out:\\n                # Funcion util para copiar de un file-like object a otro\\n                shutil.copyfileobj(f_in, f_out)\\n    except Exception as e:\\n        os.remove(FASTTEXT_FILE)\\n        raise e\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy_o830ZfdVX"
      },
      "source": [
        "GloVe embeddings from SBWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAl_SMCkfdm3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "ba9e5cbd-f3ec-4cb9-e031-809b5f3059ad"
      },
      "source": [
        "'''\n",
        "GLOVE_FILE = \"glove300d.vec\"\n",
        "\n",
        "# Descargar vectores glove de aca\n",
        "# https://github.com/dccuchile/spanish-word-embeddings\n",
        "if not os.path.exists(GLOVE_FILE):\n",
        "    print(f\"Descargando {GLOVE_FILE}\")\n",
        "    url = \"http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i25.vec.gz\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    try:\n",
        "        with gzip.open(response.raw, \"rb\") as f_in:\n",
        "            with open(GLOVE_FILE, \"wb\") as f_out:\n",
        "                # Funcion util para copiar de un file-like object a otro\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "    except Exception as e:\n",
        "        os.remove(GLOVE_FILE)\n",
        "        raise e\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nGLOVE_FILE = \"glove300d.vec\"\\n\\n# Descargar vectores glove de aca\\n# https://github.com/dccuchile/spanish-word-embeddings\\nif not os.path.exists(GLOVE_FILE):\\n    print(f\"Descargando {GLOVE_FILE}\")\\n    url = \"http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i25.vec.gz\"\\n    response = requests.get(url, stream=True)\\n    try:\\n        with gzip.open(response.raw, \"rb\") as f_in:\\n            with open(GLOVE_FILE, \"wb\") as f_out:\\n                # Funcion util para copiar de un file-like object a otro\\n                shutil.copyfileobj(f_in, f_out)\\n    except Exception as e:\\n        os.remove(GLOVE_FILE)\\n        raise e\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMud7YGMBZvg"
      },
      "source": [
        "####  Fields\n",
        "\n",
        "Un `field`:\n",
        "\n",
        "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
        "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
        "* Contiene otros par√°metros relacionados con la forma en que se debe numericalizar un tipo de datos, como un m√©todo de tokenizaci√≥n y el tipo de Tensor que se debe producir.\n",
        "\n",
        "\n",
        "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
        "\n",
        "\n",
        "```\n",
        "El O\n",
        "paciente O\n",
        "padece O\n",
        "de O\n",
        "cancer B-Disease\n",
        "de I-Disease\n",
        "colon I-Disease\n",
        ". O\n",
        "```\n",
        "\n",
        "Cada linea contiene un token y el tipo de entidad asociado en el formato IOB2 ya explicado. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
        "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`tokens`) y los NER_TAGS (`categor√≠as`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DcM_IjgCdzz"
      },
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = legacy.data.Field(lower=False) \n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = legacy.data.Field(unk_token=None)\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxOIkuw8sPlK"
      },
      "source": [
        "Cargar embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yksa22JVsPAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46732d75-4a9c-4b42-8464-4c6b521408cf"
      },
      "source": [
        "from torchtext import vocab\n",
        "\n",
        "# Para cargar los vectores de embeddings (que son esencialmente un vocabulario\n",
        "# donde cada palabra tiene asociado un vector) pueden usar la clase vocab.Vectors\n",
        "\n",
        "clinic_embeddings = vocab.Vectors(VEC_FILE)\n",
        "\n",
        "# CARGAR LOS OTROS EMBEDDINGS\n",
        "\n",
        "#suc_embeddings = vocab.Vectors(VEC2_FILE)\n",
        "#fasttext_embeddings = vocab.Vectors(FASTTEXT_FILE)\n",
        "#glove_embeddings = vocab.Vectors(GLOVE_FILE)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/57112 [00:00<?, ?it/s]Skipping token b'57112' with 1-dimensional vector [b'300']; likely a header\n",
            " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 56773/57112 [00:05<00:00, 10396.60it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCKTJOdgC5eC"
      },
      "source": [
        "####  SequenceTaggingDataset\n",
        "\n",
        "`SequenceTaggingDataset` es una clase de torchtext dise√±ada para contener datasets de sequence labelling. \n",
        "Los ejemplos que se guarden en una instancia de estos ser√°n arreglos de palabras pareados con sus respectivos tags.\n",
        "Por ejemplo, para Part-of-speech tagging:\n",
        "\n",
        "[I, love, PyTorch, .] estar√° pareado con [PRON, VERB, PROPN, PUNCT]\n",
        "\n",
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase c√≥mo cargar los datasets de prueba, validaci√≥n y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsHdGml62J21"
      },
      "source": [
        "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train.txt\",\n",
        "    validation=\"dev.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu7q3HCliia5",
        "outputId": "0ef131e2-8914-4617-f9f0-85f9e35e7da7"
      },
      "source": [
        "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"N√∫mero de ejemplos de validaci√≥n: {len(valid_data)}\")\n",
        "print(f\"N√∫mero de ejemplos de test (competencia): {len(test_data)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero de ejemplos de entrenamiento: 8025\n",
            "N√∫mero de ejemplos de validaci√≥n: 891\n",
            "N√∫mero de ejemplos de test (competencia): 992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDRnhXAdFGL-"
      },
      "source": [
        "Visualizemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T023Ld4RaSF4",
        "scrolled": false,
        "outputId": "e5e86272-0456-45cf-8df1-3ff1df7f210c"
      },
      "source": [
        "import random\n",
        "random_item_idx = random.randint(0, len(train_data))\n",
        "random_example = train_data.examples[random_item_idx]\n",
        "list(zip(random_example.text, random_example.nertags))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('solicito', 'O'),\n",
              " ('poder', 'O'),\n",
              " ('evaluar', 'O'),\n",
              " ('posibilidad', 'O'),\n",
              " ('de', 'O'),\n",
              " ('retratamiento', 'B-Procedure'),\n",
              " ('o', 'O'),\n",
              " ('de', 'O'),\n",
              " ('ser', 'O'),\n",
              " ('necesario', 'O'),\n",
              " ('realizar', 'O'),\n",
              " ('derivaci√≥n', 'O'),\n",
              " ('interna', 'O'),\n",
              " ('para', 'O'),\n",
              " ('cirug√≠a', 'B-Procedure'),\n",
              " ('apical', 'I-Procedure')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l05KYy5FSUy"
      },
      "source": [
        "#### Construir los vocabularios para el texto y las etiquetas\n",
        "\n",
        "Los vocabularios son los obbjetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields.\n",
        "El siguiente paso consiste en construirlos. Para esto, hacemos uso del m√©todo `Field.build_vocab` sobre cada uno de nuestros `fields`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBhp7WICiibL"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4OgUKM_iibO",
        "scrolled": true,
        "outputId": "fb56c326-9f2a-4558-9c01-6ffe25f9a873"
      },
      "source": [
        "print(f\"Tokens √∫nicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens √∫nicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens √∫nicos en TEXT: 17591\n",
            "Tokens √∫nicos en NER_TAGS: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4FeyL9nFnId",
        "outputId": "193976bb-ed5f-4073-e349-2f99631b8eda"
      },
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "NER_TAGS.vocab.itos"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " 'O',\n",
              " 'I-Disease',\n",
              " 'B-Disease',\n",
              " 'I-Body_Part',\n",
              " 'B-Body_Part',\n",
              " 'B-Procedure',\n",
              " 'I-Procedure',\n",
              " 'B-Medication',\n",
              " 'B-Family_Member',\n",
              " 'I-Medication',\n",
              " 'I-Family_Member']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQDoUqSHFKj"
      },
      "source": [
        "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oraci√≥n.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5eSLm4diibR",
        "outputId": "9db294e5-28a1-44a9-a833-b7c6cb25a6dd"
      },
      "source": [
        "# Tokens mas frecuentes\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 7396),\n",
              " (',', 6821),\n",
              " ('-', 4985),\n",
              " ('de', 3811),\n",
              " ('DE', 3645),\n",
              " ('/', 2317),\n",
              " (':', 2209),\n",
              " ('con', 1484),\n",
              " ('y', 1439),\n",
              " ('APS', 1429)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDyNLMPz9duD"
      },
      "source": [
        "# Seteamos algunas variables que nos ser√°n de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrYvF3X0sjWL"
      },
      "source": [
        "#### Frecuencia de los Tags\n",
        "\n",
        "Visualizemos r√°pidamente las cantidades y frecuencias de cada tag:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXOsbJUiibh",
        "outputId": "61b7766a-a598-436f-f01d-33ec5752814a"
      },
      "source": [
        "def tag_percentage(tag_counts):\n",
        "    \n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "  \n",
        "    return tag_counts_percentages\n",
        "\n",
        "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tag Ocurrencia Porcentaje\n",
            "\n",
            "O\t101671\t68.1%\n",
            "I-Disease\t21629\t14.5%\n",
            "B-Disease\t8831\t 5.9%\n",
            "I-Body_Part\t6489\t 4.3%\n",
            "B-Body_Part\t3755\t 2.5%\n",
            "B-Procedure\t2891\t 1.9%\n",
            "I-Procedure\t2819\t 1.9%\n",
            "B-Medication\t784\t 0.5%\n",
            "B-Family_Member\t228\t 0.2%\n",
            "I-Medication\t116\t 0.1%\n",
            "I-Family_Member\t9\t 0.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4wPiydnaSGs"
      },
      "source": [
        "#### Configuramos pytorch y dividimos los datos.\n",
        "\n",
        "Importante: si tienes problemas con la ram de la gpu, disminuye el tama√±o de los batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB7cwLWpaSGs",
        "outputId": "31bd4a9b-1992-4d5f-94a9-562fee876e87"
      },
      "source": [
        "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que est√° disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test\n",
        "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B21E1eAFId16"
      },
      "source": [
        "#### M√©tricas de evaluaci√≥n\n",
        "\n",
        "Adem√°s, definiremos las m√©tricas que ser√°n usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `f1`.\n",
        "**Importante**: Noten que la evaluaci√≥n solo se hace para las Named Entities (sin contar 'O')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o63ov69_rX2T",
        "outputId": "ad3b71d5-6ae5-4cbe-d8ab-2ac85653c54f"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 20kB 24.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 30kB 29.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40kB 31.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16184 sha256=0c2c1a3b41321f3fecd2151b02d593c0849530f7c5d9e2024667803f8db31024\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mUOOLEWiicU"
      },
      "source": [
        "# Definimos las m√©tricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "import warnings\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "    # Obtenemos los indices distintos de 0.\n",
        "\n",
        "    # filtramos <pad> y O para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hod516H1aSG2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### Baseline Model\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendr√° una capa de embedding, unas cuantas LSTM y una capa de salida y usar√° dropout en el entrenamiento.\n",
        "\n",
        "Este constar√° de los siguientes pasos: \n",
        "\n",
        "1. Definir la clase que contendr√° la red.\n",
        "2. Definir los hiperpar√°metros e inicializar la red. \n",
        "3. Definir la √©poca de entrenamiento\n",
        "3. Definir la funci√≥n de loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYprx75dvhLQ"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.RNN(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #print(\"EMBED SIZE\", embedded.shape)\n",
        "        \n",
        "        outputs, (hidden) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #print(\"outputs SIZE\", outputs.shape)\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #print(\"predictions SIZE\", predictions.shape)\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### Hiperpar√°metros de la red\n",
        "\n",
        "Definimos los hiperpar√°metros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHdi3QdOaSG8"
      },
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado..."
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlF1DhJeaSHA"
      },
      "source": [
        "baseline_n_epochs = 10"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la funci√≥n de loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G_4k99_aSHG"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzQSO2lawiy7"
      },
      "source": [
        "### Arquitecturas Modelos propuestos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8z_cCr3w56A"
      },
      "source": [
        "#### 1- LSTM Embedding pre-entrenados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fDFBfEbwibp"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN_EB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim,\n",
        "                 embedding_weights, \n",
        "                 embedding_dim,\n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights.clone(),\n",
        "                                                      freeze=True,\n",
        "                                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.RNN(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #print(\"EMBED SIZE\", embedded.shape)\n",
        "        \n",
        "        outputs, (hidden) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #print(\"outputs SIZE\", outputs.shape)\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #print(\"predictions SIZE\", predictions.shape)\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdTgHOJvqfJH"
      },
      "source": [
        "#### 2 - GRU \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvjQzE4Tqeko"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definir la red\n",
        "class GRU_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa GRU\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #print(\"EMBED SIZE\", embedded.shape)\n",
        "        \n",
        "        outputs, (hidden) = self.gru(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #print(\"outputs SIZE\", outputs.shape)\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #print(\"predictions SIZE\", predictions.shape)\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBkR7IE10xtv"
      },
      "source": [
        "#### 3 - GRU + Embedding pre-entrenados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsiotR1O0gOe"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definir la red\n",
        "class GRU_RNN_EB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_weights, \n",
        "                 embedding_dim,\n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights.clone(),\n",
        "                                                      freeze=True,\n",
        "                                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa GRU\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #print(\"EMBED SIZE\", embedded.shape)\n",
        "        \n",
        "        outputs, (hidden) = self.gru(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #print(\"outputs SIZE\", outputs.shape)\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #print(\"predictions SIZE\", predictions.shape)\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRYOEDiQaSHK"
      },
      "source": [
        "--------------------\n",
        "### Modelo 1: LSTM Bidireccional\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC7bhZurfEzP"
      },
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model1 = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model1_name = 'modelo1'  # nombre que tendr√° el modelo guardado..."
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-bdTY1MfD0y"
      },
      "source": [
        "n_epochs1 = 15 # itereaciones"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv38q6OGdlHa"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "criterion1 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIuJF1c8dhfF"
      },
      "source": [
        "model_1 = model1\n",
        "model_name_1 = model1_name\n",
        "n_epochs_1 = n_epochs1\n",
        "loss_1 = criterion1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV9oLkN1aSHO"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 2: LSTM + Bidireccional + Embedding Clinicos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hpxj-Jk1PQZ"
      },
      "source": [
        "from operator import attrgetter\n",
        "# Para asociarle a los tokens vectores de embedding a partir de una lista\n",
        "# de embeddings pueden usar el metodo .set_vector sobre el vocab del Field.\n",
        "# Este metodo recibe 3 argumentos, un mapeo del token al indice dentro de la\n",
        "# lista de embeddings que corresponde a dicho token, la lista de vectores\n",
        "# de embedding y la dimension de los embeddings\n",
        "TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(clinic_embeddings))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImjQ-NJX1vVM"
      },
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_WEIGHT = TEXT.vocab.vectors # Embedding pre entrenados\n",
        "EMBEDDING_DIM = EMBEDDING_WEIGHT.shape[1] # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model2 = NER_RNN_EB(INPUT_DIM, EMBEDDING_WEIGHT, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model2_name = 'modelo2'  # nombre que tendr√° el modelo guardado..."
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zwbhyrC1usj"
      },
      "source": [
        "n_epochs2 = 20 # itereaciones"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp3xylcU1tlK"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "criterion2 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWPzETaNaSHP"
      },
      "source": [
        "model_2 = model2\n",
        "model_name_2 = model2_name\n",
        "n_epochs_2 = n_epochs2\n",
        "loss_2 = criterion2"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpy3p7YaaSHT"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 3: GRU + Bidireccional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlghTZWCrfdi"
      },
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensi√≥n de la capas GRU\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS = 2  # n√∫mero de capas.\n",
        "DROPOUT = 0.2\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model3 = GRU_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model3_name = 'modelo3'  # nombre que tendr√° el modelo guardado..."
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL1EPNO2rjha"
      },
      "source": [
        "n_epochs3 = 10 # itereaciones"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm4Gm5gnrkgC"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "criterion3 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w0CFjA8aSHU"
      },
      "source": [
        "model_3 = model3\n",
        "model_name_3 = model3_name\n",
        "n_epochs_3 = n_epochs3\n",
        "loss_3 = criterion3"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxug2Fwg0DVT"
      },
      "source": [
        "### Modelo 4: GRU + Bidireccional + Embedding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gogbS8KVhO7Q"
      },
      "source": [
        "Aqui se carga el embedding que se desea utilizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWKKUj0s0_uG"
      },
      "source": [
        "#from operator import attrgetter\n",
        "# Para asociarle a los tokens vectores de embedding a partir de una lista\n",
        "# de embeddings pueden usar el metodo .set_vector sobre el vocab del Field.\n",
        "# Este metodo recibe 3 argumentos, un mapeo del token al indice dentro de la\n",
        "# lista de embeddings que corresponde a dicho token, la lista de vectores\n",
        "# de embedding y la dimension de los embeddings\n",
        "\n",
        "################################################################################\n",
        "#                         v  EMBEDDINGS v                                      #\n",
        "################################################################################\n",
        "\n",
        "\n",
        "TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(clinic_embeddings))\n",
        "#TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(suc_embeddings))\n",
        "#TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(fasttext_embeddings))\n",
        "#TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(glove_embeddings))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o16V-5Zg0DBz"
      },
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_WEIGHT = TEXT.vocab.vectors # Embedding pre entrenados\n",
        "EMBEDDING_DIM = EMBEDDING_WEIGHT.shape[1] # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model4 = GRU_RNN_EB(INPUT_DIM, EMBEDDING_WEIGHT, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model4_name = 'modelo4'  # nombre que tendr√° el modelo guardado..."
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdCzDkEW0Ky4"
      },
      "source": [
        "n_epochs4 = 20 # itereaciones"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxBhuENN0K_p"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "criterion4 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8E80WpY0LRs"
      },
      "source": [
        "model_4 = model4\n",
        "model_name_4 = model4_name\n",
        "n_epochs_4 = n_epochs4\n",
        "loss_4 = criterion4"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPGdirx7aSHZ"
      },
      "source": [
        "------\n",
        "### Entrenamos y evaluamos:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__znxtWOFj15"
      },
      "source": [
        "#### Cargar modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKjfWClJ9A8H"
      },
      "source": [
        "##### Evaluacion Modelo 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8YlGnjxaSHZ"
      },
      "source": [
        "model = model_1\n",
        "model_name = model_name_1\n",
        "criterion = loss_1\n",
        "n_epochs = n_epochs_1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv56nZV29F1r"
      },
      "source": [
        "##### Evaluacion Modelo 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfZdmlKkkIFN"
      },
      "source": [
        "model = model_2\n",
        "model_name = model_name_2\n",
        "criterion = loss_2\n",
        "n_epochs = n_epochs_2"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeDpXt1zsFd5"
      },
      "source": [
        "##### Evaluacion Modelo 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMVrCco1sHxE"
      },
      "source": [
        "model = model_3\n",
        "model_name = model_name_3\n",
        "criterion = loss_3\n",
        "n_epochs = n_epochs_3"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svhSwO2P1hnK"
      },
      "source": [
        "##### Evaluacion Modelo 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5BSM7501hLm"
      },
      "source": [
        "model = model_4\n",
        "model_name = model_name_4\n",
        "criterion = loss_4\n",
        "n_epochs = n_epochs_4"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u3S2l-NIvKT"
      },
      "source": [
        "#### Setear Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_lXic2aSHd"
      },
      "source": [
        "\n",
        "\n",
        "##### Inicializamos la red\n",
        "\n",
        "iniciamos los pesos de la red de forma aleatoria (Usando una distribuci√≥n normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-G_NWFcaSHe",
        "outputId": "7824c6ab-915e-4be1-c23d-96028c560916"
      },
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRU_RNN(\n",
              "  (embedding): Embedding(17591, 200, padding_idx=1)\n",
              "  (gru): GRU(200, 128, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=12, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjWDX2CJaSHh",
        "outputId": "9e9e6bdc-7022-4d26-b0f3-4d65e66cfaf4"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El modelo actual tiene 4,071,172 par√°metros entrenables.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVqBqerlaSHk"
      },
      "source": [
        "Por √∫ltimo, definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZvHtwpaSHq"
      },
      "source": [
        "##### Definimos el optimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH6o8_cTaSHq"
      },
      "source": [
        "# Optimizador\n",
        "\n",
        "#optimizer = optim.AdamW(model.parameters(), lr=0.0018)\n",
        "optimizer = optim.AdamW(model.parameters(), lr= 0.0018, amsgrad=True)\n",
        "#optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "#optimizer = optim.Adamax(model.parameters())\n",
        "#optimizer = optim.Adadelta(model.parameters())\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
        "#optimizer = optim.ASGD(model.parameters())\n",
        "#optimizer = optim.Rprop(model.parameters())\n",
        "#optimizer = optim.RMSprop(model.parameters())\n",
        "#optimizer = optim.Adagrad(model.parameters())\n",
        "\n",
        "# Tambien se probo Adafactor de la libreria transformer\n",
        "#optimizer = Adafactor(model.parameters()) "
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz39wa78wGYR"
      },
      "source": [
        "##### Enviamos el modelo a cuda\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqr0AJ6_iicR"
      },
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que est√© disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xlq48WjiW6U"
      },
      "source": [
        "##### Definimos el entrenamiento de la red\n",
        "\n",
        "Algunos conceptos previos: \n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracci√≥n de la √©poca. Se utilizan para entrenar mas r√°pidamente la red. (mas eficiente pasar n datos que uno en cada ejecuci√≥n del backpropagation)\n",
        "\n",
        "Esta funci√≥n est√° encargada de entrenar la red en una √©poca. Para esto, por cada batch de la √©poca actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\"\n",
        "\n",
        "Observaci√≥n: En algunos comentarios aparecer√° el tama√±o de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV6YLt0oiicW"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la √©poca:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "        #tags = [sent len * batch size]\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los par√°metros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las m√©tricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYNcwKnAz5Hf"
      },
      "source": [
        "##### `Definimos la funci√≥n de evaluaci√≥n`\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validaci√≥n. \n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las m√©tricas asociadas al conjunto de validaci√≥n. \n",
        "Ya que las m√©tricas son calculadas por cada batch, estas son retornadas promediadas por el n√∫mero de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsRuiUuHiicY"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las m√©tricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las m√©tricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs-n9Y5yiica"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOl6_UWC8wbq"
      },
      "source": [
        "### Entrenamientos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy3MVf5H0A94"
      },
      "source": [
        "\n",
        "#### Entrenamiento sin EarlyStopping\n",
        "\n",
        "Aqui se puede probar el modelo crgado sin EarlyStopping (por lo general casi todo fue ejecutado con EarlyStopping)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK5lQqpviicf"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, train_iterator, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validaci√≥n)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e6n_SCXkIFW"
      },
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI4HSnJyCph6"
      },
      "source": [
        "### EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUJmNrWBDI8n"
      },
      "source": [
        "#construiremos una clase propia para manejar el early stopping, dado\n",
        "#que pytorch no posee una en concreto.\n",
        "\n",
        "class EarlyStopping():\n",
        "\n",
        "    patience = 0;\n",
        "    delta = 0;\n",
        "    loss = 0;\n",
        "    stop = False;\n",
        "    epoch_counter = 0;\n",
        "\n",
        "    \n",
        "    #1 - patience es la cantidad acumulada de epochs donde la loss no mejora \n",
        "    #    antes de detener el entrenamiento para prevenir overfitting\n",
        "    #2 - delta es la diferencia minima entre la mejor loss hasta el momento\n",
        "    #    y la loss de la epoch actual donde se verifica que ha mejorado\n",
        "    #3 - loss es la mejor loss hasta el momento\n",
        "    #4 - stop indica cuando se debe detener la iteracion de epochs\n",
        "    #5 - epoch_counter indica la cantidad seguida de epochs que no han mejorado\n",
        "    #    la loss, si llega a patience detendremos todo\n",
        "    def __init__(self, patience, delta):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "\n",
        "    #revisamos early stopping en cada epoch\n",
        "    def call(self, current_loss):\n",
        "\n",
        "        #primera iteracion\n",
        "        if self.loss == 0:\n",
        "            self.loss = current_loss\n",
        "        \n",
        "        #la diferencia es mayor que el delta, la loss ha mejorado\n",
        "        elif self.loss - current_loss > delta:\n",
        "            self.loss = current_loss\n",
        "        \n",
        "        #es menor, no ha mejorado lo suficiente, comenzamos a contar patience\n",
        "        elif self.loss - current_loss < delta:\n",
        "            self.epoch_counter += 1\n",
        "            print(f\"Contador early stopping en {self.epoch_counter} de {self.patience}\")\n",
        "            if self.epoch_counter >= self.patience:\n",
        "                print('----- EARLY STOPPING EJECUTADO -----')\n",
        "                print(' ')\n",
        "                self.stop = True"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA17k-cRmlRw"
      },
      "source": [
        "def ES_training(patience, delta):\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    # Early stopping class\n",
        "    # Creamos el objeto de early stop\n",
        "    stop_checker = EarlyStopping(patience, delta)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "        # Entrenar\n",
        "        train_loss, train_precision, train_recall, train_f1 = train(\n",
        "            model, train_iterator, optimizer, criterion)\n",
        "        # Evaluar (valid = validaci√≥n)\n",
        "        valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "            model, valid_iterator, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "        # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
        "      \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "        # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
        "\n",
        "        # Ac√° va el early stopping.\n",
        "        # Iremos checkeando si se cumplen los criterios de patience y delta en cada\n",
        "        # epoch del entrenamiento. Si el stop es true, debemos detenerlo\n",
        "        # y deja el mejor hasta el momento, evitando overfittear.\n",
        "        stop_checker.call(valid_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(\n",
        "            f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "        )\n",
        "        print(\n",
        "            f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "        ) \n",
        "\n",
        "        if stop_checker.stop:\n",
        "            break"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUZ0onZiD-tL"
      },
      "source": [
        "#### Entrenamiento red1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhlgFv16DPsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e9b319-6506-49dc-b561-d044d17d0c7e"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 1.221 | Train f1: 0.22 | Train precision: 0.34 | Train recall: 0.17\n",
            "\t Val. Loss: 0.767 |  Val. f1: 0.52 |  Val. precision: 0.66 | Val. recall: 0.43\n",
            "Epoch: 02 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.647 | Train f1: 0.57 | Train precision: 0.67 | Train recall: 0.50\n",
            "\t Val. Loss: 0.548 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.58\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.466 | Train f1: 0.67 | Train precision: 0.73 | Train recall: 0.63\n",
            "\t Val. Loss: 0.483 |  Val. f1: 0.67 |  Val. precision: 0.74 | Val. recall: 0.61\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.361 | Train f1: 0.74 | Train precision: 0.77 | Train recall: 0.71\n",
            "\t Val. Loss: 0.447 |  Val. f1: 0.70 |  Val. precision: 0.71 | Val. recall: 0.70\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.295 | Train f1: 0.79 | Train precision: 0.80 | Train recall: 0.77\n",
            "\t Val. Loss: 0.434 |  Val. f1: 0.73 |  Val. precision: 0.76 | Val. recall: 0.71\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.247 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.448 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.73\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 07 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.214 | Train f1: 0.85 | Train precision: 0.85 | Train recall: 0.84\n",
            "\t Val. Loss: 0.454 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 08 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.190 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
            "\t Val. Loss: 0.443 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_dE-IG-PVZj"
      },
      "source": [
        "#### Entrenamiento red2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m9IdNcrPaTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452a3812-0442-473f-84d0-dbb27dde28c8"
      },
      "source": [
        "#Este es diferente a red1, se podr√≠a haber dejado trabajando m√°s, pero por \n",
        "#temas de tiempo se prefiri√≥ que no\n",
        "\n",
        "patience = 10\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 1.245 | Train f1: 0.17 | Train precision: 0.32 | Train recall: 0.12\n",
            "\t Val. Loss: 0.959 |  Val. f1: 0.35 |  Val. precision: 0.46 | Val. recall: 0.29\n",
            "Epoch: 02 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.921 | Train f1: 0.34 | Train precision: 0.54 | Train recall: 0.25\n",
            "\t Val. Loss: 0.827 |  Val. f1: 0.38 |  Val. precision: 0.66 | Val. recall: 0.27\n",
            "Contador early stopping en 1 de 10\n",
            "Epoch: 03 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.850 | Train f1: 0.39 | Train precision: 0.58 | Train recall: 0.30\n",
            "\t Val. Loss: 0.742 |  Val. f1: 0.45 |  Val. precision: 0.64 | Val. recall: 0.36\n",
            "Contador early stopping en 2 de 10\n",
            "Epoch: 04 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.810 | Train f1: 0.42 | Train precision: 0.60 | Train recall: 0.33\n",
            "\t Val. Loss: 0.738 |  Val. f1: 0.46 |  Val. precision: 0.57 | Val. recall: 0.40\n",
            "Epoch: 05 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.785 | Train f1: 0.44 | Train precision: 0.61 | Train recall: 0.35\n",
            "\t Val. Loss: 0.726 |  Val. f1: 0.46 |  Val. precision: 0.64 | Val. recall: 0.37\n",
            "Contador early stopping en 3 de 10\n",
            "Epoch: 06 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.783 | Train f1: 0.43 | Train precision: 0.62 | Train recall: 0.34\n",
            "\t Val. Loss: 0.707 |  Val. f1: 0.48 |  Val. precision: 0.62 | Val. recall: 0.39\n",
            "Contador early stopping en 4 de 10\n",
            "Epoch: 07 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.770 | Train f1: 0.45 | Train precision: 0.63 | Train recall: 0.36\n",
            "\t Val. Loss: 0.691 |  Val. f1: 0.50 |  Val. precision: 0.64 | Val. recall: 0.42\n",
            "Contador early stopping en 5 de 10\n",
            "Epoch: 08 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.741 | Train f1: 0.47 | Train precision: 0.64 | Train recall: 0.38\n",
            "\t Val. Loss: 0.654 |  Val. f1: 0.51 |  Val. precision: 0.72 | Val. recall: 0.41\n",
            "Contador early stopping en 6 de 10\n",
            "Epoch: 09 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.722 | Train f1: 0.49 | Train precision: 0.66 | Train recall: 0.39\n",
            "\t Val. Loss: 0.644 |  Val. f1: 0.52 |  Val. precision: 0.69 | Val. recall: 0.42\n",
            "Contador early stopping en 7 de 10\n",
            "Epoch: 10 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.717 | Train f1: 0.48 | Train precision: 0.65 | Train recall: 0.39\n",
            "\t Val. Loss: 0.647 |  Val. f1: 0.53 |  Val. precision: 0.65 | Val. recall: 0.45\n",
            "Contador early stopping en 8 de 10\n",
            "Epoch: 11 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.711 | Train f1: 0.49 | Train precision: 0.66 | Train recall: 0.40\n",
            "\t Val. Loss: 0.643 |  Val. f1: 0.52 |  Val. precision: 0.71 | Val. recall: 0.42\n",
            "Contador early stopping en 9 de 10\n",
            "Epoch: 12 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.699 | Train f1: 0.50 | Train precision: 0.67 | Train recall: 0.41\n",
            "\t Val. Loss: 0.631 |  Val. f1: 0.53 |  Val. precision: 0.72 | Val. recall: 0.43\n",
            "Contador early stopping en 10 de 10\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 13 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.705 | Train f1: 0.50 | Train precision: 0.67 | Train recall: 0.41\n",
            "\t Val. Loss: 0.627 |  Val. f1: 0.53 |  Val. precision: 0.70 | Val. recall: 0.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6_SlV0tsLda"
      },
      "source": [
        "#### Entrenamiento red3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4TsgcJLlWQ8"
      },
      "source": [
        "##### Entrenamiento red 3.1\n",
        "* DROPOUT: 0.5\n",
        "* Capas: 3\n",
        "* Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V-f5gpbsK-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac29c83-180e-47d3-b962-db3f33517b1e"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.841 | Train f1: 0.41 | Train precision: 0.56 | Train recall: 0.34\n",
            "\t Val. Loss: 0.519 |  Val. f1: 0.63 |  Val. precision: 0.76 | Val. recall: 0.55\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.471 | Train f1: 0.67 | Train precision: 0.73 | Train recall: 0.62\n",
            "\t Val. Loss: 0.385 |  Val. f1: 0.72 |  Val. precision: 0.77 | Val. recall: 0.69\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.321 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.75\n",
            "\t Val. Loss: 0.360 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.243 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.360 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.77\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.195 | Train f1: 0.86 | Train precision: 0.86 | Train recall: 0.85\n",
            "\t Val. Loss: 0.387 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.77\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.162 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
            "\t Val. Loss: 0.399 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.77\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.138 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.402 |  Val. f1: 0.78 |  Val. precision: 0.78 | Val. recall: 0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd4YGq1Ql4i8"
      },
      "source": [
        "##### Entrenamiento red 3.2\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYsv2ksHl43f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef41df7f-227b-4e19-eb23-d482c3e3bf66"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.655 | Train f1: 0.53 | Train precision: 0.65 | Train recall: 0.47\n",
            "\t Val. Loss: 0.394 |  Val. f1: 0.70 |  Val. precision: 0.78 | Val. recall: 0.64\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.298 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.77\n",
            "\t Val. Loss: 0.349 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.178 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.362 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.120 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
            "\t Val. Loss: 0.401 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.090 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.424 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.065 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.485 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isNYqH86mNnQ"
      },
      "source": [
        "##### Entrenamiento red 3.3\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvuROTqroPfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a94c48f-7cec-4a41-9c70-a462e33a369a"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.621 | Train f1: 0.55 | Train precision: 0.66 | Train recall: 0.49\n",
            "\t Val. Loss: 0.402 |  Val. f1: 0.70 |  Val. precision: 0.74 | Val. recall: 0.66\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.279 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.78\n",
            "\t Val. Loss: 0.352 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.72\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.171 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.365 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.112 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.398 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.084 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.457 |  Val. f1: 0.75 |  Val. precision: 0.79 | Val. recall: 0.71\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.059 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.515 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq0KcqVWokVY"
      },
      "source": [
        "##### Entrenamiento red 3.4\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 4\n",
        "* Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJAxXU5pogW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4152a90-39e7-45f4-cbc6-fe04fac4dd91"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.703 | Train f1: 0.51 | Train precision: 0.64 | Train recall: 0.45\n",
            "\t Val. Loss: 0.413 |  Val. f1: 0.70 |  Val. precision: 0.76 | Val. recall: 0.65\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.321 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.75\n",
            "\t Val. Loss: 0.357 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.194 | Train f1: 0.86 | Train precision: 0.86 | Train recall: 0.85\n",
            "\t Val. Loss: 0.379 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.136 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.425 |  Val. f1: 0.75 |  Val. precision: 0.73 | Val. recall: 0.77\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.104 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.439 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.084 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.483 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClHYf3CcovAc"
      },
      "source": [
        "##### Entrenamiento red 3.5\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW (lr = 0.001 que viene por defecto)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMfMiK21puJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a497f8c-9aec-4e59-d5c7-ef222d4c287f"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.623 | Train f1: 0.55 | Train precision: 0.66 | Train recall: 0.48\n",
            "\t Val. Loss: 0.395 |  Val. f1: 0.71 |  Val. precision: 0.77 | Val. recall: 0.66\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.287 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.77\n",
            "\t Val. Loss: 0.344 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.72\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.169 | Train f1: 0.88 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.356 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.112 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.395 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.081 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.472 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.73\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.061 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.482 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S49Ud_v6o6xt"
      },
      "source": [
        "##### Entrenamiento red 3.6\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW(lr = 0.0018)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEN9GwrPpvXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486ea700-32d4-4387-d3b2-b535b5982a80"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.582 | Train f1: 0.59 | Train precision: 0.69 | Train recall: 0.54\n",
            "\t Val. Loss: 0.367 |  Val. f1: 0.72 |  Val. precision: 0.81 | Val. recall: 0.66\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.248 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.333 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.145 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.373 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.102 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.414 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.077 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.492 |  Val. f1: 0.75 |  Val. precision: 0.79 | Val. recall: 0.71\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.061 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.513 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53yp5Jb4pCIq"
      },
      "source": [
        "##### Entrenamiento red 3.7\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: AdamW(lr = 0.0018)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4KVPwxMpv4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27eb7b28-8228-4c35-d3e9-fc29c8a3c1a6"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.588 | Train f1: 0.59 | Train precision: 0.68 | Train recall: 0.53\n",
            "\t Val. Loss: 0.372 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.249 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.344 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.149 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.372 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.107 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.397 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.082 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.413 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.77\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.063 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.503 |  Val. f1: 0.75 |  Val. precision: 0.74 | Val. recall: 0.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCEB9ly-pE51"
      },
      "source": [
        "##### Entrenamiento red 3.8\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW(lr = 0.00175)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7q6YK7YpwMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e880fac3-8e50-427a-d01c-718d18d361c0"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.553 | Train f1: 0.60 | Train precision: 0.69 | Train recall: 0.54\n",
            "\t Val. Loss: 0.360 |  Val. f1: 0.74 |  Val. precision: 0.78 | Val. recall: 0.70\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.235 | Train f1: 0.83 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.346 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.139 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.363 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.092 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.409 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.067 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.471 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.051 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.515 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuLmxnxgpIED"
      },
      "source": [
        "##### Entrenamiento red 3.9\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: AdamW(lr = 0.00175)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lI39CzXpwb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00acdae-b1c5-4f13-a347-179080ed3f5c"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.586 | Train f1: 0.59 | Train precision: 0.69 | Train recall: 0.53\n",
            "\t Val. Loss: 0.374 |  Val. f1: 0.72 |  Val. precision: 0.78 | Val. recall: 0.67\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.251 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.359 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.73\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.151 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "\t Val. Loss: 0.386 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.109 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.391 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.082 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.417 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.064 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFxPDpA3qpqf"
      },
      "source": [
        "##### Entrenamiento red 3.10\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW(lr = 0.0018, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq1v7c4Nq-P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09013e98-d704-45d1-8f2d-ccbbcf575fc9"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.550 | Train f1: 0.60 | Train precision: 0.70 | Train recall: 0.55\n",
            "\t Val. Loss: 0.355 |  Val. f1: 0.74 |  Val. precision: 0.79 | Val. recall: 0.70\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.236 | Train f1: 0.83 | Train precision: 0.84 | Train recall: 0.82\n",
            "\t Val. Loss: 0.343 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.134 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
            "\t Val. Loss: 0.375 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.089 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.423 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.73\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.065 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.448 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.047 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.509 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIyQp2CNq4sN"
      },
      "source": [
        "##### Entrenamiento red 3.11\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW(lr = 0.00175, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rj0BcAcq-dA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b8fbf0-6a06-4e52-f498-682d7cc7d4f6"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.547 | Train f1: 0.60 | Train precision: 0.70 | Train recall: 0.55\n",
            "\t Val. Loss: 0.358 |  Val. f1: 0.74 |  Val. precision: 0.77 | Val. recall: 0.72\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.234 | Train f1: 0.83 | Train precision: 0.84 | Train recall: 0.82\n",
            "\t Val. Loss: 0.343 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.137 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.356 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.088 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.411 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.064 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.76 |  Val. precision: 0.81 | Val. recall: 0.73\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.048 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.487 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0RkSajb1dnp"
      },
      "source": [
        "##### Entrenamiento red 3.12\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: AdamW(lr = 0.00175, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnIe5sGo1qs_",
        "outputId": "6c9e255e-0264-4180-c805-a1cbf816a71f"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.586 | Train f1: 0.58 | Train precision: 0.68 | Train recall: 0.53\n",
            "\t Val. Loss: 0.368 |  Val. f1: 0.74 |  Val. precision: 0.77 | Val. recall: 0.72\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.253 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.335 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.149 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.353 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.103 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
            "\t Val. Loss: 0.407 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.073 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.451 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.054 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.519 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWftY8CA1mRV"
      },
      "source": [
        "##### Entrenamiento red 3.13\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: AdamW(lr = 0.0018, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvaj5e2R1rUc",
        "outputId": "8b08d736-9763-49d5-81f9-da617306b18d"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.575 | Train f1: 0.59 | Train precision: 0.69 | Train recall: 0.54\n",
            "\t Val. Loss: 0.385 |  Val. f1: 0.71 |  Val. precision: 0.75 | Val. recall: 0.68\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.250 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.343 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.148 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.354 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.101 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.417 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.73\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.073 | Train f1: 0.95 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.446 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.056 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.461 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTpn4PCZ1od9"
      },
      "source": [
        "#### Entrenamiento red4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtlw0ZvorA_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd2480f-5c7e-4c6a-dc7f-232206ac48c8"
      },
      "source": [
        "patience = 10\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.992 | Train f1: 0.29 | Train precision: 0.48 | Train recall: 0.21\n",
            "\t Val. Loss: 0.792 |  Val. f1: 0.43 |  Val. precision: 0.64 | Val. recall: 0.33\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.800 | Train f1: 0.43 | Train precision: 0.61 | Train recall: 0.34\n",
            "\t Val. Loss: 0.688 |  Val. f1: 0.50 |  Val. precision: 0.67 | Val. recall: 0.40\n",
            "Contador early stopping en 1 de 10\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.721 | Train f1: 0.49 | Train precision: 0.66 | Train recall: 0.40\n",
            "\t Val. Loss: 0.628 |  Val. f1: 0.53 |  Val. precision: 0.68 | Val. recall: 0.44\n",
            "Contador early stopping en 2 de 10\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.668 | Train f1: 0.52 | Train precision: 0.68 | Train recall: 0.43\n",
            "\t Val. Loss: 0.589 |  Val. f1: 0.56 |  Val. precision: 0.74 | Val. recall: 0.46\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.627 | Train f1: 0.56 | Train precision: 0.70 | Train recall: 0.47\n",
            "\t Val. Loss: 0.566 |  Val. f1: 0.57 |  Val. precision: 0.75 | Val. recall: 0.47\n",
            "Contador early stopping en 3 de 10\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.598 | Train f1: 0.57 | Train precision: 0.71 | Train recall: 0.49\n",
            "\t Val. Loss: 0.554 |  Val. f1: 0.59 |  Val. precision: 0.70 | Val. recall: 0.52\n",
            "Contador early stopping en 4 de 10\n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.572 | Train f1: 0.59 | Train precision: 0.71 | Train recall: 0.51\n",
            "\t Val. Loss: 0.535 |  Val. f1: 0.60 |  Val. precision: 0.73 | Val. recall: 0.52\n",
            "Contador early stopping en 5 de 10\n",
            "Epoch: 08 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.552 | Train f1: 0.61 | Train precision: 0.73 | Train recall: 0.53\n",
            "\t Val. Loss: 0.520 |  Val. f1: 0.61 |  Val. precision: 0.73 | Val. recall: 0.54\n",
            "Contador early stopping en 6 de 10\n",
            "Epoch: 09 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.531 | Train f1: 0.62 | Train precision: 0.74 | Train recall: 0.55\n",
            "\t Val. Loss: 0.516 |  Val. f1: 0.62 |  Val. precision: 0.74 | Val. recall: 0.54\n",
            "Contador early stopping en 7 de 10\n",
            "Epoch: 10 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.514 | Train f1: 0.63 | Train precision: 0.74 | Train recall: 0.56\n",
            "\t Val. Loss: 0.494 |  Val. f1: 0.63 |  Val. precision: 0.72 | Val. recall: 0.57\n",
            "Contador early stopping en 8 de 10\n",
            "Epoch: 11 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.495 | Train f1: 0.64 | Train precision: 0.75 | Train recall: 0.57\n",
            "\t Val. Loss: 0.497 |  Val. f1: 0.63 |  Val. precision: 0.70 | Val. recall: 0.58\n",
            "Contador early stopping en 9 de 10\n",
            "Epoch: 12 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.479 | Train f1: 0.66 | Train precision: 0.75 | Train recall: 0.59\n",
            "\t Val. Loss: 0.483 |  Val. f1: 0.64 |  Val. precision: 0.71 | Val. recall: 0.58\n",
            "Contador early stopping en 10 de 10\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 13 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.468 | Train f1: 0.67 | Train precision: 0.76 | Train recall: 0.60\n",
            "\t Val. Loss: 0.478 |  Val. f1: 0.64 |  Val. precision: 0.75 | Val. recall: 0.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Sgpfl_lgMk"
      },
      "source": [
        "### Testeo y resultados finales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqXIYyBHkIFW"
      },
      "source": [
        "#### Evaluamos el set de validaci√≥n con el modelo final\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluaci√≥n con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27CNYfrjtQ-"
      },
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAYgwmTTkIFX"
      },
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF1ysw_Kw6zz"
      },
      "source": [
        "\n",
        "#### Predecir datos para la competencia\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, predeciremos las etiquetas que ser√°n evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RBs3UU4wLk3"
      },
      "source": [
        "def predict_labels(model, iterator, criterion, fields=fields):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0][1]\n",
        "    nertags_field = fields[1][1]\n",
        "    tags_vocab = nertags_field.vocab.itos\n",
        "    words_vocab = text_field.vocab.itos\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text_batch = batch.text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(batch.text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oraci√≥n predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<pad>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "                predictions.append(['EOS', 'EOS'])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model, test_iterator, criterion)"
      ],
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwQp1Ru8Oht8"
      },
      "source": [
        "#### Generar el archivo para la submission\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPfZkjJGkWyq"
      },
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for i, (word, tag) in enumerate(predictions[:-1]):\n",
        "    if word=='EOS' and tag=='EOS': f.write('\\n')\n",
        "    else: \n",
        "      if i == len(predictions[:-1])-1:\n",
        "        f.write(word + ' ' + tag)\n",
        "      else: f.write(word + ' ' + tag + '\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRcDAFXK48XL"
      },
      "source": [
        "### Resultados notables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWBFWggx4_wu"
      },
      "source": [
        "Los entrenamientos de las redes 3.6, 3.7 y 3.10 fueron los de mejor rendimiento en cuanto a m√©tricas y loss se refiere. Ac√° una tabla comparativa:\n",
        "\n",
        "| Modelo | Valid Loss | F1   | Precision | Recall |\n",
        "|--------|------------|------|-----------|--------|\n",
        "| 3.6    | 0.333      | 0.77 | 0.80      | 0.74   |\n",
        "| 3.7    | 0.344      | 0.77 | 0.80      | 0.74   |\n",
        "| 3.10   | 0.343      | 0.77 | 0.79      | 0.75   |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZEWJXrNaSIf"
      },
      "source": [
        "## Conclusiones\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtK7y43V7Z_"
      },
      "source": [
        "La competencia se present√≥ acompa√±ada de un ab√°nico de dificultades por superar, que permitieron al grupo aprender sostenidamente en el per√≠odo de su realizaci√≥n.\n",
        "\n",
        "Para extender lo dicho en el apartado de dise√±o \n",
        "experimental, el ajuste de aspectos como cantidad de capas, dimensiones, n√∫mero de neuronas, valor del dropout, entre otros, comprender en profundidad el proceso computacional se torn√≥ relativamente cr√≠ptico y complicado. Se pudo observar c√≥mo estos par√°metros variaban los resultados, pero cost√≥ comprender las razones que generaron dichas distinciones, dado que los estudiantes del grupo a√∫n no poseen el conocimiento necesario ni la experiencia requerida para diferenciar eso en mayor detalles.\n",
        "\n",
        "El p√°rrafo anterior podr√° ser desalentador, pero en realidad es una muestra de lo extenso que es el mundo de NLP, aprendizaje de m√°quinas y deep learning. Esto motiva a los estudiantes del grupo para continuar explorando estas disciplinas. \n",
        "\n",
        "Ahondando en los experimentos y resultados, a partir de las pruebas realizadas:\n",
        "\n",
        "- El entrenamiento de la red 2 podr√≠a haberse extendido con m√°s iteraciones. Si bien su recall terminaba estanc√°ndose, existe la posibilidad de que, al ajustar otros par√°metros y a√±adirle m√°s configuraciones, podr√≠a haber arrojado mejores resultados.\n",
        "\n",
        "- Pareciera que los embeddings utilizados no mejoraban los modelos, pero se cree que esto se debe a la falta de experiencia manej√°ndolos y no a un caso donde √©stos fueran perjudiciales para el procesamiento y aprendizaje de los modelos.\n",
        "\n",
        "- GRU aport√≥ mejores resultados que LSTM en este caso. Un dropout de 0.2 era un punto adecuado,en general, para resultados estables.\n",
        "\n",
        "- Las redes neuronales bidireccionales mostraron mejores resultados en los ensayos preliminares, notando su capacidad superior en varios casos.\n",
        "\n",
        "- El optimizador Adam y sus variantes tuvieron mejor desempe√±o de manera general, pero ajustar sus hiperpar√°metros fue complejo de entender. Se considera que eso se aprendi√≥ someramente, requiriendo estudios m√°s profundos para comprenderlo mejor. Se trabaj√≥ principalmente con ensayo y error.\n",
        "\n",
        "- La t√©cnica de early stopping fue de suma utilidad para prevenir overfitteos. Aquello fue bastante perjudicial en el trabajo de la competencia 1.\n",
        "\n",
        "Finalizando, las mejores a futuro y trabajos propuestos:\n",
        "\n",
        "- Utilizar herramientas que autom√°ticamente optimicen los hiperpar√°emtros de los modelos, en lugar de hacerlo por tanteo o intuici√≥n manualmente.\n",
        "\n",
        "- Implementar una herramienta de learning rate m√°s all√° del par√°metro que permiten los optimizadores. Esto podr√≠a haber funcionado con el entrenamiento de la red 2, porque sus losses de training y validaci√≥n no alcanzaron a divergir en las epoch propuestas. Se piensa que el modelo podr√≠a haber continuado aprendiendo y mejorando sin overfittearse. \n",
        "\n",
        "- Estudiar la implementaci√≥n adecuada de capas adicionales en las arquitecturas de redes neuronales y ver sus efectos en el aprendizaje de los modelos.\n",
        "\n",
        "- Probar m√°s funciones de loss.\n"
      ]
    }
  ]
}