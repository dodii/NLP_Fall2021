{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Competencia2_CC62051.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6u3S2l-NIvKT",
        "S0Sgpfl_lgMk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0tyIsliieNr"
      },
      "source": [
        "# **Competencia 2 - CC6205 Natural Language Processing 📚**\n",
        "\n",
        "**Integrantes:** Vicente Ardiles, Rodrigo Oportot\n",
        "\n",
        "**Usuario del equipo en CodaLab:** NLPachi\n",
        "\n",
        "**Fecha límite de entrega 📆:** 12 de Julio.\n",
        "\n",
        "**Tiempo estimado de dedicación:** 40 horas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQlYlmGaSFH"
      },
      "source": [
        "## Introducción\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVL0-01AUOzL"
      },
      "source": [
        "La competencia tiene como objetivo resolver una task de NER sobre un corpus perteneciente a la lista de espera NO GES en Chile. La task en cuestión se aborda como un problema de Sequence Labeling. Tal como se explica en el enunciado de la competencia, se tiene lo siguiente:\n",
        "\n",
        "- Dada una secuencia de tokens (oración), sequence labeling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia. En pocas palabras, dada una lista de tokens, se espera encontrar la mejor secuencia de etiquetas asociadas a esa lista.\n",
        "\n",
        "NER es un ejemplo de un problema de Sequence Labeling. Primero es necesario definir los siguientes conceptos, antes de formalizar la task en cuestión:\n",
        "\n",
        "- *Token*: secuencia de caracteres, pudiendo ser una palabra, un número o un símbolo.\n",
        "\n",
        "- *Entidad*: un trozo de texto (uno o más tokens) asociado a una categoría predefinida.\n",
        "\n",
        "- *Límites de una entidad*: índices de inicio y fín de los tokens dentro de una entidad.\n",
        "\n",
        "- *Tipo de entidad*: categoría predefinida asociada a la entidad.\n",
        "\n",
        "Ahora se puede definir formalmente una entidad como una tupla: $(s, e, t)$, donde $s, e$ son los límites de la entidad (índices de los tokens de inicio y fin, respectivamente) y e corresponde al tipo de entidad o categoría.\n",
        "\n",
        "El corpus está constituido originalmente por 7 tipos de entidades, pero por simplicidad de la competencia se trabajará con las siguientes:\n",
        "\n",
        "- **Disease**\n",
        "- **Body_Part**\n",
        "- **Medication** \n",
        "- **Procedures** \n",
        "- **Family_Member**\n",
        "\n",
        "La formalización del problema se construye tal como sigue:\n",
        "\n",
        "**Formato ConLL**\n",
        "\n",
        "Los archivos vienen en un formato estándar llamado ConLL. Es un archivo de texto que cumple las siguientes propiedades:\n",
        "\n",
        "- Un salto de linea corresponde a la separación entre oraciones. Permite pasar una lista de oraciones como batches a las redes neuronales.\n",
        "\n",
        "- La primera columna del archivo contiene todos los tokens de la partición.\n",
        "\n",
        "- La segunda columna del archivo contiene el tipo de entidad asociado al token de la primera columna.\n",
        "\n",
        "- Los tipos de entidades siguen un formato denominado *IOB2*. Si un tipo de entidad comienza con el prefijo \"B-\" (Beginning) significa que es el token de inicio de una entidad, si comienza con \"I-\" (Inside) es un token distinto al de inicio y si un token está asociado a la categoría O (Outside) significa que no pertenece a ninguna entidad.\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "```\n",
        "PACIENTE O\n",
        "PRESENTA O\n",
        "FRACTURA B-Disease\n",
        "CORONARIA I-Disease\n",
        "COMPLICADA I-Disease\n",
        "EN O\n",
        "PIE B-Body_Part\n",
        "IZQUIERDO I-Body_Part\n",
        ". O\n",
        "SE O\n",
        "REALIZA O\n",
        "INSTRUMENTACION B-Procedure\n",
        "INTRACONDUCTO I-Procedure\n",
        ". O\n",
        "```\n",
        "\n",
        "Por la definición se tienen las siguientes tres entidades (enumerando desde 0): \n",
        "\n",
        "- $(2, 4, Disease)$\n",
        "- $(6, 7, Body Part)$\n",
        "- $(11, 12, Procedure)$\n",
        "\n",
        "El rendimiento de los modelos es sometido bajo **métricas estrictas**. Solamente se considera correcta una predicción si, al compararla con las entidades reales, **coinciden tanto los límites de la entidad como el tipo.** \n",
        "\n",
        "Tomando el caso anterior, si un modelo es capaz de encontrar la siguiente entidad: $(2, 3, Disease)$, entonces se considera incorrecto ya que pudo predecir dos de los tres tokens de dicha enfermedad. Se busca una métrica que sea alta a nivel de entidad y no a nivel de token.\n",
        "\n",
        "Se vislumbra como desafío el diseño de redes neuronales capaces de solucionar la task correctamente. La información es de carácter nacional, lo que la hace aún más interesante. El formato del corpus exige predecir a nivel de entidad y no de token, también aumentando la dificultad para resolver la task correctamente. Por último, queda señalar que la naturaleza de los algoritmos a emplear, al ser no supervisada (Deep Learning), dificulta un poco estas primeras etapas de aprendizaje para los alumnos, por su grado de complejidad. No obstante, se espera aprender mucho al haber finalizado la competencia, acumulando experiencia para los siguientes cursos que los alumnos puedan tomar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbA1EmhCaSFI"
      },
      "source": [
        "## Modelos \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HsvlfPJUSId"
      },
      "source": [
        "En primera instancia, a los siguientes modelos se les fueron alterando los siguientes hiperparámetros (en caso de poseerlos) para realizar los experimentos:\n",
        "\n",
        "- INPUT_DIM, tamaño del vocabulario.\n",
        "- EMBEDDING_WEIGHT, embedding pre entrenados.\n",
        "- EMBEDDING_DIM, dimensión de los embeddings.\n",
        "- HIDDEN_DIM = 128, dimensión de la capas.\n",
        "- OUTPUT_DIM, número de clases.\n",
        "- N_LAYERS = 3, número de capas.\n",
        "- DROPOUT = 0.2, valor del dropout (probabilidad de remover neuronas en las capas para evitar que se co-adapten entre ellas).\n",
        "- BIDIRECTIONAL, booleano para activar bidireccionalidad.\n",
        "- n_epochs, itereaciones para el entrenamiento.\n",
        "\n",
        "Como función de loss, se mantuvo constante Cross Entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKTQFSL7tZ06"
      },
      "source": [
        "### Modelo 1: LSTM Bidireccional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYhZ6j9MtcS1"
      },
      "source": [
        "Este modelo es similar al de la baseline, pero se activó el parámetro bidireccional. Más adelante se fueron ajustado algunos hiperparámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aEJMvQHtfHP"
      },
      "source": [
        "### Modelo 2: LSTM + Bidireccional + Embedding Clinicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8_w-TQZtgoF"
      },
      "source": [
        "Este modelo es similar al de la baseline, pero se activó el parámetro bidireccional. También se le fueron ajustado algunos hiperparámetros más adelante. El punto importante fue la adición de embeddings clínicos pre-entrenados de la lista de espera chilena (obtenido de https://zenodo.org/record/3924799) a la arquitectura de la RNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_x99c2RtjDh"
      },
      "source": [
        "### Modelo 3: GRU + Bidireccional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5RuHUGctkp0"
      },
      "source": [
        "GRU bidireccional. Como nota aparte, en las primeras instancias del trabajo, fue aquel que entregó mejores resultados preliminares, por lo que se prefirió por sobre los LSTM anteriormente mencionados en gran parte de los experimentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hJNaDzwtmQV"
      },
      "source": [
        "### Modelo 4: GRU + Bidireccional + Embedding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB2I5RJ9tnuz"
      },
      "source": [
        "Similar al modelo 3, pero con la adición de distintos embeddings pre-entrenados:\n",
        "\n",
        " - Los clínicos de https://zenodo.org/record/3924799 ya mencionados.\n",
        " - FastText embeddings de SUC.\n",
        " - GloveEmbeddings de SBWC.\n",
        " - FastText embeddings SBWC.\n",
        "\n",
        "Se añadieron más embeddings en comparación al modelo 2 por lo mencionado en el modelo 3. Todos fueron obtenidos de https://github.com/dccuchile/spanish-word-embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVhZ5iaaSFK"
      },
      "source": [
        "## Métricas de evaluación\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl3GaVMUYA7"
      },
      "source": [
        "- **Métrica estricta:** Para que una entidad se considere predicha exitosamente por un modelo, deben reconocerse correctamente los límites y el tipo de entidad en conjunto. Si al menos uno de esos dos aspectos es erróneo, toda la entidad estará incorrectamente catalogada.\n",
        "- **Precision:** Esta métrica indica qué tan válidos son los resultados extraídos. Es la fracción de resultados de una categoría **que realmente lo son** sobre resultados seleccionados en dicha categoría por el algoritmo: $\\frac{verdaderos \\ positivos}{elementos \\ seleccionados \\ como \\ positivos}$. \n",
        "- **Recall:** Esta métrica indica qué tan completos son los resultados extraídos. De manera sencilla, sería: $\\frac{verdaderos \\ positivos}{todos \\ los \\ positivos}$.\n",
        "- **Micro F1 score:** Es el promedio armónico entre precision y recall, por lo tanto: $\\frac{2}{recall^{-1} \\ * \\ precision^{-1}} = \\frac{verdadero \\ positivo}{verdadero \\ positivo \\ + \\ 1/2 \\ * \\ (falso \\ positivo \\ + \\ falso \\ negativo)}$.\n",
        "\n",
        "    Macro F1 computa las métricas de F1 para todas las clases de manera independiento para luego promediarlas , mientras que micro F1 suma las contribuciones de todas las clases (los verdaderos positivos) para calcular dicho promedio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27WffRVUj4v"
      },
      "source": [
        "## Diseño experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwfIfQpT8-A6"
      },
      "source": [
        "### Experimentos preliminares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O228DbeUmE7"
      },
      "source": [
        "En un principio, se compararon los tres primeros modelos propuestos con los parámetros predeterminados (baseline), observando que el modelo 3 arrojaba mejores resultados de manera notoria en comparación a los otros dos modelos, haciendo referencia a las métricas estrictas como criterios de evaluación. Por esta razón, se prefirió continuar con él. A partir de esto, surgió el modelo 4, explicado más adelante. \n",
        "\n",
        "Cabe señalar que el modelo 2, que adoptaba embeddings pre-entrenados,  daba resultados inferiores al modelo 1. También que casi la totalidad de los experimentos incluyeron early stopping para prevenir modelos overfitteados. Esto será analizado más adelante en los resultados y conclusiones.\n",
        "\n",
        "En relación a porqué no se optó por refinar los primeros dos modelos para continuar comparándolos, esto es a causa de no comprender en su totalidad la injerencia de los diversos parámetros e hiperparámetros de los modelos de redes neuronales. Si bien se pueden estudiar sus definiciones, conocer a fondo la totalidad de sus repercusiones en su procesamiento al ser estos alterados aún escapa de las habilidades de los estudiantes. Se fueron modificando y comparando los parámetros de manera muy sencilla, con el fin de aprender lo mejor posible, yendo en desmedro de complejizar las estructuras encargadas de resolver las tareas. Si bien esto puede resultar cuestionable, permitió acercarse de manera más didáctica a la competencia. Esto se pudo notar al superar los resultados de la baseline y aprender a identificar un modelo overfitteado, problema que el grupo pudo sobrellevar de mejor manera al mirar retrospectivamente el trabajo de la competencia 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bt5opE6t2El"
      },
      "source": [
        "### Experimentos principales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7vTO7fyt3aj"
      },
      "source": [
        "Luego de lo anterior, el resto de los experimentos involucraron al modelo 3 y 4, en donde se variaron una diversidad de parámetros y optimizadores (Adam, AdamW, Adafactor, etc) para realizarlos. Respecto a esto último, finalmente se le dio prioridad a Adam, dado que arrojó mejores resultados que el resto. Resumiendo este apartado, se observó que el modelo 3 era superior al modelo 4, tanto en loss como métricas estrictas. Por esta razón, se optó por continuar con éste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PShipID2tvlu"
      },
      "source": [
        "### Experimentos finales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tLvtNtItyWW"
      },
      "source": [
        "Finalizando los diseños, se buscó refinar lo más posible el modelo 3, ajustando sus hiperparámetros y aspectos asociados de manera exhaustiva. Esto es explicado en la sección de resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFM-wNt8aSFM"
      },
      "source": [
        "## Experimentos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMgKjfYC_Go-"
      },
      "source": [
        "###  Carga de datos y Preprocesamiento\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librería [`torchtext`](https://github.com/pytorch/text).\n",
        "En particular usaremos su módulo `data`, el cual según su documentación original provee: \n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso será el siguiente: \n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27csY87GaSFO",
        "scrolled": false,
        "outputId": "035674ed-bde0-4aa7-d722-d13a129b97cf"
      },
      "source": [
        "# Instalamos torchtext que nos facilitará la vida en el pre-procesamiento del formato ConLL.\n",
        "!pip3 install --upgrade torchtext"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torchtext in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng7wRGEyawjM"
      },
      "source": [
        "import torch\n",
        "from torchtext import data, datasets, legacy\n",
        "\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehSou6rCvwg"
      },
      "source": [
        "#### Obtener datos\n",
        "\n",
        "Descargamos los datos de entrenamiento, validación y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbT0g_kC18Jb",
        "outputId": "a0938b54-4671-4c62-a7d5-e29075a879bf"
      },
      "source": [
        "#%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de Validación (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¡¡SON LOS QUE DEBEN SER PREDICHOS!!"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-13 02:38:29--  https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=a51666ea45f4c403f7b515ef76d51e07d423d671239e052b1da4a189fcaf60af&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-13 02:38:29--  https://github-releases.githubusercontent.com/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=a51666ea45f4c403f7b515ef76d51e07d423d671239e052b1da4a189fcaf60af&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.109.154, 185.199.108.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1607913 (1.5M) [application/octet-stream]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>]   1.53M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-13 02:38:29 (57.8 MB/s) - ‘train.txt’ saved [1607913/1607913]\n",
            "\n",
            "--2021-07-13 02:38:29--  https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=e37b4bc2483809c26a280d061f6c6925e700fe4395db758c52963da78f2be66b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-13 02:38:29--  https://github-releases.githubusercontent.com/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=e37b4bc2483809c26a280d061f6c6925e700fe4395db758c52963da78f2be66b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.109.154, 185.199.108.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177166 (173K) [application/octet-stream]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>] 173.01K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-07-13 02:38:29 (44.3 MB/s) - ‘dev.txt’ saved [177166/177166]\n",
            "\n",
            "--2021-07-13 02:38:29--  https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=babfa22b93c05ded61a27daf49715110c664d5182780f591b7a44372033cd4c5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-13 02:38:29--  https://github-releases.githubusercontent.com/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T023753Z&X-Amz-Expires=300&X-Amz-Signature=babfa22b93c05ded61a27daf49715110c664d5182780f591b7a44372033cd4c5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.109.154, 185.199.108.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147052 (144K) [application/octet-stream]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>] 143.61K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-07-13 02:38:29 (31.2 MB/s) - ‘test.txt’ saved [147052/147052]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKg0h8A1nire"
      },
      "source": [
        "#### Obtener embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8_m6HBJoW7r"
      },
      "source": [
        "import csv\n",
        "import gzip\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import requests"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqj5N_jjQZdq"
      },
      "source": [
        "Embeddings clinicos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKEQH5PDnetF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d58bd8-5819-4eaf-ecf9-409b17318e5d"
      },
      "source": [
        "VEC_FILE = \"cwlce.vec\"\n",
        "\n",
        "# Descargar embeddings de aca\n",
        "# https://zenodo.org/record/3924799/files/cwlce.vec?download=1\n",
        "if not os.path.exists(VEC_FILE):\n",
        "    print(f\"Descargando {VEC_FILE}\")\n",
        "    url = \"https://zenodo.org/record/3924799/files/cwlce.vec?download=1\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    try:\n",
        "        open(VEC_FILE, \"wb\").write(response.content)\n",
        "    except Exception as e:\n",
        "        os.remove(VEC_FILE)\n",
        "        raise e"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Descargando cwlce.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VshE7y-dp8RG"
      },
      "source": [
        "Los siguientes embeddings son bastante pesados asi que estos fueron ejecutados en la medida que se requerian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp25sARIX3R-"
      },
      "source": [
        "Spanish Unannotated Corpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOpY9-pwXTQP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "bfa7f0e9-7506-42c5-c370-47c9a1a3d358"
      },
      "source": [
        "'''\n",
        "VEC2_FILE = \"em1.vec\"\n",
        "\n",
        "# Descargar embeddings de aca\n",
        "# https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1\n",
        "if not os.path.exists(VEC2_FILE):\n",
        "    print(f\"Descargando {VEC2_FILE}\")\n",
        "    url = \"https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    try:\n",
        "        open(VEC2_FILE, \"wb\").write(response.content)\n",
        "    except Exception as e:\n",
        "        os.remove(VEC2_FILE)\n",
        "        raise e\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nVEC2_FILE = \"em1.vec\"\\n\\n# Descargar embeddings de aca\\n# https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1\\nif not os.path.exists(VEC2_FILE):\\n    print(f\"Descargando {VEC2_FILE}\")\\n    url = \"https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1\"\\n    response = requests.get(url, stream=True)\\n    try:\\n        open(VEC2_FILE, \"wb\").write(response.content)\\n    except Exception as e:\\n        os.remove(VEC2_FILE)\\n        raise e\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJxPZB63ena7"
      },
      "source": [
        "FastText embeddings from SBWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRT-61Olenv_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "477c4059-5727-4d11-f2ba-66a8472e2469"
      },
      "source": [
        "'''\n",
        "FASTTEXT_FILE = \"fasttext300d.vec\"\n",
        "\n",
        "# Descargar vectores fasttext de aca\n",
        "# http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.vec.gz\n",
        "if not os.path.exists(FASTTEXT_FILE):\n",
        "    print(f\"Descargando {FASTTEXT_FILE}\")\n",
        "    url = \"http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.vec.gz\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    try:\n",
        "        with gzip.open(response.raw, \"rb\") as f_in:\n",
        "            with open(FASTTEXT_FILE, \"wb\") as f_out:\n",
        "                # Funcion util para copiar de un file-like object a otro\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "    except Exception as e:\n",
        "        os.remove(FASTTEXT_FILE)\n",
        "        raise e\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nFASTTEXT_FILE = \"fasttext300d.vec\"\\n\\n# Descargar vectores fasttext de aca\\n# http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.vec.gz\\nif not os.path.exists(FASTTEXT_FILE):\\n    print(f\"Descargando {FASTTEXT_FILE}\")\\n    url = \"http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.vec.gz\"\\n    response = requests.get(url, stream=True)\\n    try:\\n        with gzip.open(response.raw, \"rb\") as f_in:\\n            with open(FASTTEXT_FILE, \"wb\") as f_out:\\n                # Funcion util para copiar de un file-like object a otro\\n                shutil.copyfileobj(f_in, f_out)\\n    except Exception as e:\\n        os.remove(FASTTEXT_FILE)\\n        raise e\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy_o830ZfdVX"
      },
      "source": [
        "GloVe embeddings from SBWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAl_SMCkfdm3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "ba9e5cbd-f3ec-4cb9-e031-809b5f3059ad"
      },
      "source": [
        "'''\n",
        "GLOVE_FILE = \"glove300d.vec\"\n",
        "\n",
        "# Descargar vectores glove de aca\n",
        "# https://github.com/dccuchile/spanish-word-embeddings\n",
        "if not os.path.exists(GLOVE_FILE):\n",
        "    print(f\"Descargando {GLOVE_FILE}\")\n",
        "    url = \"http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i25.vec.gz\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    try:\n",
        "        with gzip.open(response.raw, \"rb\") as f_in:\n",
        "            with open(GLOVE_FILE, \"wb\") as f_out:\n",
        "                # Funcion util para copiar de un file-like object a otro\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "    except Exception as e:\n",
        "        os.remove(GLOVE_FILE)\n",
        "        raise e\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nGLOVE_FILE = \"glove300d.vec\"\\n\\n# Descargar vectores glove de aca\\n# https://github.com/dccuchile/spanish-word-embeddings\\nif not os.path.exists(GLOVE_FILE):\\n    print(f\"Descargando {GLOVE_FILE}\")\\n    url = \"http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i25.vec.gz\"\\n    response = requests.get(url, stream=True)\\n    try:\\n        with gzip.open(response.raw, \"rb\") as f_in:\\n            with open(GLOVE_FILE, \"wb\") as f_out:\\n                # Funcion util para copiar de un file-like object a otro\\n                shutil.copyfileobj(f_in, f_out)\\n    except Exception as e:\\n        os.remove(GLOVE_FILE)\\n        raise e\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMud7YGMBZvg"
      },
      "source": [
        "####  Fields\n",
        "\n",
        "Un `field`:\n",
        "\n",
        "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
        "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
        "* Contiene otros parámetros relacionados con la forma en que se debe numericalizar un tipo de datos, como un método de tokenización y el tipo de Tensor que se debe producir.\n",
        "\n",
        "\n",
        "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
        "\n",
        "\n",
        "```\n",
        "El O\n",
        "paciente O\n",
        "padece O\n",
        "de O\n",
        "cancer B-Disease\n",
        "de I-Disease\n",
        "colon I-Disease\n",
        ". O\n",
        "```\n",
        "\n",
        "Cada linea contiene un token y el tipo de entidad asociado en el formato IOB2 ya explicado. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
        "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`tokens`) y los NER_TAGS (`categorías`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DcM_IjgCdzz"
      },
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = legacy.data.Field(lower=False) \n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = legacy.data.Field(unk_token=None)\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxOIkuw8sPlK"
      },
      "source": [
        "Cargar embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yksa22JVsPAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46732d75-4a9c-4b42-8464-4c6b521408cf"
      },
      "source": [
        "from torchtext import vocab\n",
        "\n",
        "# Para cargar los vectores de embeddings (que son esencialmente un vocabulario\n",
        "# donde cada palabra tiene asociado un vector) pueden usar la clase vocab.Vectors\n",
        "\n",
        "clinic_embeddings = vocab.Vectors(VEC_FILE)\n",
        "\n",
        "# CARGAR LOS OTROS EMBEDDINGS\n",
        "\n",
        "#suc_embeddings = vocab.Vectors(VEC2_FILE)\n",
        "#fasttext_embeddings = vocab.Vectors(FASTTEXT_FILE)\n",
        "#glove_embeddings = vocab.Vectors(GLOVE_FILE)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/57112 [00:00<?, ?it/s]Skipping token b'57112' with 1-dimensional vector [b'300']; likely a header\n",
            " 99%|█████████▉| 56773/57112 [00:05<00:00, 10396.60it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCKTJOdgC5eC"
      },
      "source": [
        "####  SequenceTaggingDataset\n",
        "\n",
        "`SequenceTaggingDataset` es una clase de torchtext diseñada para contener datasets de sequence labelling. \n",
        "Los ejemplos que se guarden en una instancia de estos serán arreglos de palabras pareados con sus respectivos tags.\n",
        "Por ejemplo, para Part-of-speech tagging:\n",
        "\n",
        "[I, love, PyTorch, .] estará pareado con [PRON, VERB, PROPN, PUNCT]\n",
        "\n",
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase cómo cargar los datasets de prueba, validación y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsHdGml62J21"
      },
      "source": [
        "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train.txt\",\n",
        "    validation=\"dev.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu7q3HCliia5",
        "outputId": "0ef131e2-8914-4617-f9f0-85f9e35e7da7"
      },
      "source": [
        "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"Número de ejemplos de validación: {len(valid_data)}\")\n",
        "print(f\"Número de ejemplos de test (competencia): {len(test_data)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero de ejemplos de entrenamiento: 8025\n",
            "Número de ejemplos de validación: 891\n",
            "Número de ejemplos de test (competencia): 992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDRnhXAdFGL-"
      },
      "source": [
        "Visualizemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T023Ld4RaSF4",
        "scrolled": false,
        "outputId": "e5e86272-0456-45cf-8df1-3ff1df7f210c"
      },
      "source": [
        "import random\n",
        "random_item_idx = random.randint(0, len(train_data))\n",
        "random_example = train_data.examples[random_item_idx]\n",
        "list(zip(random_example.text, random_example.nertags))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('solicito', 'O'),\n",
              " ('poder', 'O'),\n",
              " ('evaluar', 'O'),\n",
              " ('posibilidad', 'O'),\n",
              " ('de', 'O'),\n",
              " ('retratamiento', 'B-Procedure'),\n",
              " ('o', 'O'),\n",
              " ('de', 'O'),\n",
              " ('ser', 'O'),\n",
              " ('necesario', 'O'),\n",
              " ('realizar', 'O'),\n",
              " ('derivación', 'O'),\n",
              " ('interna', 'O'),\n",
              " ('para', 'O'),\n",
              " ('cirugía', 'B-Procedure'),\n",
              " ('apical', 'I-Procedure')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l05KYy5FSUy"
      },
      "source": [
        "#### Construir los vocabularios para el texto y las etiquetas\n",
        "\n",
        "Los vocabularios son los obbjetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields.\n",
        "El siguiente paso consiste en construirlos. Para esto, hacemos uso del método `Field.build_vocab` sobre cada uno de nuestros `fields`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBhp7WICiibL"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4OgUKM_iibO",
        "scrolled": true,
        "outputId": "fb56c326-9f2a-4558-9c01-6ffe25f9a873"
      },
      "source": [
        "print(f\"Tokens únicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens únicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens únicos en TEXT: 17591\n",
            "Tokens únicos en NER_TAGS: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4FeyL9nFnId",
        "outputId": "193976bb-ed5f-4073-e349-2f99631b8eda"
      },
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "NER_TAGS.vocab.itos"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " 'O',\n",
              " 'I-Disease',\n",
              " 'B-Disease',\n",
              " 'I-Body_Part',\n",
              " 'B-Body_Part',\n",
              " 'B-Procedure',\n",
              " 'I-Procedure',\n",
              " 'B-Medication',\n",
              " 'B-Family_Member',\n",
              " 'I-Medication',\n",
              " 'I-Family_Member']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQDoUqSHFKj"
      },
      "source": [
        "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oración.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5eSLm4diibR",
        "outputId": "9db294e5-28a1-44a9-a833-b7c6cb25a6dd"
      },
      "source": [
        "# Tokens mas frecuentes\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 7396),\n",
              " (',', 6821),\n",
              " ('-', 4985),\n",
              " ('de', 3811),\n",
              " ('DE', 3645),\n",
              " ('/', 2317),\n",
              " (':', 2209),\n",
              " ('con', 1484),\n",
              " ('y', 1439),\n",
              " ('APS', 1429)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDyNLMPz9duD"
      },
      "source": [
        "# Seteamos algunas variables que nos serán de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrYvF3X0sjWL"
      },
      "source": [
        "#### Frecuencia de los Tags\n",
        "\n",
        "Visualizemos rápidamente las cantidades y frecuencias de cada tag:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXOsbJUiibh",
        "outputId": "61b7766a-a598-436f-f01d-33ec5752814a"
      },
      "source": [
        "def tag_percentage(tag_counts):\n",
        "    \n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "  \n",
        "    return tag_counts_percentages\n",
        "\n",
        "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tag Ocurrencia Porcentaje\n",
            "\n",
            "O\t101671\t68.1%\n",
            "I-Disease\t21629\t14.5%\n",
            "B-Disease\t8831\t 5.9%\n",
            "I-Body_Part\t6489\t 4.3%\n",
            "B-Body_Part\t3755\t 2.5%\n",
            "B-Procedure\t2891\t 1.9%\n",
            "I-Procedure\t2819\t 1.9%\n",
            "B-Medication\t784\t 0.5%\n",
            "B-Family_Member\t228\t 0.2%\n",
            "I-Medication\t116\t 0.1%\n",
            "I-Family_Member\t9\t 0.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4wPiydnaSGs"
      },
      "source": [
        "#### Configuramos pytorch y dividimos los datos.\n",
        "\n",
        "Importante: si tienes problemas con la ram de la gpu, disminuye el tamaño de los batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB7cwLWpaSGs",
        "outputId": "31bd4a9b-1992-4d5f-94a9-562fee876e87"
      },
      "source": [
        "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que está disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test\n",
        "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B21E1eAFId16"
      },
      "source": [
        "#### Métricas de evaluación\n",
        "\n",
        "Además, definiremos las métricas que serán usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `f1`.\n",
        "**Importante**: Noten que la evaluación solo se hace para las Named Entities (sin contar 'O')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o63ov69_rX2T",
        "outputId": "ad3b71d5-6ae5-4cbe-d8ab-2ac85653c54f"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 24.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 29.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 31.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16184 sha256=0c2c1a3b41321f3fecd2151b02d593c0849530f7c5d9e2024667803f8db31024\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mUOOLEWiicU"
      },
      "source": [
        "# Definimos las métricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "import warnings\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "    # Obtenemos los indices distintos de 0.\n",
        "\n",
        "    # filtramos <pad> y O para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hod516H1aSG2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### Baseline Model\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendrá una capa de embedding, unas cuantas LSTM y una capa de salida y usará dropout en el entrenamiento.\n",
        "\n",
        "Este constará de los siguientes pasos: \n",
        "\n",
        "1. Definir la clase que contendrá la red.\n",
        "2. Definir los hiperparámetros e inicializar la red. \n",
        "3. Definir la época de entrenamiento\n",
        "3. Definir la función de loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYprx75dvhLQ"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.RNN(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #print(\"EMBED SIZE\", embedded.shape)\n",
        "        \n",
        "        outputs, (hidden) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #print(\"outputs SIZE\", outputs.shape)\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #print(\"predictions SIZE\", predictions.shape)\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### Hiperparámetros de la red\n",
        "\n",
        "Definimos los hiperparámetros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHdi3QdOaSG8"
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlF1DhJeaSHA"
      },
      "source": [
        "baseline_n_epochs = 10"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la función de loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G_4k99_aSHG"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzQSO2lawiy7"
      },
      "source": [
        "### Arquitecturas Modelos propuestos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8z_cCr3w56A"
      },
      "source": [
        "#### 1- LSTM Embedding pre-entrenados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fDFBfEbwibp"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN_EB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim,\n",
        "                 embedding_weights, \n",
        "                 embedding_dim,\n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights.clone(),\n",
        "                                                      freeze=True,\n",
        "                                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.RNN(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #print(\"EMBED SIZE\", embedded.shape)\n",
        "        \n",
        "        outputs, (hidden) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #print(\"outputs SIZE\", outputs.shape)\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #print(\"predictions SIZE\", predictions.shape)\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdTgHOJvqfJH"
      },
      "source": [
        "#### 2 - GRU \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvjQzE4Tqeko"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definir la red\n",
        "class GRU_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa GRU\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #print(\"EMBED SIZE\", embedded.shape)\n",
        "        \n",
        "        outputs, (hidden) = self.gru(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #print(\"outputs SIZE\", outputs.shape)\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #print(\"predictions SIZE\", predictions.shape)\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBkR7IE10xtv"
      },
      "source": [
        "#### 3 - GRU + Embedding pre-entrenados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsiotR1O0gOe"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definir la red\n",
        "class GRU_RNN_EB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_weights, \n",
        "                 embedding_dim,\n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights.clone(),\n",
        "                                                      freeze=True,\n",
        "                                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa GRU\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #print(\"EMBED SIZE\", embedded.shape)\n",
        "        \n",
        "        outputs, (hidden) = self.gru(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #print(\"outputs SIZE\", outputs.shape)\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #print(\"predictions SIZE\", predictions.shape)\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRYOEDiQaSHK"
      },
      "source": [
        "--------------------\n",
        "### Modelo 1: LSTM Bidireccional\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC7bhZurfEzP"
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model1 = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model1_name = 'modelo1'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-bdTY1MfD0y"
      },
      "source": [
        "n_epochs1 = 15 # itereaciones"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv38q6OGdlHa"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "criterion1 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIuJF1c8dhfF"
      },
      "source": [
        "model_1 = model1\n",
        "model_name_1 = model1_name\n",
        "n_epochs_1 = n_epochs1\n",
        "loss_1 = criterion1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV9oLkN1aSHO"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 2: LSTM + Bidireccional + Embedding Clinicos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hpxj-Jk1PQZ"
      },
      "source": [
        "from operator import attrgetter\n",
        "# Para asociarle a los tokens vectores de embedding a partir de una lista\n",
        "# de embeddings pueden usar el metodo .set_vector sobre el vocab del Field.\n",
        "# Este metodo recibe 3 argumentos, un mapeo del token al indice dentro de la\n",
        "# lista de embeddings que corresponde a dicho token, la lista de vectores\n",
        "# de embedding y la dimension de los embeddings\n",
        "TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(clinic_embeddings))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImjQ-NJX1vVM"
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_WEIGHT = TEXT.vocab.vectors # Embedding pre entrenados\n",
        "EMBEDDING_DIM = EMBEDDING_WEIGHT.shape[1] # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model2 = NER_RNN_EB(INPUT_DIM, EMBEDDING_WEIGHT, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model2_name = 'modelo2'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zwbhyrC1usj"
      },
      "source": [
        "n_epochs2 = 20 # itereaciones"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp3xylcU1tlK"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "criterion2 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWPzETaNaSHP"
      },
      "source": [
        "model_2 = model2\n",
        "model_name_2 = model2_name\n",
        "n_epochs_2 = n_epochs2\n",
        "loss_2 = criterion2"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpy3p7YaaSHT"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 3: GRU + Bidireccional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlghTZWCrfdi"
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensión de la capas GRU\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.2\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model3 = GRU_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model3_name = 'modelo3'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL1EPNO2rjha"
      },
      "source": [
        "n_epochs3 = 10 # itereaciones"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm4Gm5gnrkgC"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "criterion3 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w0CFjA8aSHU"
      },
      "source": [
        "model_3 = model3\n",
        "model_name_3 = model3_name\n",
        "n_epochs_3 = n_epochs3\n",
        "loss_3 = criterion3"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxug2Fwg0DVT"
      },
      "source": [
        "### Modelo 4: GRU + Bidireccional + Embedding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gogbS8KVhO7Q"
      },
      "source": [
        "Aqui se carga el embedding que se desea utilizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWKKUj0s0_uG"
      },
      "source": [
        "#from operator import attrgetter\n",
        "# Para asociarle a los tokens vectores de embedding a partir de una lista\n",
        "# de embeddings pueden usar el metodo .set_vector sobre el vocab del Field.\n",
        "# Este metodo recibe 3 argumentos, un mapeo del token al indice dentro de la\n",
        "# lista de embeddings que corresponde a dicho token, la lista de vectores\n",
        "# de embedding y la dimension de los embeddings\n",
        "\n",
        "################################################################################\n",
        "#                         v  EMBEDDINGS v                                      #\n",
        "################################################################################\n",
        "\n",
        "\n",
        "TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(clinic_embeddings))\n",
        "#TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(suc_embeddings))\n",
        "#TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(fasttext_embeddings))\n",
        "#TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(glove_embeddings))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o16V-5Zg0DBz"
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_WEIGHT = TEXT.vocab.vectors # Embedding pre entrenados\n",
        "EMBEDDING_DIM = EMBEDDING_WEIGHT.shape[1] # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model4 = GRU_RNN_EB(INPUT_DIM, EMBEDDING_WEIGHT, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model4_name = 'modelo4'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdCzDkEW0Ky4"
      },
      "source": [
        "n_epochs4 = 20 # itereaciones"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxBhuENN0K_p"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "criterion4 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8E80WpY0LRs"
      },
      "source": [
        "model_4 = model4\n",
        "model_name_4 = model4_name\n",
        "n_epochs_4 = n_epochs4\n",
        "loss_4 = criterion4"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPGdirx7aSHZ"
      },
      "source": [
        "------\n",
        "### Entrenamos y evaluamos:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__znxtWOFj15"
      },
      "source": [
        "#### Cargar modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKjfWClJ9A8H"
      },
      "source": [
        "##### Evaluacion Modelo 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8YlGnjxaSHZ"
      },
      "source": [
        "model = model_1\n",
        "model_name = model_name_1\n",
        "criterion = loss_1\n",
        "n_epochs = n_epochs_1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv56nZV29F1r"
      },
      "source": [
        "##### Evaluacion Modelo 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfZdmlKkkIFN"
      },
      "source": [
        "model = model_2\n",
        "model_name = model_name_2\n",
        "criterion = loss_2\n",
        "n_epochs = n_epochs_2"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeDpXt1zsFd5"
      },
      "source": [
        "##### Evaluacion Modelo 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMVrCco1sHxE"
      },
      "source": [
        "model = model_3\n",
        "model_name = model_name_3\n",
        "criterion = loss_3\n",
        "n_epochs = n_epochs_3"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svhSwO2P1hnK"
      },
      "source": [
        "##### Evaluacion Modelo 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5BSM7501hLm"
      },
      "source": [
        "model = model_4\n",
        "model_name = model_name_4\n",
        "criterion = loss_4\n",
        "n_epochs = n_epochs_4"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u3S2l-NIvKT"
      },
      "source": [
        "#### Setear Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_lXic2aSHd"
      },
      "source": [
        "\n",
        "\n",
        "##### Inicializamos la red\n",
        "\n",
        "iniciamos los pesos de la red de forma aleatoria (Usando una distribución normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-G_NWFcaSHe",
        "outputId": "7824c6ab-915e-4be1-c23d-96028c560916"
      },
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRU_RNN(\n",
              "  (embedding): Embedding(17591, 200, padding_idx=1)\n",
              "  (gru): GRU(200, 128, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=12, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjWDX2CJaSHh",
        "outputId": "9e9e6bdc-7022-4d26-b0f3-4d65e66cfaf4"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El modelo actual tiene 4,071,172 parámetros entrenables.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVqBqerlaSHk"
      },
      "source": [
        "Por último, definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZvHtwpaSHq"
      },
      "source": [
        "##### Definimos el optimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH6o8_cTaSHq"
      },
      "source": [
        "# Optimizador\n",
        "\n",
        "#optimizer = optim.AdamW(model.parameters(), lr=0.0018)\n",
        "optimizer = optim.AdamW(model.parameters(), lr= 0.0018, amsgrad=True)\n",
        "#optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "#optimizer = optim.Adamax(model.parameters())\n",
        "#optimizer = optim.Adadelta(model.parameters())\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
        "#optimizer = optim.ASGD(model.parameters())\n",
        "#optimizer = optim.Rprop(model.parameters())\n",
        "#optimizer = optim.RMSprop(model.parameters())\n",
        "#optimizer = optim.Adagrad(model.parameters())\n",
        "\n",
        "# Tambien se probo Adafactor de la libreria transformer\n",
        "#optimizer = Adafactor(model.parameters()) "
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz39wa78wGYR"
      },
      "source": [
        "##### Enviamos el modelo a cuda\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqr0AJ6_iicR"
      },
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que esté disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xlq48WjiW6U"
      },
      "source": [
        "##### Definimos el entrenamiento de la red\n",
        "\n",
        "Algunos conceptos previos: \n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracción de la época. Se utilizan para entrenar mas rápidamente la red. (mas eficiente pasar n datos que uno en cada ejecución del backpropagation)\n",
        "\n",
        "Esta función está encargada de entrenar la red en una época. Para esto, por cada batch de la época actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\"\n",
        "\n",
        "Observación: En algunos comentarios aparecerá el tamaño de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV6YLt0oiicW"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la época:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteración anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "        #tags = [sent len * batch size]\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los parámetros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las métricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYNcwKnAz5Hf"
      },
      "source": [
        "##### `Definimos la función de evaluación`\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validación. \n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las métricas asociadas al conjunto de validación. \n",
        "Ya que las métricas son calculadas por cada batch, estas son retornadas promediadas por el número de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsRuiUuHiicY"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las métricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las métricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs-n9Y5yiica"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOl6_UWC8wbq"
      },
      "source": [
        "### Entrenamientos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy3MVf5H0A94"
      },
      "source": [
        "\n",
        "#### Entrenamiento sin EarlyStopping\n",
        "\n",
        "Aqui se puede probar el modelo crgado sin EarlyStopping (por lo general casi todo fue ejecutado con EarlyStopping)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK5lQqpviicf"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, train_iterator, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validación)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e6n_SCXkIFW"
      },
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI4HSnJyCph6"
      },
      "source": [
        "### EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUJmNrWBDI8n"
      },
      "source": [
        "#construiremos una clase propia para manejar el early stopping, dado\n",
        "#que pytorch no posee una en concreto.\n",
        "\n",
        "class EarlyStopping():\n",
        "\n",
        "    patience = 0;\n",
        "    delta = 0;\n",
        "    loss = 0;\n",
        "    stop = False;\n",
        "    epoch_counter = 0;\n",
        "\n",
        "    \n",
        "    #1 - patience es la cantidad acumulada de epochs donde la loss no mejora \n",
        "    #    antes de detener el entrenamiento para prevenir overfitting\n",
        "    #2 - delta es la diferencia minima entre la mejor loss hasta el momento\n",
        "    #    y la loss de la epoch actual donde se verifica que ha mejorado\n",
        "    #3 - loss es la mejor loss hasta el momento\n",
        "    #4 - stop indica cuando se debe detener la iteracion de epochs\n",
        "    #5 - epoch_counter indica la cantidad seguida de epochs que no han mejorado\n",
        "    #    la loss, si llega a patience detendremos todo\n",
        "    def __init__(self, patience, delta):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "\n",
        "    #revisamos early stopping en cada epoch\n",
        "    def call(self, current_loss):\n",
        "\n",
        "        #primera iteracion\n",
        "        if self.loss == 0:\n",
        "            self.loss = current_loss\n",
        "        \n",
        "        #la diferencia es mayor que el delta, la loss ha mejorado\n",
        "        elif self.loss - current_loss > delta:\n",
        "            self.loss = current_loss\n",
        "        \n",
        "        #es menor, no ha mejorado lo suficiente, comenzamos a contar patience\n",
        "        elif self.loss - current_loss < delta:\n",
        "            self.epoch_counter += 1\n",
        "            print(f\"Contador early stopping en {self.epoch_counter} de {self.patience}\")\n",
        "            if self.epoch_counter >= self.patience:\n",
        "                print('----- EARLY STOPPING EJECUTADO -----')\n",
        "                print(' ')\n",
        "                self.stop = True"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA17k-cRmlRw"
      },
      "source": [
        "def ES_training(patience, delta):\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    # Early stopping class\n",
        "    # Creamos el objeto de early stop\n",
        "    stop_checker = EarlyStopping(patience, delta)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "        # Entrenar\n",
        "        train_loss, train_precision, train_recall, train_f1 = train(\n",
        "            model, train_iterator, optimizer, criterion)\n",
        "        # Evaluar (valid = validación)\n",
        "        valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "            model, valid_iterator, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "        # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
        "      \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "        # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
        "\n",
        "        # Acá va el early stopping.\n",
        "        # Iremos checkeando si se cumplen los criterios de patience y delta en cada\n",
        "        # epoch del entrenamiento. Si el stop es true, debemos detenerlo\n",
        "        # y deja el mejor hasta el momento, evitando overfittear.\n",
        "        stop_checker.call(valid_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(\n",
        "            f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "        )\n",
        "        print(\n",
        "            f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "        ) \n",
        "\n",
        "        if stop_checker.stop:\n",
        "            break"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUZ0onZiD-tL"
      },
      "source": [
        "#### Entrenamiento red1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhlgFv16DPsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e9b319-6506-49dc-b561-d044d17d0c7e"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 1.221 | Train f1: 0.22 | Train precision: 0.34 | Train recall: 0.17\n",
            "\t Val. Loss: 0.767 |  Val. f1: 0.52 |  Val. precision: 0.66 | Val. recall: 0.43\n",
            "Epoch: 02 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.647 | Train f1: 0.57 | Train precision: 0.67 | Train recall: 0.50\n",
            "\t Val. Loss: 0.548 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.58\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.466 | Train f1: 0.67 | Train precision: 0.73 | Train recall: 0.63\n",
            "\t Val. Loss: 0.483 |  Val. f1: 0.67 |  Val. precision: 0.74 | Val. recall: 0.61\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.361 | Train f1: 0.74 | Train precision: 0.77 | Train recall: 0.71\n",
            "\t Val. Loss: 0.447 |  Val. f1: 0.70 |  Val. precision: 0.71 | Val. recall: 0.70\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.295 | Train f1: 0.79 | Train precision: 0.80 | Train recall: 0.77\n",
            "\t Val. Loss: 0.434 |  Val. f1: 0.73 |  Val. precision: 0.76 | Val. recall: 0.71\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.247 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.448 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.73\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 07 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.214 | Train f1: 0.85 | Train precision: 0.85 | Train recall: 0.84\n",
            "\t Val. Loss: 0.454 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 08 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.190 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
            "\t Val. Loss: 0.443 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_dE-IG-PVZj"
      },
      "source": [
        "#### Entrenamiento red2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m9IdNcrPaTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452a3812-0442-473f-84d0-dbb27dde28c8"
      },
      "source": [
        "#Este es diferente a red1, se podría haber dejado trabajando más, pero por \n",
        "#temas de tiempo se prefirió que no\n",
        "\n",
        "patience = 10\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 1.245 | Train f1: 0.17 | Train precision: 0.32 | Train recall: 0.12\n",
            "\t Val. Loss: 0.959 |  Val. f1: 0.35 |  Val. precision: 0.46 | Val. recall: 0.29\n",
            "Epoch: 02 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.921 | Train f1: 0.34 | Train precision: 0.54 | Train recall: 0.25\n",
            "\t Val. Loss: 0.827 |  Val. f1: 0.38 |  Val. precision: 0.66 | Val. recall: 0.27\n",
            "Contador early stopping en 1 de 10\n",
            "Epoch: 03 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.850 | Train f1: 0.39 | Train precision: 0.58 | Train recall: 0.30\n",
            "\t Val. Loss: 0.742 |  Val. f1: 0.45 |  Val. precision: 0.64 | Val. recall: 0.36\n",
            "Contador early stopping en 2 de 10\n",
            "Epoch: 04 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.810 | Train f1: 0.42 | Train precision: 0.60 | Train recall: 0.33\n",
            "\t Val. Loss: 0.738 |  Val. f1: 0.46 |  Val. precision: 0.57 | Val. recall: 0.40\n",
            "Epoch: 05 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.785 | Train f1: 0.44 | Train precision: 0.61 | Train recall: 0.35\n",
            "\t Val. Loss: 0.726 |  Val. f1: 0.46 |  Val. precision: 0.64 | Val. recall: 0.37\n",
            "Contador early stopping en 3 de 10\n",
            "Epoch: 06 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.783 | Train f1: 0.43 | Train precision: 0.62 | Train recall: 0.34\n",
            "\t Val. Loss: 0.707 |  Val. f1: 0.48 |  Val. precision: 0.62 | Val. recall: 0.39\n",
            "Contador early stopping en 4 de 10\n",
            "Epoch: 07 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.770 | Train f1: 0.45 | Train precision: 0.63 | Train recall: 0.36\n",
            "\t Val. Loss: 0.691 |  Val. f1: 0.50 |  Val. precision: 0.64 | Val. recall: 0.42\n",
            "Contador early stopping en 5 de 10\n",
            "Epoch: 08 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.741 | Train f1: 0.47 | Train precision: 0.64 | Train recall: 0.38\n",
            "\t Val. Loss: 0.654 |  Val. f1: 0.51 |  Val. precision: 0.72 | Val. recall: 0.41\n",
            "Contador early stopping en 6 de 10\n",
            "Epoch: 09 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.722 | Train f1: 0.49 | Train precision: 0.66 | Train recall: 0.39\n",
            "\t Val. Loss: 0.644 |  Val. f1: 0.52 |  Val. precision: 0.69 | Val. recall: 0.42\n",
            "Contador early stopping en 7 de 10\n",
            "Epoch: 10 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.717 | Train f1: 0.48 | Train precision: 0.65 | Train recall: 0.39\n",
            "\t Val. Loss: 0.647 |  Val. f1: 0.53 |  Val. precision: 0.65 | Val. recall: 0.45\n",
            "Contador early stopping en 8 de 10\n",
            "Epoch: 11 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.711 | Train f1: 0.49 | Train precision: 0.66 | Train recall: 0.40\n",
            "\t Val. Loss: 0.643 |  Val. f1: 0.52 |  Val. precision: 0.71 | Val. recall: 0.42\n",
            "Contador early stopping en 9 de 10\n",
            "Epoch: 12 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.699 | Train f1: 0.50 | Train precision: 0.67 | Train recall: 0.41\n",
            "\t Val. Loss: 0.631 |  Val. f1: 0.53 |  Val. precision: 0.72 | Val. recall: 0.43\n",
            "Contador early stopping en 10 de 10\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 13 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.705 | Train f1: 0.50 | Train precision: 0.67 | Train recall: 0.41\n",
            "\t Val. Loss: 0.627 |  Val. f1: 0.53 |  Val. precision: 0.70 | Val. recall: 0.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6_SlV0tsLda"
      },
      "source": [
        "#### Entrenamiento red3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4TsgcJLlWQ8"
      },
      "source": [
        "##### Entrenamiento red 3.1\n",
        "* DROPOUT: 0.5\n",
        "* Capas: 3\n",
        "* Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V-f5gpbsK-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac29c83-180e-47d3-b962-db3f33517b1e"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.841 | Train f1: 0.41 | Train precision: 0.56 | Train recall: 0.34\n",
            "\t Val. Loss: 0.519 |  Val. f1: 0.63 |  Val. precision: 0.76 | Val. recall: 0.55\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.471 | Train f1: 0.67 | Train precision: 0.73 | Train recall: 0.62\n",
            "\t Val. Loss: 0.385 |  Val. f1: 0.72 |  Val. precision: 0.77 | Val. recall: 0.69\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.321 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.75\n",
            "\t Val. Loss: 0.360 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.243 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.360 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.77\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.195 | Train f1: 0.86 | Train precision: 0.86 | Train recall: 0.85\n",
            "\t Val. Loss: 0.387 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.77\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.162 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
            "\t Val. Loss: 0.399 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.77\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.138 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.402 |  Val. f1: 0.78 |  Val. precision: 0.78 | Val. recall: 0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd4YGq1Ql4i8"
      },
      "source": [
        "##### Entrenamiento red 3.2\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYsv2ksHl43f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef41df7f-227b-4e19-eb23-d482c3e3bf66"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.655 | Train f1: 0.53 | Train precision: 0.65 | Train recall: 0.47\n",
            "\t Val. Loss: 0.394 |  Val. f1: 0.70 |  Val. precision: 0.78 | Val. recall: 0.64\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.298 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.77\n",
            "\t Val. Loss: 0.349 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.178 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.362 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.120 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
            "\t Val. Loss: 0.401 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.090 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.424 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.065 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.485 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isNYqH86mNnQ"
      },
      "source": [
        "##### Entrenamiento red 3.3\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvuROTqroPfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a94c48f-7cec-4a41-9c70-a462e33a369a"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.621 | Train f1: 0.55 | Train precision: 0.66 | Train recall: 0.49\n",
            "\t Val. Loss: 0.402 |  Val. f1: 0.70 |  Val. precision: 0.74 | Val. recall: 0.66\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.279 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.78\n",
            "\t Val. Loss: 0.352 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.72\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.171 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.365 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.112 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.398 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.084 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.457 |  Val. f1: 0.75 |  Val. precision: 0.79 | Val. recall: 0.71\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.059 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.515 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq0KcqVWokVY"
      },
      "source": [
        "##### Entrenamiento red 3.4\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 4\n",
        "* Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJAxXU5pogW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4152a90-39e7-45f4-cbc6-fe04fac4dd91"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.703 | Train f1: 0.51 | Train precision: 0.64 | Train recall: 0.45\n",
            "\t Val. Loss: 0.413 |  Val. f1: 0.70 |  Val. precision: 0.76 | Val. recall: 0.65\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.321 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.75\n",
            "\t Val. Loss: 0.357 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.194 | Train f1: 0.86 | Train precision: 0.86 | Train recall: 0.85\n",
            "\t Val. Loss: 0.379 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.136 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.425 |  Val. f1: 0.75 |  Val. precision: 0.73 | Val. recall: 0.77\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.104 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.439 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.084 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.483 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClHYf3CcovAc"
      },
      "source": [
        "##### Entrenamiento red 3.5\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW (lr = 0.001 que viene por defecto)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMfMiK21puJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a497f8c-9aec-4e59-d5c7-ef222d4c287f"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.623 | Train f1: 0.55 | Train precision: 0.66 | Train recall: 0.48\n",
            "\t Val. Loss: 0.395 |  Val. f1: 0.71 |  Val. precision: 0.77 | Val. recall: 0.66\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.287 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.77\n",
            "\t Val. Loss: 0.344 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.72\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.169 | Train f1: 0.88 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.356 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.112 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.395 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.081 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.472 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.73\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.061 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.482 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S49Ud_v6o6xt"
      },
      "source": [
        "##### Entrenamiento red 3.6\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW(lr = 0.0018)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEN9GwrPpvXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486ea700-32d4-4387-d3b2-b535b5982a80"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.582 | Train f1: 0.59 | Train precision: 0.69 | Train recall: 0.54\n",
            "\t Val. Loss: 0.367 |  Val. f1: 0.72 |  Val. precision: 0.81 | Val. recall: 0.66\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.248 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.333 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.145 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.373 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.102 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.414 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.077 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.492 |  Val. f1: 0.75 |  Val. precision: 0.79 | Val. recall: 0.71\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.061 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.513 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53yp5Jb4pCIq"
      },
      "source": [
        "##### Entrenamiento red 3.7\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: AdamW(lr = 0.0018)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4KVPwxMpv4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27eb7b28-8228-4c35-d3e9-fc29c8a3c1a6"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.588 | Train f1: 0.59 | Train precision: 0.68 | Train recall: 0.53\n",
            "\t Val. Loss: 0.372 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.249 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.344 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.149 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.372 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.107 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.397 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.082 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.413 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.77\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.063 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.503 |  Val. f1: 0.75 |  Val. precision: 0.74 | Val. recall: 0.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCEB9ly-pE51"
      },
      "source": [
        "##### Entrenamiento red 3.8\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW(lr = 0.00175)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7q6YK7YpwMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e880fac3-8e50-427a-d01c-718d18d361c0"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.553 | Train f1: 0.60 | Train precision: 0.69 | Train recall: 0.54\n",
            "\t Val. Loss: 0.360 |  Val. f1: 0.74 |  Val. precision: 0.78 | Val. recall: 0.70\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.235 | Train f1: 0.83 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.346 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.139 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.363 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.092 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.409 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.067 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.471 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.051 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.515 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuLmxnxgpIED"
      },
      "source": [
        "##### Entrenamiento red 3.9\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: AdamW(lr = 0.00175)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lI39CzXpwb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00acdae-b1c5-4f13-a347-179080ed3f5c"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.586 | Train f1: 0.59 | Train precision: 0.69 | Train recall: 0.53\n",
            "\t Val. Loss: 0.374 |  Val. f1: 0.72 |  Val. precision: 0.78 | Val. recall: 0.67\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.251 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.359 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.73\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.151 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "\t Val. Loss: 0.386 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.109 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.391 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.082 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.417 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.064 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFxPDpA3qpqf"
      },
      "source": [
        "##### Entrenamiento red 3.10\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW(lr = 0.0018, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq1v7c4Nq-P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09013e98-d704-45d1-8f2d-ccbbcf575fc9"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.550 | Train f1: 0.60 | Train precision: 0.70 | Train recall: 0.55\n",
            "\t Val. Loss: 0.355 |  Val. f1: 0.74 |  Val. precision: 0.79 | Val. recall: 0.70\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.236 | Train f1: 0.83 | Train precision: 0.84 | Train recall: 0.82\n",
            "\t Val. Loss: 0.343 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.134 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
            "\t Val. Loss: 0.375 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.089 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.423 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.73\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.065 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.448 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.047 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.509 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIyQp2CNq4sN"
      },
      "source": [
        "##### Entrenamiento red 3.11\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 2\n",
        "* Optimizador: AdamW(lr = 0.00175, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rj0BcAcq-dA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b8fbf0-6a06-4e52-f498-682d7cc7d4f6"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.547 | Train f1: 0.60 | Train precision: 0.70 | Train recall: 0.55\n",
            "\t Val. Loss: 0.358 |  Val. f1: 0.74 |  Val. precision: 0.77 | Val. recall: 0.72\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.234 | Train f1: 0.83 | Train precision: 0.84 | Train recall: 0.82\n",
            "\t Val. Loss: 0.343 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.137 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.356 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.088 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.411 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.064 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.76 |  Val. precision: 0.81 | Val. recall: 0.73\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.048 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.487 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0RkSajb1dnp"
      },
      "source": [
        "##### Entrenamiento red 3.12\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: AdamW(lr = 0.00175, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnIe5sGo1qs_",
        "outputId": "6c9e255e-0264-4180-c805-a1cbf816a71f"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.586 | Train f1: 0.58 | Train precision: 0.68 | Train recall: 0.53\n",
            "\t Val. Loss: 0.368 |  Val. f1: 0.74 |  Val. precision: 0.77 | Val. recall: 0.72\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.253 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.335 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.149 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.353 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.103 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
            "\t Val. Loss: 0.407 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.073 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.451 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.054 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.519 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWftY8CA1mRV"
      },
      "source": [
        "##### Entrenamiento red 3.13\n",
        "* DROPOUT: 0.2\n",
        "* Capas: 3\n",
        "* Optimizador: AdamW(lr = 0.0018, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvaj5e2R1rUc",
        "outputId": "8b08d736-9763-49d5-81f9-da617306b18d"
      },
      "source": [
        "patience = 5\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.575 | Train f1: 0.59 | Train precision: 0.69 | Train recall: 0.54\n",
            "\t Val. Loss: 0.385 |  Val. f1: 0.71 |  Val. precision: 0.75 | Val. recall: 0.68\n",
            "Contador early stopping en 1 de 5\n",
            "Epoch: 02 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.250 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.343 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Contador early stopping en 2 de 5\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.148 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.354 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
            "Contador early stopping en 3 de 5\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.101 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.417 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.73\n",
            "Contador early stopping en 4 de 5\n",
            "Epoch: 05 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.073 | Train f1: 0.95 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.446 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Contador early stopping en 5 de 5\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.056 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.461 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTpn4PCZ1od9"
      },
      "source": [
        "#### Entrenamiento red4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtlw0ZvorA_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd2480f-5c7e-4c6a-dc7f-232206ac48c8"
      },
      "source": [
        "patience = 10\n",
        "delta = 0.1\n",
        "ES_training(patience, delta)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.992 | Train f1: 0.29 | Train precision: 0.48 | Train recall: 0.21\n",
            "\t Val. Loss: 0.792 |  Val. f1: 0.43 |  Val. precision: 0.64 | Val. recall: 0.33\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.800 | Train f1: 0.43 | Train precision: 0.61 | Train recall: 0.34\n",
            "\t Val. Loss: 0.688 |  Val. f1: 0.50 |  Val. precision: 0.67 | Val. recall: 0.40\n",
            "Contador early stopping en 1 de 10\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.721 | Train f1: 0.49 | Train precision: 0.66 | Train recall: 0.40\n",
            "\t Val. Loss: 0.628 |  Val. f1: 0.53 |  Val. precision: 0.68 | Val. recall: 0.44\n",
            "Contador early stopping en 2 de 10\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.668 | Train f1: 0.52 | Train precision: 0.68 | Train recall: 0.43\n",
            "\t Val. Loss: 0.589 |  Val. f1: 0.56 |  Val. precision: 0.74 | Val. recall: 0.46\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.627 | Train f1: 0.56 | Train precision: 0.70 | Train recall: 0.47\n",
            "\t Val. Loss: 0.566 |  Val. f1: 0.57 |  Val. precision: 0.75 | Val. recall: 0.47\n",
            "Contador early stopping en 3 de 10\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.598 | Train f1: 0.57 | Train precision: 0.71 | Train recall: 0.49\n",
            "\t Val. Loss: 0.554 |  Val. f1: 0.59 |  Val. precision: 0.70 | Val. recall: 0.52\n",
            "Contador early stopping en 4 de 10\n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.572 | Train f1: 0.59 | Train precision: 0.71 | Train recall: 0.51\n",
            "\t Val. Loss: 0.535 |  Val. f1: 0.60 |  Val. precision: 0.73 | Val. recall: 0.52\n",
            "Contador early stopping en 5 de 10\n",
            "Epoch: 08 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.552 | Train f1: 0.61 | Train precision: 0.73 | Train recall: 0.53\n",
            "\t Val. Loss: 0.520 |  Val. f1: 0.61 |  Val. precision: 0.73 | Val. recall: 0.54\n",
            "Contador early stopping en 6 de 10\n",
            "Epoch: 09 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.531 | Train f1: 0.62 | Train precision: 0.74 | Train recall: 0.55\n",
            "\t Val. Loss: 0.516 |  Val. f1: 0.62 |  Val. precision: 0.74 | Val. recall: 0.54\n",
            "Contador early stopping en 7 de 10\n",
            "Epoch: 10 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.514 | Train f1: 0.63 | Train precision: 0.74 | Train recall: 0.56\n",
            "\t Val. Loss: 0.494 |  Val. f1: 0.63 |  Val. precision: 0.72 | Val. recall: 0.57\n",
            "Contador early stopping en 8 de 10\n",
            "Epoch: 11 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.495 | Train f1: 0.64 | Train precision: 0.75 | Train recall: 0.57\n",
            "\t Val. Loss: 0.497 |  Val. f1: 0.63 |  Val. precision: 0.70 | Val. recall: 0.58\n",
            "Contador early stopping en 9 de 10\n",
            "Epoch: 12 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.479 | Train f1: 0.66 | Train precision: 0.75 | Train recall: 0.59\n",
            "\t Val. Loss: 0.483 |  Val. f1: 0.64 |  Val. precision: 0.71 | Val. recall: 0.58\n",
            "Contador early stopping en 10 de 10\n",
            "----- EARLY STOPPING EJECUTADO -----\n",
            " \n",
            "Epoch: 13 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.468 | Train f1: 0.67 | Train precision: 0.76 | Train recall: 0.60\n",
            "\t Val. Loss: 0.478 |  Val. f1: 0.64 |  Val. precision: 0.75 | Val. recall: 0.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Sgpfl_lgMk"
      },
      "source": [
        "### Testeo y resultados finales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqXIYyBHkIFW"
      },
      "source": [
        "#### Evaluamos el set de validación con el modelo final\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluación con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27CNYfrjtQ-"
      },
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAYgwmTTkIFX"
      },
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF1ysw_Kw6zz"
      },
      "source": [
        "\n",
        "#### Predecir datos para la competencia\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, predeciremos las etiquetas que serán evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RBs3UU4wLk3"
      },
      "source": [
        "def predict_labels(model, iterator, criterion, fields=fields):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0][1]\n",
        "    nertags_field = fields[1][1]\n",
        "    tags_vocab = nertags_field.vocab.itos\n",
        "    words_vocab = text_field.vocab.itos\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text_batch = batch.text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(batch.text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oración predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<pad>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "                predictions.append(['EOS', 'EOS'])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model, test_iterator, criterion)"
      ],
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwQp1Ru8Oht8"
      },
      "source": [
        "#### Generar el archivo para la submission\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPfZkjJGkWyq"
      },
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for i, (word, tag) in enumerate(predictions[:-1]):\n",
        "    if word=='EOS' and tag=='EOS': f.write('\\n')\n",
        "    else: \n",
        "      if i == len(predictions[:-1])-1:\n",
        "        f.write(word + ' ' + tag)\n",
        "      else: f.write(word + ' ' + tag + '\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRcDAFXK48XL"
      },
      "source": [
        "### Resultados notables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWBFWggx4_wu"
      },
      "source": [
        "Los entrenamientos de las redes 3.6, 3.7 y 3.10 fueron los de mejor rendimiento en cuanto a métricas y loss se refiere. Acá una tabla comparativa:\n",
        "\n",
        "| Modelo | Valid Loss | F1   | Precision | Recall |\n",
        "|--------|------------|------|-----------|--------|\n",
        "| 3.6    | 0.333      | 0.77 | 0.80      | 0.74   |\n",
        "| 3.7    | 0.344      | 0.77 | 0.80      | 0.74   |\n",
        "| 3.10   | 0.343      | 0.77 | 0.79      | 0.75   |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZEWJXrNaSIf"
      },
      "source": [
        "## Conclusiones\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtK7y43V7Z_"
      },
      "source": [
        "La competencia se presentó acompañada de un abánico de dificultades por superar, que permitieron al grupo aprender sostenidamente en el período de su realización.\n",
        "\n",
        "Para extender lo dicho en el apartado de diseño \n",
        "experimental, el ajuste de aspectos como cantidad de capas, dimensiones, número de neuronas, valor del dropout, entre otros, comprender en profundidad el proceso computacional se tornó relativamente críptico y complicado. Se pudo observar cómo estos parámetros variaban los resultados, pero costó comprender las razones que generaron dichas distinciones, dado que los estudiantes del grupo aún no poseen el conocimiento necesario ni la experiencia requerida para diferenciar eso en mayor detalles.\n",
        "\n",
        "El párrafo anterior podrá ser desalentador, pero en realidad es una muestra de lo extenso que es el mundo de NLP, aprendizaje de máquinas y deep learning. Esto motiva a los estudiantes del grupo para continuar explorando estas disciplinas. \n",
        "\n",
        "Ahondando en los experimentos y resultados, a partir de las pruebas realizadas:\n",
        "\n",
        "- El entrenamiento de la red 2 podría haberse extendido con más iteraciones. Si bien su recall terminaba estancándose, existe la posibilidad de que, al ajustar otros parámetros y añadirle más configuraciones, podría haber arrojado mejores resultados.\n",
        "\n",
        "- Pareciera que los embeddings utilizados no mejoraban los modelos, pero se cree que esto se debe a la falta de experiencia manejándolos y no a un caso donde éstos fueran perjudiciales para el procesamiento y aprendizaje de los modelos.\n",
        "\n",
        "- GRU aportó mejores resultados que LSTM en este caso. Un dropout de 0.2 era un punto adecuado,en general, para resultados estables.\n",
        "\n",
        "- Las redes neuronales bidireccionales mostraron mejores resultados en los ensayos preliminares, notando su capacidad superior en varios casos.\n",
        "\n",
        "- El optimizador Adam y sus variantes tuvieron mejor desempeño de manera general, pero ajustar sus hiperparámetros fue complejo de entender. Se considera que eso se aprendió someramente, requiriendo estudios más profundos para comprenderlo mejor. Se trabajó principalmente con ensayo y error.\n",
        "\n",
        "- La técnica de early stopping fue de suma utilidad para prevenir overfitteos. Aquello fue bastante perjudicial en el trabajo de la competencia 1.\n",
        "\n",
        "Finalizando, las mejores a futuro y trabajos propuestos:\n",
        "\n",
        "- Utilizar herramientas que automáticamente optimicen los hiperparáemtros de los modelos, en lugar de hacerlo por tanteo o intuición manualmente.\n",
        "\n",
        "- Implementar una herramienta de learning rate más allá del parámetro que permiten los optimizadores. Esto podría haber funcionado con el entrenamiento de la red 2, porque sus losses de training y validación no alcanzaron a divergir en las epoch propuestas. Se piensa que el modelo podría haber continuado aprendiendo y mejorando sin overfittearse. \n",
        "\n",
        "- Estudiar la implementación adecuada de capas adicionales en las arquitecturas de redes neuronales y ver sus efectos en el aprendizaje de los modelos.\n",
        "\n",
        "- Probar más funciones de loss.\n"
      ]
    }
  ]
}