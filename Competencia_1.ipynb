{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Competencia_1_entrega.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:49:08.174519Z",
          "start_time": "2020-03-31T13:49:08.165989Z"
        },
        "id": "gpbvNOH0zvIi"
      },
      "source": [
        "# **Competencia 1 - CC6205 Natural Language Processing üìö**\n",
        "\n",
        "**Integrantes: Vicente Ardiles y Rodrigo Oportot**\n",
        "\n",
        "**Usuario del equipo en CodaLab: NLPachi**\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** Mi√©rcoles 12 de mayo.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n: 25hrs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT7ZpVRuzGAF"
      },
      "source": [
        "# **Entregable.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:34:25.683540Z",
          "start_time": "2020-03-31T13:34:25.673430Z"
        },
        "id": "E29LEMZ9zvIo"
      },
      "source": [
        "## **1. Introducci√≥n**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W20NnoduzvIo"
      },
      "source": [
        "De manera general, el problema a resolver consiste en clasificar la intensidad de un sentimiento relacionado a un tweet mediante el uso de herramientas asociadas a la disciplina de NLP. Una formalizaci√≥n concreta de la *task* en cuesti√≥n es:\n",
        "\n",
        "- Clasificar la intensidad de un sentimiento en un conjunto de tweets conociendo dicho sentimiento de antemano.\n",
        "\n",
        "La task pertenece al mundo de clasificaci√≥n de texto. \n",
        "\n",
        "Los inputs del problema tienen la siguiente estructura: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRpFyqZ9UaPX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "ca5c123c-1b13-4973-9806-76ed4c709758"
      },
      "source": [
        "train['anger'].sample(1)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>10576</td>\n",
              "      <td>@ticcikasie1 With a frown, she let's out a dis...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... sentiment_intensity\n",
              "576  10576  ...              medium\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xolnpEIUcPk"
      },
      "source": [
        "El archivo que contiene los datos est√° en formato CSV y es le√≠do utilizando la librer√≠a Pandas, en donde se encuentran 4 columnas indicando el id del tweet, su contenido, el sentimiento asociado y la intensidad de este √∫ltimo. En particular, hay cuatro sentimientos y tres intensidades para cada uno, siendo:\n",
        "\n",
        "- Sentimientos: **anger, fear, joy y sadness**\n",
        "- Intensidad: **low, medium y high**\n",
        "\n",
        "La pipeline encargada de resolver el problema toma el set de entrenamiento y luego el de testeo (tambi√©n hay casos donde hubo set de entrenamiento, validaci√≥n y finalmente testeo). Para entrenar al clasificador, esta estructura se apoya en un tokenizador, un vectorizador y en varias features descritas en las secciones posteriores del reporte. Finalmente, se obtienen los valores para cada m√©trica de an√°lisis sobre el nuevo etiquetado producido por el clasificador.\n",
        "\n",
        "A primera vista, el corpus a trabajar presenta ciertos desaf√≠os como la manera de manejar los emoticones, ya que esto puede significar muchas cosas: contarlos, catalogarlos y asociarlos a ciertos sentimientos, asignarles una ponderaci√≥n para representar tal sentimiento, etc. Por otro lado, est√° el tratamiento de los hashtags \"#\", pudiendo estos aportar de manera concreta al c√°lculo de clasificaci√≥n de intensidad en base a su contenido.\n",
        "\n",
        "Por un lado de car√°cter subyacente, otras dificultades por afrontar contemplan la creaci√≥n de features especiales para la task, as√≠ como la elecci√≥n particular de ellas al momento de experimentar con la pipeline, buscando una combinaci√≥n adecuada. A causa de la maldici√≥n de la dimensionalidad, colocar todas las features de una vez en un solo experimento (incluso con reducci√≥n, por ejemplo con PCA), podr√≠a resultar contraproducente, entorpeciendo la correcta clasificaci√≥n descrita en la task. \n",
        "\n",
        "Para finalizar este apartado, la metodolog√≠a de desarrollo consiste en programar las features en coherencia con el corpus y la task, pre-procesar el corpus tokenizando de manera personalizada apoy√°ndose en vectorizadores de sklearn y los distintos m√≥dulos de nltk y, finalmente, tomar tres algoritmos de clasificaci√≥n (clasificadores Naive Bayes, SVM y Regresi√≥n Log√≠stica) para introducir en la pipeline, con el fin de analizar el desempe√±o del conjunto elegido de herramientas en base a los valores que tomen las m√©tricas de evaluaci√≥n descritas m√°s adelante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:13.474238Z",
          "start_time": "2020-03-31T13:47:13.454068Z"
        },
        "id": "OTAIEnSJzvIp"
      },
      "source": [
        "## **2. Representaciones**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:17.719268Z",
          "start_time": "2020-03-31T13:47:17.709207Z"
        },
        "id": "EV1qBv-MzvIp"
      },
      "source": [
        "Para representar los tweets de manera num√©rica, se utilizaron dos tipos de *vectorizers*: CountVectorizer y TfidfVectorizer, convirtiendo a matrices num√©ricas los tweets, junto con un tokenizador personalizado que los pre-procesa mediante:\n",
        "\n",
        "- TweetTokenizer(strip_handles=True, reduce_len=True), proveniente de la librer√≠a nltk, encargado de tokenizar eliminando s√≠mbolos como @ y s√≠mbolos ([. ! ?] entre otros) repetidos m√°s de tres veces.\n",
        "\n",
        "- mark_negation() de nltk, que marca las palabras despu√©s de una negaci√≥n con un tag \"_Neg\", para conservar su contexto al quitar stopwords posteriormente.\n",
        "\n",
        "- remStopWords() con las stopwords de nlkt, removiendo las stopwords.\n",
        "\n",
        "- hashtagToWord(), que quita el s√≠mbolo # de un hashtag, dejando solamente la palabra (esta decisi√≥n se explicar√° m√°s adelante, pero en resumen, se obtienen mejores resultados de esta manera).\n",
        "\n",
        "- stemmize() con SnowballStemmer de nltk.\n",
        "\n",
        "- lemmatize() con WordNetLemmatizer de nltk.\n",
        "\n",
        "Como se mencion√≥ anteriormente, todas estas funciones son parte de my_tokenizer(text), a√±adida como par√°metro a los vectorizers por utilizar. \n",
        "\n",
        "Se crearon y utilizaron diversas features mediante los transformers de sklearn:\n",
        "\n",
        "- DotsMarksCountTransformer, que cuenta en un tweet la cantidad de veces que los s√≠mbolos [. ! ?] se repiten, al menos, 3 veces consecutivamente.\n",
        "\n",
        "- UppercaseCountTransformer, contando la cantidad de palabras que est√°n completamente en may√∫sculas y son de largo mayor a 1.\n",
        "\n",
        "- ElongatedWordsCountTransformer, contando la cantidad de palabras que tienen al menos 3 letras repetidas de manera consecutiva.\n",
        "\n",
        "- NegationCountTransformer, contando la cantidad de palabras con la etiqueta \"_NEG\" a causa de pre-procesar el tweet con mark_negation() de nltk.\n",
        "\n",
        "- CharsCountTransformer, la feature que ven√≠a con el c√≥digo base, contando el n√∫mero de s√≠mbolos [# ! ? @] en u tweet.\n",
        "\n",
        "- LexiconCountTransformer(emotion), feature m√°s compleja que calcula el n√∫mero de lexicons asociado al tipo de sentimiento de un tweet, junto con calcular el promedio del valor que representa la intensidad de dicha emoci√≥n. Por ejemplo, si un tweet proveniente del dataset angry tuviera un lexicon A asociado a angry con una ponderaci√≥n de 0.8 en relaci√≥n a la intensidad, y tambi√©n tuviera un lexicon B asociado a angry con una intensidad de 0.2, la feature entregar√≠a [2 (0.8 + 0.2)/2] como resultado. Dado que hay valores negativos para la intensidad de emoci√≥n en los lexicons (extra√≠dos de SenticNet5), se toma el valor absoluto de ellos. \n",
        "\n",
        "- EmojiCount, contando la cantidad de emojis por tweet\n",
        "\n",
        "- Doc2VecTransformer, feature analogada del auxiliar de words embedding para extraer las mismas de los tweets a analizar, trabajando con la librer√≠a gensim.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMOjYSQezvIq"
      },
      "source": [
        "## **3. Algoritmos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bPiFs33zilS"
      },
      "source": [
        "Se utilizaron los algoritmos de Naive Bayes, Support Vector Machine (SVM) lineal y Logistic Regression. \n",
        "\n",
        "Naive Bayes (MultinomialNB) es un algoritmo basado en el teorema de Bayes,  que asume independencia entre las features asociadas al momento de clasificar, simplificando los c√°lculos. Es por esta raz√≥n que no es posible utilizar features de n-gramas en este caso, ya que romper√≠a con la suposici√≥n inicial. Cabe destacar que, si se tomara un enfoque m√°s detallista, la suposici√≥n de independencia no ser√≠a cierta dependiendo del corpus por analizar.\n",
        "\n",
        "Este algoritmo emplea el teorema de Bayes para asignarle a cada documento del corpus la probabilidad de pertenecer a una clase dada. Sin entrar en detalles matem√°ticos, el algoritmo revisa los componentes tokenizados, viendo la probabilidad de que cada palabra pertenezca o no a cada clase. No importa el orden de palabras en este algoritmo, pudiendo usar como tokenizador una simple bag of words (en el caso m√°s sencillo). \n",
        "\n",
        "El segundo clasificador utilizado fue SVM lineal. Matem√°ticamente es complejo de detallar, pero de manera llana, se encarga de buscar el hiperplano √≥ptimo para separar los conjuntos de datos en las clases por clasificar dentro de $\\mathbb{R}^{2}$.\n",
        "\n",
        "Por √∫ltimo, Logistic Regression es un modelo de clasificaci√≥n lineal donde las probabilidades para un posible resultado son modelas usando una funci√≥n log√≠stica. Dada su complejidad, tampoco se entrar√° en detalle su funcionamiento espec√≠fico. Sin embargo, se puede especificar que los resultados dependen de variables que se modelan binariamente. Lo anterior implica varios supuestos, como el hecho de que las variables sean binarias, sumado a que deben ser linealmente independientes entre s√≠. En este caso, se us√≥ la extensi√≥n multinomial del clasificador para predecir la intensidad del sentimiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:52.064631Z",
          "start_time": "2020-03-31T13:47:52.044451Z"
        },
        "id": "ECjkdgdwzvIq"
      },
      "source": [
        "## **4. M√©tricas de Evaluaci√≥n**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6eHJdHBzvIr"
      },
      "source": [
        "Para evaluar el desempe√±o de la soluci√≥n, se utilizan las siguientes m√©tricas:\n",
        "\n",
        "- AUC: El √°rea bajo la curva ROC, donde toma el valor 1 si es un clasificador perfecto y 0.5 si es uno aleatorio. La curva ROC grafica la proporci√≥n de verdaderos positivos (recall) versus la proporci√≥n de falsos positivos. \n",
        "\n",
        "- Kappa: En casos donde una task de clasificaci√≥n deba trabajar con datos desbalanceados en favor de una clase mayoritaria, esta m√©trica trata esto normalizando la accuracy de clasificaci√≥n en base a tal desbalance. Un clasificador que siempre est√° correcto tendr√° Kappa igual a 1, mientras que un clasificador que acierte con la misma probabilidad que uno aleatorio, tendr√° un valor igual a 0.\n",
        "\n",
        "- Accuracy: Cantidad de documentos clasificados correctamente vs la cantidad total de documentos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJyTrr2onLOo"
      },
      "source": [
        "## **5. Dise√±o experimental**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnhEwjh_nP9M"
      },
      "source": [
        "### **5.1. Dise√±os de prueba y ensayos**\n",
        "\n",
        "Previo a comentar la metodolog√≠a experimental, es necesario destacar los ensayos de prueba que permitieron dise√±ar los experimentos finales. Los resultados de √©stos fueron analizados en el momento, de tal manera que sirvieran como una gu√≠a que encaminara el desarrollo experimental.\n",
        "\n",
        "- En primera instancia se prob√≥ con Oversampling solamente sobre las muestras de training, pero m√°s adelante se descart√≥ ya que se obtuvieron clasificadores overfitteados para todos los modelos, con resultados sumamente optimista por todas las m√©tricas de evaluaci√≥n. Se not√≥ que las particiones de training y testeo segu√≠an desbalanceadas cuando el clasificador las recib√≠a con fit. Para solucionar esto, se volvi√≥ a trabajar con el dataset original desbalanceado, bajo la premisa de que, para la clasificaci√≥n, es m√°s sencillo catalogar los casos extremos (high y low) que los de medium, decidiendo transmitir ese sesgo a trav√©s del modelo, ya que un sentimiento neutro parece ser m√°s dif√≠cil de detectar, a pesar de tener m√°s muestras de √©l. La premisa es cuestionable, obviamente, pero tomarla como referencia ayud√≥ a obtener resultados posteriores mucho mejores que en un comienzo. Esto es algo que podr√≠a volver a discutirse en el futuro. \n",
        "\n",
        "- A su vez, el modelo MultinomialNB fue descartado posteriormente (a pesar de experimentar con √©l), ya que sus resultados eran inferiores a los producidos por SVM y Logistic Regression (LR por abreviar). \n",
        "\n",
        "- Las features de UpperCase, ElongatedWords y EmojiCount fueron descartadas ya que empeoraban el desempe√±o de los clasificadores en todos los casos.\n",
        "\n",
        "- Se decidi√≥ no utilizar la funci√≥n que remueve stopwords en el tokenizador, ya que despu√©s de varios ensayos, se not√≥ que afectaba negativamente el desempe√±o de los clasificadores.\n",
        "\n",
        "- La feature de Lexicons a veces arrojaba errores con otras features. Fue un problema dif√≠cil de abordar, por lo que se decidi√≥, en mayor medida, descartar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2HpvSe11gPZ"
      },
      "source": [
        "### 5.2. Dise√±o experimental\n",
        "\n",
        "\n",
        "\n",
        "La metodolog√≠a experimental consisti√≥ en tomar variables de control en las pipelines construidas, para comparar diversos modelos, tokenizers y features a la hora de entrenar los modelos. \n",
        "\n",
        "Cabe destacar que los siguientes experimentos fueron realizados con el mismo tokenizador *my_tokenizer(text)*, con las funciones descritas en el apartado 2 de Representaciones, expecto la de stopwords, ya que arrojaba resultados inferiores en los intentos de prueba.\n",
        "\n",
        "- Experimento 1:\n",
        "Modelo LR con TfidfCountVectorizer(tokenizer=my_tokenizer) y las features chars_count, NegationCount, WordEmbeddings. \n",
        "\n",
        "- Experimento 2:\n",
        "Modelo LR con CountVectorizer(ngram=(1,3), max_features=9000, tokenizer=my_tokenizer) y las features chars_count, NegationCount, WordEmbeddings.\n",
        "\n",
        "- Experimento 3:\n",
        "Modelo SVC con CountVectorizer(ngram=(1,3), max_features=9000, tokenizer=my_tokenizer) y las features chars_count, NegationCount, WordEmbeddings.\n",
        "\n",
        "- Experimento 4:\n",
        "Modelo SVC con TfidfCountVectorizer(tokenizer=my_tokenizer) y las features chars_count, NegationCount, WordEmbeddings.\n",
        "\n",
        "- Experimento 5:\n",
        "Modelo LR con TfidfCountVectorizer(tokenizer=my_tokenizer) y las features chars_count, NegationCount, WordEmbeddings y Lexicons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX5Ib_pCzvIr"
      },
      "source": [
        "## **6. Experimentos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:31:40.023344Z",
          "start_time": "2020-03-31T13:31:40.003541Z"
        },
        "id": "aK24MJ8jzvIr"
      },
      "source": [
        "### Importar librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.587160Z",
          "start_time": "2020-04-07T15:44:19.319386Z"
        },
        "id": "FukgFUTUzvIs"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FevBPus0zvIs"
      },
      "source": [
        "### Definir m√©todos de evaluaci√≥n (**NO tocar este c√≥digo**)\n",
        "\n",
        "Estas funciones est√°n a cargo de evaluar los resultados de la tarea. No deber√≠an cambiarlas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.604066Z",
          "start_time": "2020-04-07T15:44:20.589106Z"
        },
        "id": "9wlllV7PzvIs"
      },
      "source": [
        "def auc_score(test_set, predicted_set):\n",
        "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
        "    medium_predicted = np.array(\n",
        "        [prediction[1] for prediction in predicted_set])\n",
        "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
        "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
        "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
        "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
        "    auc_high = roc_auc_score(high_test, high_predicted)\n",
        "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
        "    auc_low = roc_auc_score(low_test, low_predicted)\n",
        "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
        "             high_test.sum() * auc_high) / (\n",
        "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
        "    return auc_w\n",
        "\n",
        "\n",
        "def evaluate(predicted_probabilities, y_test, labels, dataset_name):\n",
        "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
        "    # entregar el arreglo de clases aprendido por el clasificador.\n",
        "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "\n",
        "\n",
        "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
        "    print(\n",
        "        confusion_matrix(y_test,\n",
        "                         predicted_labels,\n",
        "                         labels=['low', 'medium', 'high']))\n",
        "\n",
        "    print('\\nClassification Report:\\n')\n",
        "    print(\n",
        "        classification_report(y_test,\n",
        "                              predicted_labels,\n",
        "                              labels=['low', 'medium', 'high']))\n",
        "\n",
        "    # Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "    \n",
        "    predicted_probabilities = predicted_probabilities[:, [\n",
        "        labels.index('low'),\n",
        "        labels.index('medium'),\n",
        "        labels.index('high')\n",
        "    ]]\n",
        "    \n",
        "    \n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    #print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    #print(\"Kappa:\", kappa, end='\\t')\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    #print(\"Accuracy:\", accuracy)\n",
        "    #print('------------------------------------------------------\\n')\n",
        "    return np.array([auc, kappa, accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOP6ugwzvIt"
      },
      "source": [
        "### Datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.068137Z",
          "start_time": "2020-04-07T15:44:20.606061Z"
        },
        "id": "D1XhFPhrzvIt"
      },
      "source": [
        "# Datasets de entrenamiento.\n",
        "train = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
        "}\n",
        "\n",
        "for key, value in train.items():\n",
        "    value.rename(columns={'class': 'sentiment'}, inplace=True)\n",
        "\n",
        "# Datasets que deber√°n predecir para la competencia.\n",
        "target = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
        "}\n",
        "\n",
        "for key, value in target.items():\n",
        "    value.rename(columns={'class': 'sentiment'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.088707Z",
          "start_time": "2020-04-07T15:44:21.069757Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "flg2Zw2mzvIt",
        "outputId": "5f341db6-ab0c-4602-e13f-c5c586903783"
      },
      "source": [
        "# Ejemplo de algunas filas aleatorias del dataset etiquetado:\n",
        "train['anger'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>10375</td>\n",
              "      <td>@cc_yandian @HillaryClinton her team must draw...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>10449</td>\n",
              "      <td>Hey folks sorry if anything offensive got post...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>10302</td>\n",
              "      <td>ESPN just assumed I wanted their free magazine...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>10216</td>\n",
              "      <td>I don't talk about politics because people now...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>10363</td>\n",
              "      <td>@laura221b I've left it for my dad to deal wit...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... sentiment_intensity\n",
              "375  10375  ...              medium\n",
              "449  10449  ...              medium\n",
              "302  10302  ...              medium\n",
              "216  10216  ...              medium\n",
              "363  10363  ...              medium\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "XB7hb7KH2DFK",
        "outputId": "55d6cdf7-1028-4f2b-8c65-4a89659f2084"
      },
      "source": [
        "# Ejemplo de algunas filas aleatorias del dataset no etiquetado\n",
        "target['anger'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>11332</td>\n",
              "      <td>@RadioX @ChrisMoyles wow. not heard this in fo...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>11209</td>\n",
              "      <td>i resent being tired and annoyed that the word...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>11174</td>\n",
              "      <td>@thehill no wonder USA is going to shit with s...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647</th>\n",
              "      <td>11588</td>\n",
              "      <td>And Republicans, you, namely Graham, Flake, Sa...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>11438</td>\n",
              "      <td>@victoriarghhh not mad but tilting? slightly i...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... sentiment_intensity\n",
              "391  11332  ...                 NaN\n",
              "268  11209  ...                 NaN\n",
              "233  11174  ...                 NaN\n",
              "647  11588  ...                 NaN\n",
              "497  11438  ...                 NaN\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5aNqEfVzvIv"
      },
      "source": [
        "### Analizar los datos \n",
        "\n",
        "En esta secci√≥n analizaremos el balance de los datos. Para ello se imprime la cantidad de tweets de cada dataset agrupados por la intensidad de sentimiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.117633Z",
          "start_time": "2020-04-07T15:44:21.090703Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5007JRgzvIv",
        "outputId": "8ae696a7-3ed5-46b6-8bfb-94b9a6d3ac1a"
      },
      "source": [
        "for dataset_name in train:\n",
        "    print(f'Dataset: {dataset_name} \\n', train[dataset_name].groupby(['sentiment_intensity']).size())\n",
        "    print('----------------------------------------------------------\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: anger \n",
            " sentiment_intensity\n",
            "high      163\n",
            "low       161\n",
            "medium    617\n",
            "dtype: int64\n",
            "----------------------------------------------------------\n",
            "\n",
            "Dataset: fear \n",
            " sentiment_intensity\n",
            "high      270\n",
            "low       288\n",
            "medium    699\n",
            "dtype: int64\n",
            "----------------------------------------------------------\n",
            "\n",
            "Dataset: joy \n",
            " sentiment_intensity\n",
            "high      195\n",
            "low       219\n",
            "medium    488\n",
            "dtype: int64\n",
            "----------------------------------------------------------\n",
            "\n",
            "Dataset: sadness \n",
            " sentiment_intensity\n",
            "high      197\n",
            "low       210\n",
            "medium    453\n",
            "dtype: int64\n",
            "----------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD0pHbyV0Rzg"
      },
      "source": [
        "Undersampling que se almacena en una nueva variable en caso de ser usado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xcvlNzWZ6S-"
      },
      "source": [
        "#Under-sampling\n",
        "#anger\n",
        "under_anger = pd.DataFrame(train['anger'])\n",
        "MIN_ANGER = np.min(under_anger.sentiment_intensity.value_counts().values)\n",
        "\n",
        "a = train['anger'].groupby('sentiment_intensity')\n",
        "under_anger = pd.DataFrame(a.apply(lambda x: x.sample(MIN_ANGER).reset_index(drop=True))).reset_index(\n",
        "        drop=True)\n",
        "under_anger.sentiment_intensity.value_counts()\n",
        "\n",
        "#fear\n",
        "under_fear = pd.DataFrame(train['fear'])\n",
        "MIN_FEAR = np.min(under_fear.sentiment_intensity.value_counts().values)\n",
        "\n",
        "a = train['fear'].groupby('sentiment_intensity')\n",
        "under_fear = pd.DataFrame(a.apply(lambda x: x.sample(MIN_FEAR).reset_index(drop=True))).reset_index(\n",
        "        drop=True)\n",
        "under_fear.sentiment_intensity.value_counts()\n",
        "\n",
        "#joy\n",
        "under_joy = pd.DataFrame(train['joy'])\n",
        "MIN_JOY = np.min(under_joy.sentiment_intensity.value_counts().values)\n",
        "\n",
        "a = train['joy'].groupby('sentiment_intensity')\n",
        "under_joy = pd.DataFrame(a.apply(lambda x: x.sample(MIN_JOY).reset_index(drop=True))).reset_index(\n",
        "        drop=True)\n",
        "under_joy.sentiment_intensity.value_counts()\n",
        "\n",
        "#sadness\n",
        "under_sadness = pd.DataFrame(train['sadness'])\n",
        "MIN_SADNESS = np.min(under_sadness.sentiment_intensity.value_counts().values)\n",
        "\n",
        "a = train['sadness'].groupby('sentiment_intensity')\n",
        "under_sadness = pd.DataFrame(a.apply(lambda x: x.sample(MIN_SADNESS).reset_index(drop=True))).reset_index(\n",
        "        drop=True)\n",
        "under_sadness.sentiment_intensity.value_counts()\n",
        "\n",
        "train_undersampling = {\n",
        "    'anger': under_anger,\n",
        "    'fear': under_fear,\n",
        "    'joy': under_joy,\n",
        "    'sadness': under_sadness\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d90RkTgt0TSq"
      },
      "source": [
        "Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYgRd7FHcqA7"
      },
      "source": [
        "#Over-sampling\n",
        "#anger\n",
        "over_anger = pd.DataFrame(train['anger'])\n",
        "MAX_ANGER = np.max(over_anger.sentiment_intensity.value_counts().values)\n",
        "\n",
        "a = train['anger'].groupby('sentiment_intensity')\n",
        "over_anger = pd.DataFrame(a.apply(lambda x: x.sample(MAX_ANGER, replace=True).reset_index(drop=True))).reset_index(\n",
        "        drop=True)\n",
        "\n",
        "#fear\n",
        "over_fear = pd.DataFrame(train['fear'])\n",
        "MAX_FEAR = np.max(over_fear.sentiment_intensity.value_counts().values)\n",
        "\n",
        "a = train['fear'].groupby('sentiment_intensity')\n",
        "over_fear = pd.DataFrame(a.apply(lambda x: x.sample(MAX_FEAR, replace=True).reset_index(drop=True))).reset_index(\n",
        "        drop=True)\n",
        "\n",
        "#joy\n",
        "over_joy = pd.DataFrame(train['joy'])\n",
        "MAX_JOY = np.max(over_joy.sentiment_intensity.value_counts().values)\n",
        "\n",
        "a = train['joy'].groupby('sentiment_intensity')\n",
        "over_joy = pd.DataFrame(a.apply(lambda x: x.sample(MAX_JOY, replace=True).reset_index(drop=True))).reset_index(\n",
        "        drop=True)\n",
        "\n",
        "#sadness\n",
        "over_sadness = pd.DataFrame(train['sadness'])\n",
        "MAX_SADNESS = np.max(over_sadness.sentiment_intensity.value_counts().values)\n",
        "\n",
        "a = train['sadness'].groupby('sentiment_intensity')\n",
        "over_sadness = pd.DataFrame(a.apply(lambda x: x.sample(MAX_SADNESS, replace=True).reset_index(drop=True))).reset_index(\n",
        "        drop=True)\n",
        "\n",
        "train_oversampling = {\n",
        "    'anger': over_anger,\n",
        "    'fear': over_fear,\n",
        "    'joy': over_joy,\n",
        "    'sadness': over_sadness\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stZ6ig5hzvIv"
      },
      "source": [
        "### Custom Features \n",
        "\n",
        "Features personalizadas implementando nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSdzdz8_2Pu-"
      },
      "source": [
        "#### Chars Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.128600Z",
          "start_time": "2020-04-07T15:44:21.119624Z"
        },
        "id": "tNPB8zc9zvIw"
      },
      "source": [
        "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        num_hashtags = tweet.count('#')\n",
        "        num_exclamations = tweet.count('!')\n",
        "        num_interrogations = tweet.count('?')\n",
        "        num_at = tweet.count('@')\n",
        "        return [num_hashtags, num_exclamations, num_interrogations, num_at]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.145564Z",
          "start_time": "2020-04-07T15:44:21.131593Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCQzsuAbzvIw",
        "outputId": "0f0443f8-e1bb-4943-d389-65876e54cf51"
      },
      "source": [
        "# Veamos que sucede si ejecutamos el transformer\n",
        "sample = train['anger'].sample(5, random_state = 1).tweet\n",
        "sample_features = CharsCountTransformer().transform(sample)\n",
        "\n",
        "# Se puede verificar que el conteo de s√≠mbolos es consistente con el transformer creado.\n",
        "print(f'Tweet original: {sample}')\n",
        "print(f'Features creados: {sample_features}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweet original: 604    @everycolorbot more like every color looks the...\n",
            "527    @thomeagle Just to help maintain and boost our...\n",
            "894    i live and die for mchanzo honeymoon crashing ...\n",
            "195    @RealBD_ @ReyesAverie 47 unarmed blacks killed...\n",
            "422         Drop Snapchat names #bored #snap #swap #pics\n",
            "Name: tweet, dtype: object\n",
            "Features creados: [[2 0 0 1]\n",
            " [1 0 0 1]\n",
            " [0 0 0 0]\n",
            " [0 0 0 2]\n",
            " [4 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hMHUAQg2JFI"
      },
      "source": [
        "#### Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-olWqBi2UnMI"
      },
      "source": [
        "############## word embeddings ######################\n",
        "#tenemos dict de train para esto\n",
        "emb_cont = {}\n",
        "\n",
        "for emotion in ['anger', 'fear', 'joy', 'sadness']:\n",
        "    emb_cont[emotion] = train[emotion].tweet\n",
        "\n",
        "##codigo del aux 2\n",
        "\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# limpiar puntuaciones y separar por tokens.\n",
        "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "\n",
        "def simple_tokenizer(doc, lower=False):\n",
        "    if lower:\n",
        "        tokenized_doc = doc.translate(str.maketrans(\n",
        "            '', '', punctuation)).lower().split()\n",
        "\n",
        "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
        "    tokenized_doc = [\n",
        "        token for token in tokenized_doc if token.lower() not in stopwords\n",
        "    ]\n",
        "    return tokenized_doc\n",
        "\n",
        "clean_cont = {}\n",
        "\n",
        "for emotion in ['anger', 'fear', 'joy', 'sadness']:\n",
        "    clean_cont[emotion] = [simple_tokenizer(doc) for doc in emb_cont[emotion].values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac4BEWySU5Xb",
        "outputId": "b9ed8c4f-58af-4a48-f98b-1ba733c1c447"
      },
      "source": [
        "#phrases gemsin\n",
        "\n",
        "phrases = {}\n",
        "\n",
        "for emotion in ['anger', 'fear', 'joy', 'sadness']:\n",
        "    phrases[emotion] = Phrases(clean_cont[emotion], min_count=10, progress_per=5000) \n",
        "\n",
        "#phraser gemsin\n",
        "bigram_phrases = {}\n",
        "sentences = {}\n",
        "\n",
        "for emotion in ['anger', 'fear', 'joy', 'sadness']:\n",
        "    bigram_phrases[emotion] = Phraser(phrases[emotion])\n",
        "    sentences[emotion] = bigram_phrases[emotion][clean_cont[emotion]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 23:02:44: collecting all words and their counts\n",
            "INFO - 23:02:44: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "INFO - 23:02:44: collected 8012 word types from a corpus of 6447 words (unigram + bigrams) and 941 sentences\n",
            "INFO - 23:02:44: using 8012 counts as vocab in Phrases<0 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
            "INFO - 23:02:44: collecting all words and their counts\n",
            "INFO - 23:02:44: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "INFO - 23:02:44: collected 10696 word types from a corpus of 9099 words (unigram + bigrams) and 1257 sentences\n",
            "INFO - 23:02:44: using 10696 counts as vocab in Phrases<0 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
            "INFO - 23:02:44: collecting all words and their counts\n",
            "INFO - 23:02:44: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "INFO - 23:02:44: collected 7934 word types from a corpus of 6353 words (unigram + bigrams) and 902 sentences\n",
            "INFO - 23:02:44: using 7934 counts as vocab in Phrases<0 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
            "INFO - 23:02:44: collecting all words and their counts\n",
            "INFO - 23:02:44: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "INFO - 23:02:44: collected 8499 word types from a corpus of 6117 words (unigram + bigrams) and 860 sentences\n",
            "INFO - 23:02:44: using 8499 counts as vocab in Phrases<0 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
            "INFO - 23:02:44: source_vocab length 8012\n",
            "INFO - 23:02:44: Phraser built with 0 phrasegrams\n",
            "INFO - 23:02:44: source_vocab length 10696\n",
            "INFO - 23:02:44: Phraser built with 0 phrasegrams\n",
            "INFO - 23:02:44: source_vocab length 7934\n",
            "INFO - 23:02:44: Phraser built with 4 phrasegrams\n",
            "INFO - 23:02:44: source_vocab length 8499\n",
            "INFO - 23:02:44: Phraser built with 0 phrasegrams\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e14vUHvGUqUt",
        "outputId": "89fe25d7-b0ee-4e2d-b877-aba30900ab59"
      },
      "source": [
        "#ahora, el modelo\n",
        "import multiprocessing\n",
        "from time import time \n",
        "\n",
        "models = {}\n",
        "t = time()\n",
        "\n",
        "for emotion in ['anger', 'fear', 'joy', 'sadness']:\n",
        "    models[emotion] = Word2Vec(min_count=5,\n",
        "                      window=4,\n",
        "                      size=200,\n",
        "                      sample=6e-5,\n",
        "                      alpha=0.03,\n",
        "                      min_alpha=0.0007,\n",
        "                      negative=20,\n",
        "                      workers=multiprocessing.cpu_count())\n",
        "    models[emotion].build_vocab(sentences[emotion], progress_per=10000)\n",
        "    models[emotion].train(sentences[emotion], \n",
        "                          total_examples=models[emotion].corpus_count, epochs=15, \n",
        "                          report_delay=10)\n",
        "    models[emotion].init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 23:02:47: collecting all words and their counts\n",
            "INFO - 23:02:47: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO - 23:02:47: collected 3400 word types from a corpus of 6447 raw words and 941 sentences\n",
            "INFO - 23:02:47: Loading a fresh vocabulary\n",
            "INFO - 23:02:47: effective_min_count=5 retains 167 unique words (4% of original 3400, drops 3233)\n",
            "INFO - 23:02:47: effective_min_count=5 leaves 1755 word corpus (27% of original 6447, drops 4692)\n",
            "INFO - 23:02:47: deleting the raw counts dictionary of 3400 items\n",
            "INFO - 23:02:47: sample=6e-05 downsamples 167 most-common words\n",
            "INFO - 23:02:47: downsampling leaves estimated 182 word corpus (10.4% of prior 1755)\n",
            "INFO - 23:02:47: estimated required memory for 167 words and 200 dimensions: 350700 bytes\n",
            "INFO - 23:02:47: resetting layer weights\n",
            "INFO - 23:02:47: training model with 2 workers on 167 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:47: EPOCH - 1 : training on 6447 raw words (156 effective words) took 0.0s, 5698 effective words/s\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:47: EPOCH - 2 : training on 6447 raw words (177 effective words) took 0.0s, 6658 effective words/s\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:47: EPOCH - 3 : training on 6447 raw words (165 effective words) took 0.0s, 6185 effective words/s\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:47: EPOCH - 4 : training on 6447 raw words (178 effective words) took 0.0s, 6494 effective words/s\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:47: EPOCH - 5 : training on 6447 raw words (167 effective words) took 0.0s, 5879 effective words/s\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:47: EPOCH - 6 : training on 6447 raw words (192 effective words) took 0.0s, 5237 effective words/s\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:47: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:47: EPOCH - 7 : training on 6447 raw words (173 effective words) took 0.0s, 5723 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 8 : training on 6447 raw words (190 effective words) took 0.0s, 5745 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 9 : training on 6447 raw words (187 effective words) took 0.0s, 7452 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 10 : training on 6447 raw words (177 effective words) took 0.0s, 7040 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 11 : training on 6447 raw words (211 effective words) took 0.0s, 6256 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 12 : training on 6447 raw words (183 effective words) took 0.0s, 6234 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 13 : training on 6447 raw words (182 effective words) took 0.0s, 5287 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 14 : training on 6447 raw words (217 effective words) took 0.0s, 6371 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 15 : training on 6447 raw words (185 effective words) took 0.0s, 7508 effective words/s\n",
            "INFO - 23:02:48: training on a 96705 raw words (2740 effective words) took 0.6s, 4251 effective words/s\n",
            "WARNING - 23:02:48: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "INFO - 23:02:48: precomputing L2-norms of word weight vectors\n",
            "INFO - 23:02:48: collecting all words and their counts\n",
            "INFO - 23:02:48: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO - 23:02:48: collected 4408 word types from a corpus of 9099 raw words and 1257 sentences\n",
            "INFO - 23:02:48: Loading a fresh vocabulary\n",
            "INFO - 23:02:48: effective_min_count=5 retains 263 unique words (5% of original 4408, drops 4145)\n",
            "INFO - 23:02:48: effective_min_count=5 leaves 2835 word corpus (31% of original 9099, drops 6264)\n",
            "INFO - 23:02:48: deleting the raw counts dictionary of 4408 items\n",
            "INFO - 23:02:48: sample=6e-05 downsamples 263 most-common words\n",
            "INFO - 23:02:48: downsampling leaves estimated 377 word corpus (13.3% of prior 2835)\n",
            "INFO - 23:02:48: estimated required memory for 263 words and 200 dimensions: 552300 bytes\n",
            "INFO - 23:02:48: resetting layer weights\n",
            "INFO - 23:02:48: training model with 2 workers on 263 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 1 : training on 9099 raw words (342 effective words) took 0.0s, 8688 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 2 : training on 9099 raw words (356 effective words) took 0.0s, 7989 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 3 : training on 9099 raw words (373 effective words) took 0.0s, 8445 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 4 : training on 9099 raw words (377 effective words) took 0.0s, 7973 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 5 : training on 9099 raw words (351 effective words) took 0.0s, 8971 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 6 : training on 9099 raw words (362 effective words) took 0.0s, 8736 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 7 : training on 9099 raw words (388 effective words) took 0.0s, 9336 effective words/s\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:48: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:48: EPOCH - 8 : training on 9099 raw words (358 effective words) took 0.0s, 8312 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 9 : training on 9099 raw words (406 effective words) took 0.0s, 9147 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 10 : training on 9099 raw words (373 effective words) took 0.0s, 9623 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 11 : training on 9099 raw words (359 effective words) took 0.0s, 9730 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 12 : training on 9099 raw words (379 effective words) took 0.0s, 8537 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 13 : training on 9099 raw words (419 effective words) took 0.0s, 10607 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 14 : training on 9099 raw words (364 effective words) took 0.0s, 11179 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 15 : training on 9099 raw words (346 effective words) took 0.0s, 8501 effective words/s\n",
            "INFO - 23:02:49: training on a 136485 raw words (5553 effective words) took 0.8s, 6830 effective words/s\n",
            "WARNING - 23:02:49: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "INFO - 23:02:49: precomputing L2-norms of word weight vectors\n",
            "INFO - 23:02:49: collecting all words and their counts\n",
            "INFO - 23:02:49: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO - 23:02:49: collected 3383 word types from a corpus of 6219 raw words and 902 sentences\n",
            "INFO - 23:02:49: Loading a fresh vocabulary\n",
            "INFO - 23:02:49: effective_min_count=5 retains 148 unique words (4% of original 3383, drops 3235)\n",
            "INFO - 23:02:49: effective_min_count=5 leaves 1867 word corpus (30% of original 6219, drops 4352)\n",
            "INFO - 23:02:49: deleting the raw counts dictionary of 3383 items\n",
            "INFO - 23:02:49: sample=6e-05 downsamples 148 most-common words\n",
            "INFO - 23:02:49: downsampling leaves estimated 180 word corpus (9.7% of prior 1867)\n",
            "INFO - 23:02:49: estimated required memory for 148 words and 200 dimensions: 310800 bytes\n",
            "INFO - 23:02:49: resetting layer weights\n",
            "INFO - 23:02:49: training model with 2 workers on 148 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 1 : training on 6219 raw words (172 effective words) took 0.0s, 6496 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 2 : training on 6219 raw words (184 effective words) took 0.0s, 7110 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 3 : training on 6219 raw words (170 effective words) took 0.0s, 5055 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 4 : training on 6219 raw words (184 effective words) took 0.0s, 6979 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 5 : training on 6219 raw words (222 effective words) took 0.0s, 7927 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 6 : training on 6219 raw words (188 effective words) took 0.0s, 4742 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 7 : training on 6219 raw words (179 effective words) took 0.0s, 5682 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 8 : training on 6219 raw words (190 effective words) took 0.0s, 7369 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 9 : training on 6219 raw words (163 effective words) took 0.0s, 5452 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 10 : training on 6219 raw words (185 effective words) took 0.0s, 6668 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:49: EPOCH - 11 : training on 6219 raw words (194 effective words) took 0.0s, 6440 effective words/s\n",
            "INFO - 23:02:49: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 12 : training on 6219 raw words (169 effective words) took 0.0s, 5283 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 13 : training on 6219 raw words (174 effective words) took 0.0s, 7010 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 14 : training on 6219 raw words (186 effective words) took 0.0s, 7060 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 15 : training on 6219 raw words (186 effective words) took 0.0s, 8752 effective words/s\n",
            "INFO - 23:02:50: training on a 93285 raw words (2746 effective words) took 0.6s, 4250 effective words/s\n",
            "WARNING - 23:02:50: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "INFO - 23:02:50: precomputing L2-norms of word weight vectors\n",
            "INFO - 23:02:50: collecting all words and their counts\n",
            "INFO - 23:02:50: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO - 23:02:50: collected 3586 word types from a corpus of 6117 raw words and 860 sentences\n",
            "INFO - 23:02:50: Loading a fresh vocabulary\n",
            "INFO - 23:02:50: effective_min_count=5 retains 150 unique words (4% of original 3586, drops 3436)\n",
            "INFO - 23:02:50: effective_min_count=5 leaves 1644 word corpus (26% of original 6117, drops 4473)\n",
            "INFO - 23:02:50: deleting the raw counts dictionary of 3586 items\n",
            "INFO - 23:02:50: sample=6e-05 downsamples 150 most-common words\n",
            "INFO - 23:02:50: downsampling leaves estimated 161 word corpus (9.8% of prior 1644)\n",
            "INFO - 23:02:50: estimated required memory for 150 words and 200 dimensions: 315000 bytes\n",
            "INFO - 23:02:50: resetting layer weights\n",
            "INFO - 23:02:50: training model with 2 workers on 150 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 1 : training on 6117 raw words (155 effective words) took 0.0s, 4915 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 2 : training on 6117 raw words (145 effective words) took 0.0s, 4694 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 3 : training on 6117 raw words (179 effective words) took 0.0s, 6622 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 4 : training on 6117 raw words (173 effective words) took 0.0s, 7115 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 5 : training on 6117 raw words (168 effective words) took 0.0s, 7619 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 6 : training on 6117 raw words (153 effective words) took 0.0s, 4613 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 7 : training on 6117 raw words (149 effective words) took 0.0s, 6017 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 8 : training on 6117 raw words (145 effective words) took 0.0s, 5814 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 9 : training on 6117 raw words (165 effective words) took 0.0s, 6472 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 10 : training on 6117 raw words (147 effective words) took 0.0s, 6131 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 11 : training on 6117 raw words (161 effective words) took 0.0s, 5719 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 12 : training on 6117 raw words (153 effective words) took 0.0s, 6311 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 13 : training on 6117 raw words (176 effective words) took 0.0s, 6447 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 14 : training on 6117 raw words (166 effective words) took 0.0s, 7045 effective words/s\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 1 more threads\n",
            "INFO - 23:02:50: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 23:02:50: EPOCH - 15 : training on 6117 raw words (158 effective words) took 0.0s, 6069 effective words/s\n",
            "INFO - 23:02:50: training on a 91755 raw words (2393 effective words) took 0.6s, 3788 effective words/s\n",
            "WARNING - 23:02:50: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "INFO - 23:02:50: precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdVCq2t1U9Ma"
      },
      "source": [
        "#feature para words embeddings, c√≥digo extra√≠do del aux 2\n",
        "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Transforma tweets a representaciones vectoriales usando alg√∫n modelo de Word Embeddings.\n",
        "    \"\"\"\n",
        "    emotion = ''\n",
        "    def __init__(self, aggregation_func, emotion):\n",
        "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
        "        self.emotion = emotion\n",
        "\n",
        "        # indicamos la funci√≥n de agregaci√≥n (np.min, np.max, np.mean, np.sum, ...)\n",
        "        self.aggregation_func = aggregation_func\n",
        "\n",
        "    def simple_tokenizer(self, doc, lower=False):\n",
        "        \"\"\"Tokenizador. Elimina signos de puntuaci√≥n, lleva las letras a min√∫scula(opcional) y \n",
        "           separa el tweet por espacios.\n",
        "        \"\"\"\n",
        "        if lower:\n",
        "            doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "        return doc.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \n",
        "        doc_embeddings = []\n",
        "        \n",
        "        for doc in X:\n",
        "            # tokenizamos el documento. Se llevan todos los tokens a min√∫scula. \n",
        "            # ojo con esto, ya que puede que tokens con min√∫scula y may√∫scula tengan\n",
        "            # distintas representaciones\n",
        "            tokens = self.simple_tokenizer(doc, lower = True) \n",
        "            \n",
        "            selected_wv = []\n",
        "            for token in tokens:\n",
        "                if token in models[emotion].wv.vocab:\n",
        "                    selected_wv.append(models[emotion].wv[token])\n",
        "                    \n",
        "            # si seleccionamos por lo menos un embedding para el tweet, lo agregamos y luego lo a√±adimos.\n",
        "            if len(selected_wv) > 0:\n",
        "                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
        "                doc_embeddings.append(doc_embedding)\n",
        "            # si no, a√±adimos un vector de ceros que represente a ese documento.\n",
        "            else: \n",
        "                #print('No pude encontrar ning√∫n embedding en el tweet: {}. Agregando vector de ceros.'.format(doc))\n",
        "                doc_embeddings.append(np.zeros(models[emotion].wv.vector_size)) # la dimension del modelo \n",
        "\n",
        "        return np.array(doc_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxEchBTI2XeV"
      },
      "source": [
        "#### Emoji Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaWg4EN9AtCG"
      },
      "source": [
        "import regex as re\n",
        "class EmojiCount(BaseEstimator, TransformerMixin):\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        split = len(re.findall(r'\\U00002764', tweet))\n",
        "        return [split]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0KiTmuM2biM"
      },
      "source": [
        "#### Lexicon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU6Jw4WphYQI"
      },
      "source": [
        "from lexicons import getLexicons\n",
        "###################################################################\n",
        "#Llamamos a getLexicons() de lexicons.py para generar el dict\n",
        "#completo de lexicons con todas sus variables asociadas.\n",
        "lexicons_dict = getLexicons()\n",
        "\n",
        "#retornamos un diccionario que contiene listas con las lexicons y sus\n",
        "#polaridades para cada sentimiento\n",
        "def lexicons_sentiments(): \n",
        "        emotionsLex = {\n",
        "            'anger': [],\n",
        "            'fear': [],\n",
        "            'joy': [],\n",
        "            'sadness': []\n",
        "        }\n",
        "\n",
        "        for key in lexicons_dict:\n",
        "            for emotion in ['anger', 'fear', 'joy', 'sadness']:\n",
        "                if (lexicons_dict[key][4] == ('#' + emotion)):\n",
        "                    polarity = lexicons_dict[key][6] #positive or negative\n",
        "                    polarity_val = lexicons_dict[key][7] #polarity value\n",
        "\n",
        "                    aux_list = [key, polarity, polarity_val]\n",
        "                    emotionsLex[emotion].append(aux_list)\n",
        "                    \n",
        "        return emotionsLex\n",
        "\n",
        "#Feature para las lexicons de cada tweet\n",
        "class LexiconCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    #lexicons_dict = {}\n",
        "\n",
        "    #se ejecuta lexicons_sentiments para asignarle a cada sentimiento\n",
        "    #estudiado (anger, fear, joy, sadness) las lexicons correspondientes\n",
        "    lexicons_emotions = {}\n",
        "\n",
        "    #string aux que almacena el nombre del sentimiento del dataset\n",
        "    #en proceso de clasificacion (ej: anger)\n",
        "    name_emotion = ''\n",
        "\n",
        "    #Al iniciar la clase, se debe llamar al m√©todo lexicons_sentiments()\n",
        "    #para que cree el diccionario correspondiente, a la vez que se asigna\n",
        "    #el sentimiento del dataset a trabajar.\n",
        "    \n",
        "    def __init__(self, nombre_emocion):\n",
        "        self.name_emotion = nombre_emocion\n",
        "        self.lexicons_emotions = lexicons_sentiments()\n",
        "\n",
        "    #entrega promedio polaridad (si es positiva o negativa)\n",
        "    #y numero de lexicons positivos y negativos\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        polarities_values = [0]\n",
        "        positives = 0\n",
        "        negatives = 0\n",
        "\n",
        "        #tweet a lower case para que coincidan las lexicons\n",
        "        lower_case = tweet.lower()\n",
        "        \n",
        "        for lex in self.lexicons_emotions[self.name_emotion]:\n",
        "            parsed_string = self.parse_underscores(lex[0])\n",
        "            if (parsed_string in lower_case):\n",
        "                polarities_values.append(abs(float(lex[2])))\n",
        "\n",
        "                if (lex[1] == 'positive'):\n",
        "                    positives += 1\n",
        "                else:\n",
        "                    negatives += 1\n",
        "\n",
        "        return [np.mean(polarities_values), positives, negatives]\n",
        "\n",
        "    #dado que las keys de las lexicons son precisamente la frase\n",
        "    #que podr√≠a aparecer en el tweet, es necesario parsearlas\n",
        "    #quit√°ndoles el _ y reemplaz√°ndolas por espacios blancos, tal\n",
        "    #como aparecer√≠a en un tweet escrito en lenguaje natural\n",
        "    def parse_underscores(self, input):\n",
        "        output = ''\n",
        "        for char in input:\n",
        "            if (char == '_'):\n",
        "                output += ' '\n",
        "            else:\n",
        "                output += char\n",
        "        return output\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        polarities = []\n",
        "\n",
        "        for tweet in X:\n",
        "            polarities.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(polarities)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "#######################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU_pYV2h2f7-"
      },
      "source": [
        "#### Negation Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEoITjk5O0vY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff96392d-9696-45e3-a3c7-d0548d99b91c"
      },
      "source": [
        "#feature para las negaciones en un documento\n",
        "from nltk.sentiment.util import mark_negation\n",
        "class NegationCountTransformer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def get_negated_count(self, tweet):\n",
        "        count = 0\n",
        "        negations = mark_negation(tweet.split())\n",
        "\n",
        "        for word in negations:\n",
        "            if (\"_NEG\" in word):\n",
        "                count +=1\n",
        "        return [count]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        negated = []\n",
        "        for tweet in X:\n",
        "            negated.append(self.get_negated_count(tweet))\n",
        "\n",
        "        return np.array(negated)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESR_F7su2io1"
      },
      "source": [
        "#### Elongated Word Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYigzVkZQsZ9"
      },
      "source": [
        "#feature para las palabras con m√°s de dos letras repetidas\n",
        "class ElongatedWordsCountTransformer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def get_elongated_words(self, tweet):\n",
        "        num_elongated = 0\n",
        "        splits = tweet.split(' ')\n",
        "\n",
        "        for word in splits:\n",
        "            if (len(word) > 1):\n",
        "                i = 0\n",
        "                while (i < (len(word) - 2)):\n",
        "                    if (word[i] == word[i+1] == word[i+2]):\n",
        "                        num_elongated += 1\n",
        "                        break\n",
        "                    i+=1\n",
        "                        \n",
        "        return [num_elongated]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        elongated = []\n",
        "        for tweet in X:\n",
        "            elongated.append(self.get_elongated_words(tweet))\n",
        "        \n",
        "        return np.array(elongated)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEDfW7bg2l3w"
      },
      "source": [
        "#### Uppercase Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rANWF1V_TA8_"
      },
      "source": [
        "#Feature para el n√∫mero de palabras completamente en mayus\n",
        "class UppercaseCountTransformer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def get_uppercase_words(self, tweet):\n",
        "        num_uppercase = 0\n",
        "        splits = tweet.split(' ')\n",
        "\n",
        "        for word in splits:\n",
        "            flag = True\n",
        "\n",
        "            for char in word:\n",
        "                if (len(word) == 1):\n",
        "                    \n",
        "                    flag = False\n",
        "                else:\n",
        "                    if (char.isupper() == False):\n",
        "                        flag = False\n",
        "            if (flag == True):\n",
        "                num_uppercase += 1\n",
        "        return [num_uppercase]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        words_uppercase = []\n",
        "        for tweet in X:\n",
        "            words_uppercase.append(self.get_uppercase_words(tweet))\n",
        "        return np.array(words_uppercase)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTANyXTd2pYv"
      },
      "source": [
        "#### DotsMarks Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yih6zZWUuzQ"
      },
      "source": [
        "#feature para secuencias de puntos, exclamaciones e interrogaciones\n",
        "class DotsMarksCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    #se cuenta si hay al menos tres de cada uno\n",
        "    def get_dotsmarks_count(self, tweet):\n",
        "        #number of [dots, ?, !]\n",
        "        dots = 0\n",
        "        q_marks = 0\n",
        "        ex_marks = 0\n",
        "\n",
        "        splits = tweet.split()\n",
        "\n",
        "        for word in splits:\n",
        "            if ('...' in word):\n",
        "                dots += 1\n",
        "            if ('???' in word):\n",
        "                q_marks += 1\n",
        "            if ('!!!' in word):\n",
        "                ex_marks += 1\n",
        "\n",
        "        return [dots, q_marks, ex_marks]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        sequences = []\n",
        "        for tweet in X:\n",
        "            sequences.append(self.get_dotsmarks_count(tweet))\n",
        "\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzKcgalLj4fB"
      },
      "source": [
        "### Custom Tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CfYC0FVn4W7"
      },
      "source": [
        "Crearemos funciones para cada tokenizador o procesamiento que se desea realizar sobre un tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3nrHgo79WK0",
        "outputId": "a23ed929-fa40-4509-837f-78d5f937238b"
      },
      "source": [
        "# Librerias usadas por algunos tokenizadores\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpcur9GG21c7"
      },
      "source": [
        "#### Remover Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzPTtl7nUsQN"
      },
      "source": [
        "from nltk.corpus import \n",
        "# Tokenizador que remueve stopwords del ingles y simbolos\n",
        "def remStopWords(tokens):\n",
        "    newTokens = []\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    stopWords_extended = stopWords | {'.', ',', ':', '+', '-', '(',')',';','\\'','..','...','‚Äî', '>','<','\\\\'}\n",
        "    for token in tokens:\n",
        "        if (token not in stopWords_extended) and (token[0:1]!='@'):\n",
        "            newTokens.append(token)\n",
        "    return newTokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGQakjg724l5"
      },
      "source": [
        "#### Remover Hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-mISVLH87Lk"
      },
      "source": [
        "# Tokenizador que remueve los #\n",
        "def hashtagToWord(tokens):\n",
        "    newTokens = []\n",
        "    for token in tokens:\n",
        "        newTokens.append(token.replace('#',''))\n",
        "    return newTokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXwaboYn27Mp"
      },
      "source": [
        "#### Lematizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hnIOM5i7LsZ"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# Funcion que lematiza los terminos\n",
        "def lemmatize(tokens):\n",
        "    lem = WordNetLemmatizer()\n",
        "    lemmatizedTokens = []\n",
        "    for word in tokens:\n",
        "        lemmatizedTokens.append(lem.lemmatize(word.lower()))\n",
        "    return lemmatizedTokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoBmLZdY2-ii"
      },
      "source": [
        "#### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zanj2L0l-Mbs"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "# Funcion que realiza steamming sobre los terminos\n",
        "def stemmize(tokens):\n",
        "    ps =  SnowballStemmer(language='english')\n",
        "    stemmedTokens = []\n",
        "    for word in tokens:\n",
        "        stemmedTokens.append(ps.stem(word))\n",
        "    return stemmedTokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxE8u2Fa3Cyz"
      },
      "source": [
        "#### Tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt5uxRADntAB"
      },
      "source": [
        "Creamos una funcion que se encarga de juntar todos los procesamiento y tokkenizadores que deseamos usar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a19PJSiJiNgz"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.sentiment.util import mark_negation\n",
        "# Funcion que se usara en el Pipeline como tokenizador ,\n",
        "# la cual agrupa todos los procesamientos y tokenizadores que se usaran sobre\n",
        "# un tweet.\n",
        "def my_tokenizer(text):\n",
        "    tokens = TweetTokenizer(strip_handles=True, reduce_len=True).tokenize(text)\n",
        "    # Nos dimos cuenta que remover stopwords en un tweet no es favorable.\n",
        "    # tokens = remStopWords(tokens)\n",
        "    tokens = mark_negation(tokens)\n",
        "    tokens = hashtagToWord(tokens)\n",
        "    tokens = stemmize(tokens)\n",
        "    tokens = lemmatize(tokens)\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKDwQBhvnRce"
      },
      "source": [
        "Testeo de nuestro tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPYy51YHfgcP",
        "outputId": "f1be75d2-eecf-4567-8876-516a1a9c8468"
      },
      "source": [
        " print(my_tokenizer('Time wounds all heels.\\\\n\\\\n #DrunkJesus #rt #lol #wisdom #quote #comedy #self #Revenge  #hate #time #funny #politics #Trump #POTUS2016'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['time', 'wound', 'all', 'heel', '.', '\\\\', 'n', '\\\\', 'n', 'drunkjesus', 'rt', 'lol', 'wisdom', 'quot', 'comedi', 'self', 'reveng', 'hate', 'time', 'funni', 'polit', 'trump', 'potus2016']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT1k2Ra4kFHC",
        "outputId": "f5a38b2c-540d-4628-aea6-1879db03016b"
      },
      "source": [
        "print(my_tokenizer(\"@soyoprincess they irritate me. Them and their inch thick made up masks\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['they', 'irrit', 'me', '.', 'them', 'and', 'their', 'inch', 'thick', 'made', 'up', 'mask']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO_yIepczvIx"
      },
      "source": [
        "### Definir la representaci√≥n y el clasificador\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IJVgjmTo9cK"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dhbukPqompD"
      },
      "source": [
        "Experimento 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRXSd7WWQAFV"
      },
      "source": [
        "# LogisticRegression + tfidf + chars_count + negation_count + word_embedding\n",
        "\n",
        "def experimento1(dataset_name):\n",
        "\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('tfidf_vec', TfidfVectorizer(tokenizer=my_tokenizer, use_idf=True)),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ('negation_count', NegationCountTransformer()),\n",
        "                                    ('word_embedding', Doc2VecTransformer(np.mean, dataset_name)),\n",
        "                                    ])),('clf', LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISHma9ksotcu"
      },
      "source": [
        "Experimento 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaC6Ci9lO3uP"
      },
      "source": [
        "# LogisticRegression + bow + chars_count + negation_count + word_embedding\n",
        "\n",
        "def experimento2(dataset_name):\n",
        "\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer(tokenizer=my_tokenizer, ngram_range=(1,3), max_features=9000)),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ('negation_count', NegationCountTransformer()),\n",
        "                                    ('word_embedding', Doc2VecTransformer(np.mean, dataset_name)),\n",
        "                                    ])),('clf', LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk_3UltHp0bf"
      },
      "source": [
        "Experimento 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpUlah9SW-1V"
      },
      "source": [
        "# SVC + bow + chars_count + negation_count + word_embedding\n",
        " \n",
        "def experimento3(dataset_name):\n",
        "\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer(tokenizer=my_tokenizer, ngram_range=(1,3), max_features=9000)),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ('negation_count', NegationCountTransformer()),\n",
        "                                    ('word_embedding', Doc2VecTransformer(np.mean, dataset_name)),\n",
        "                                    ])),('clf', SVC(kernel='linear', probability=True))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjhRxRYOqKML"
      },
      "source": [
        "Experimento 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDGnk7RtY9Er"
      },
      "source": [
        "# SVC + tfidf + chars_count + negation_count + word_embedding\n",
        "\n",
        "def experimento4(dataset_name):\n",
        "\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('tfidf_vec', TfidfVectorizer(tokenizer=my_tokenizer, use_idf=True)),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ('negation_count', NegationCountTransformer()),\n",
        "                                    ('word_embedding', Doc2VecTransformer(np.mean, dataset_name)),\n",
        "                                    ])),('clf', SVC(kernel='linear', probability=True))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG-ePVnbqLsp"
      },
      "source": [
        "Experimento 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OugPgJgqa-rQ"
      },
      "source": [
        "# Logistic Regression + tfidf + chars_count + negation_count + Dotsmaks_count+ word_embedding\n",
        "\n",
        "def experimento5(dataset_name):\n",
        "\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('tfidf_vec', TfidfVectorizer(tokenizer=my_tokenizer, use_idf=True)),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ('negation_count', NegationCountTransformer()),\n",
        "                                    ('dotsMarks_count', DotsMarksCountTransformer()),\n",
        "                                    ('word_embedding', Doc2VecTransformer(np.mean, dataset_name))\n",
        "                                    ])),('clf', LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU79heJxqNtt"
      },
      "source": [
        "Experimento 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA4M3hOec2oW"
      },
      "source": [
        "# Logistic Regression + tfidf + chars_count + negation_count +  word_embedding + Lexicon\n",
        "\n",
        "def experimento6(dataset_name):\n",
        "\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('tfidf_vec', TfidfVectorizer(tokenizer=my_tokenizer, use_idf=True)),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ('negation_count', NegationCountTransformer()),\n",
        "                                    ('word_embedding', Doc2VecTransformer(np.mean, dataset_name)),\n",
        "                                    ('lexicon_count', LexiconCountTransformer(dataset_name))\n",
        "                                    ])),('clf', LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmMdm98vzvIy"
      },
      "source": [
        "### Ejecutar el pipeline para alg√∫n dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj2uV1YGxy1C"
      },
      "source": [
        "Run principal que deja un 33% de los datos como testeo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.167498Z",
          "start_time": "2020-04-07T15:44:21.157540Z"
        },
        "scrolled": true,
        "id": "_eX0cEu-zvIz"
      },
      "source": [
        "def run(dataset, dataset_name, pipeline):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test, a√∫n no se transforma de Strings a valores num√©ricos.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        stratify=dataset.sentiment_intensity,\n",
        "        test_size=0.33)\n",
        "    \n",
        "    print(f'# Datos de entrenamiento en dataset {dataset_name}: {len(X_train)}')\n",
        "    print(f'# Datos de testing en dataset {dataset_name}: {len(X_test)}')\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
        "    # En este caso el Bag of Words es el encargado de transformar de Strings a vectores num√©ricos.\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "    \n",
        "    # Evaluamos:\n",
        "    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-avtd7Symd-"
      },
      "source": [
        "Este run es mas interesante porque reliza un cross_validation sobre nuestro dataset y nos entrega accuracy, por lo tanto para medir la generalidad nos sirvio mucho mas este run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUTSxvbOJT0V"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "def run2(dataset, dataset_name, pipeline):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test, a√∫n no se transforma de Strings a valores num√©ricos.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        stratify=dataset.sentiment_intensity,\n",
        "        test_size=0.33)\n",
        "    \n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
        "    # En este caso el Bag of Words es el encargado de transformar de Strings a vectores num√©ricos.\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "    \n",
        "    # Evaluamos:\n",
        "    scores = cross_val_score(pipeline, dataset.tweet, dataset.sentiment_intensity, cv=10)\n",
        "    return pipeline, learned_labels, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z96C1ZOfzvIz"
      },
      "source": [
        "### Ejecutar el sistema creado por cada train set\n",
        "\n",
        "Este c√≥digo crea y entrena los 4 sistemas de clasificaci√≥n y luego los evalua. Para los experimentos, pueden copiar este c√≥digo variando el pipeline cuantas veces estimen conveniente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.384119Z",
          "start_time": "2020-04-07T15:44:21.170488Z"
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXAxZBdVzvI0",
        "outputId": "438de7fc-63db-4866-a120-e81cfd673374"
      },
      "source": [
        "classifiers = []\n",
        "learned_labels_array = []\n",
        "scores_array = []\n",
        "\n",
        "# El parametro \"a\" lo usamos para repetir el experimento varias veces y por lo tanto\n",
        "# las metricas obtenidas se veian menos afectadas por la aleatoridad en las particiones\n",
        "a = 1\n",
        "for i in range(a):\n",
        "    # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "    for dataset_name, dataset in train.items():\n",
        "        \n",
        "        # creamos el pipeline\n",
        "        #pipeline = get_experiment_3_pipeline(dataset_name)\n",
        "        pipeline = experimento1(dataset_name)\n",
        "        \n",
        "        # ejecutamos el pipeline sobre el dataset\n",
        "        classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
        "\n",
        "        # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "        classifiers.append(classifier)\n",
        "\n",
        "        # guardamos las labels aprendidas por el clasificador\n",
        "        learned_labels_array.append(learned_labels)\n",
        "\n",
        "        # guardamos los scores obtenidos\n",
        "        scores_array.append(scores)\n",
        "\n",
        "#print(\"AVG Accuracy:\", np.mean(scores_array))\n",
        "# print avg scores\n",
        "print(\n",
        "    \"Average scores:\\n\\n\",\n",
        "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "    .format(*np.array(scores_array).mean(axis=0)))\n",
        "# print max scores\n",
        "print(\n",
        "    \"MAX scores:\\n\\n\",\n",
        "    \"MAX AUC: {0:.3g}\\t MAX Kappa: {1:.3g}\\t MAX Accuracy: {2:.3g}\"\n",
        "    .format(*np.array(scores_array).max(axis=0)))\n",
        "# print min scores\n",
        "print(\n",
        "    \"MIN scores:\\n\\n\",\n",
        "    \"MIN AUC: {0:.3g}\\t MIN Kappa: {1:.3g}\\t MIN Accuracy: {2:.3g}\"\n",
        "    .format(*np.array(scores_array).min(axis=0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  5  47   1]\n",
            " [  8 189   7]\n",
            " [  0  38  16]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.38      0.09      0.15        53\n",
            "      medium       0.69      0.93      0.79       204\n",
            "        high       0.67      0.30      0.41        54\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.58      0.44      0.45       311\n",
            "weighted avg       0.63      0.68      0.62       311\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 32  59   4]\n",
            " [ 31 180  20]\n",
            " [  6  53  30]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.46      0.34      0.39        95\n",
            "      medium       0.62      0.78      0.69       231\n",
            "        high       0.56      0.34      0.42        89\n",
            "\n",
            "    accuracy                           0.58       415\n",
            "   macro avg       0.55      0.48      0.50       415\n",
            "weighted avg       0.57      0.58      0.56       415\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 22  47   3]\n",
            " [ 19 129  13]\n",
            " [  4  33  28]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.49      0.31      0.38        72\n",
            "      medium       0.62      0.80      0.70       161\n",
            "        high       0.64      0.43      0.51        65\n",
            "\n",
            "    accuracy                           0.60       298\n",
            "   macro avg       0.58      0.51      0.53       298\n",
            "weighted avg       0.59      0.60      0.58       298\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 21  44   4]\n",
            " [ 21 103  26]\n",
            " [  2  35  28]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.48      0.30      0.37        69\n",
            "      medium       0.57      0.69      0.62       150\n",
            "        high       0.48      0.43      0.46        65\n",
            "\n",
            "    accuracy                           0.54       284\n",
            "   macro avg       0.51      0.47      0.48       284\n",
            "weighted avg       0.53      0.54      0.52       284\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.667\t Average Kappa: 0.224\t Average Accuracy: 0.599\n",
            "MAX scores:\n",
            "\n",
            " MAX AUC: 0.725\t MAX Kappa: 0.277\t MAX Accuracy: 0.675\n",
            "MIN scores:\n",
            "\n",
            " MIN AUC: 0.614\t MIN Kappa: 0.191\t MIN Accuracy: 0.535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_hMlS9K0IyW"
      },
      "source": [
        "Resultados de las metricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjYQvb4J-Gbt",
        "outputId": "5f2f3867-d313-4abd-d840-823979a76309"
      },
      "source": [
        "print(scores_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([0.671, 0.117, 0.678]), array([0.668, 0.168, 0.588]), array([0.748, 0.296, 0.634]), array([0.635, 0.169, 0.546])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:37:43.169737Z",
          "start_time": "2019-08-21T19:37:43.166744Z"
        },
        "id": "IUKwcde_zvI0"
      },
      "source": [
        "### Predecir los target set y crear la submission\n",
        "\n",
        "Aqu√≠ predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.392097Z",
          "start_time": "2020-04-07T15:44:21.386114Z"
        },
        "id": "mWDUoSmbzvI1"
      },
      "source": [
        "def predict_target(dataset, classifier, labels):\n",
        "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
        "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
        "    \n",
        "    # Agregar ids\n",
        "    predicted['id'] = dataset.id.values\n",
        "    # Reordenar las columnas\n",
        "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
        "    return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.588573Z",
          "start_time": "2020-04-07T15:44:21.394094Z"
        },
        "scrolled": true,
        "id": "5CJ4PTwZzvI1"
      },
      "source": [
        "predicted_target = {}\n",
        "\n",
        "# Crear carpeta ./predictions\n",
        "if (not os.path.exists('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "# por cada target set:\n",
        "for idx, key in enumerate(target):\n",
        "    # Predecirlo\n",
        "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
        "                                           learned_labels_array[idx])\n",
        "    # Guardar predicciones en archivos separados. \n",
        "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
        "                                 sep='\\t',\n",
        "                                 header=False,\n",
        "                                 index=False)\n",
        "\n",
        "# Crear archivo zip\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCAyJIj8nTlU"
      },
      "source": [
        "## **7. Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1AsX0JM1qYO"
      },
      "source": [
        "- Experimentos No.1 y No.2:\n",
        "Arrojaron resultados promedios casi id√©nticos, pero poseen algunas diferencias marginalmente menores a causa del Vectorizer elegido. Es dif√≠cil comprender el porqu√© de esto, puede deberse al idf-score que Tfidf posee, ya que considera la relevancia de los t√©rminos, mientras que bag of words de CountVectorizer solamente hace un conteo, perdiendo algunos datos.\n",
        "\n",
        "- Experimento No.3 y No.4:\n",
        "Es notoriamente mejor el desempe√±o del No.4. Tiene sentido, ya que Tfidf considera la relevancia de los t√©rminos, tal como en el caso anterior. Es curioso ver que, en este caso, el cambio en el Vectorizer utilizado s√≠ arroja diferencias sustanciales usando este modelo, contrast√°ndolo con el caso anterior.\n",
        "\n",
        "- Experimento No. 5 y No.1:\n",
        "A√±adir la feature de dotsMarks_count mejor√≥ un poco la m√©trica AUC, pero afect√≥ Kappa. Nuevamente es dif√≠cil comprender las razones de esto, con estudios posteriores podr√≠a lograr comprenderse.\n",
        "\n",
        "- Experimento No. 6 y No.1: El No.1 sigue siendo mejor. La feature de Lexicons a√±adida al No. 6 afect√≥ todas las m√©tricas, en mayor o menor medida. Puede ser que la mayor√≠a de las lexicons no coincidan con el string bruto del tweet, dado que, por una implementaci√≥n incorrecta, no se analizaba despu√©s de ser tokenizado (lo que filtrar√≠a hashtags, s√≠mbolos, emojis, etc). En caso contrario, puede que mejorara el desempe√±o.\n",
        "\n",
        "Se decidi√≥ enviar como submission el experimento No.1 a CodaLabs, por su mejor desempe√±o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAYwupS3naYX"
      },
      "source": [
        "\n",
        "| No. | Approach                       || Dataset   | AUC   | Kappa | Accuracy |\n",
        "|-----|--------------------------------||-----------|-------|-------|----------|\n",
        "|     | Features        | Clasifier     |           |       |       |          |\n",
        "| 0   | bow+chars_count | MultinomialNB | anger     | 0.622 | 0.163 | 0.688    |\n",
        "|     |                 |               | fear      | 0.597 | 0.091 | 0.559    |\n",
        "|     |                 |               | joy       | 0.728 | 0.251 | 0.601    |\n",
        "|     |                 |               | sadness   | 0.645 | 0.166 | 0.581    |\n",
        "|     |                 |               |**average**| 0.648 | 0.168 | 0.607    |\n",
        "|||||||\n",
        "| 1   |tfidfVect + chars_count + NegationCount + WordEmbedding | LogisticRegression  | anger     | 0.604 | 0.061 | 0.646 |\n",
        "|     |                 |               | fear      | 0.686 | 0.203 | 0.61    |\n",
        "|     |                 |               | joy       | 0.742 | 0.259 | 0.597    |\n",
        "|     |                 |               | sadness   | 0.678 | 0.285 | 0.606    |\n",
        "|     |                 |               |**average**| 0.682 | 0.265 | 0.621    |\n",
        "|||||||\n",
        "| 2   |bow (ngram=(1,3), max_features=9000) + chars_count + NegationCount + WordEmbedding | LogisticRegression | anger     | 0.652 | 0.197 | 0.669 |\n",
        "|     |                 |               | fear      | 0.686 | 0.297 | 0.614    |\n",
        "|     |                 |               | joy       | 0.711 | 0.279 | 0.597    |\n",
        "|     |                 |               | sadness   | 0.678 | 0.285 | 0.606    |\n",
        "|     |                 |               |**average**| 0.682 | 0.265 | 0.621    |\n",
        "|||||||\n",
        "| 3   |bow (ngram=(1,3), max_features=9000) + chars_count + NegationCount + WordEmbedding | SVC | anger     | 0.648 | 0.097 | 0.675 |\n",
        "|     |                 |               | fear      | 0.677 | 0.205 | 0.607    |\n",
        "|     |                 |               | joy       | 0.689 | 0.243 | 0.601    |\n",
        "|     |                 |               | sadness   | 0.62 | 0.135 | 0.546    |\n",
        "|     |                 |               |**average**| 0.659 | 0.17 | 0.607    |\n",
        "|||||||\n",
        "| 4   |tfidfVect + chars_count + NegationCount + WordEmbedding | SVC | anger     | 0.685 | 0.177 | 0.685 |\n",
        "|     |                 |               | fear      | 0.66 | 0.164 | 0.578    |\n",
        "|     |                 |               | joy       | 0.713 | 0.283 | 0.604    |\n",
        "|     |                 |               | sadness   | 0.68 | 0.187 | 0.585    |\n",
        "|     |                 |               |**average**| 0.685 | 0.203 | 0.613    |\n",
        "|||||||\n",
        "| 5   |tfidfVect + chars_count + NegationCount + dotsMarks_count + WordEmbedding | LogisticRegression  | anger     | 0.635 | 0.117 | 0.678 |\n",
        "|     |                 |               | fear      | 0.707 | 0.142 | 0.586    |\n",
        "|     |                 |               | joy       | 0.72 | 0.258 | 0.607    |\n",
        "|     |                 |               | sadness   | 0.701 | 0.19 | 0.563    |\n",
        "|     |                 |               |**average**| 0.687 | 0.194 | 0.621    |\n",
        "|||||||\n",
        "| 6   |tfidfVect + chars_count + NegationCount + WordEmbedding + Lexicon | LogisticRegression  | anger     | 0.671 | 0.117 | 0.678 |\n",
        "|     |                 |               | fear      | 0.668 | 0.168 | 0.588    |\n",
        "|     |                 |               | joy       | 0.748 | 0.296 | 0.634    |\n",
        "|     |                 |               | sadness   | 0.635 | 0.169 | 0.546    |\n",
        "|     |                 |               |**average**| 0.68 | 0.188 | 0.611    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqlew0iizvI1"
      },
      "source": [
        "## **8. Conclusiones**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFTikGbszZuc"
      },
      "source": [
        "Un trabajo a futuro es usar un clasificador generalizado para los distintos sentimientos, construyendo pipelines especializados en cada uno. Respecto al tokenizador y features, fue curioso c√≥mo las stopwords empeoraron, en este caso, el desempe√±o de los clasificadores. En relaci√≥n a los casos de ensayo, se pudo notar la diferencia entre los modelos, siendo notoria la diferencia entre Naive Bayes con los otros dos. \n",
        "\n",
        "La mayor parte de la discusi√≥n de resultados est√° en el apartado anterior, pero detallando un poco m√°s, hay que ser cuidadoso al momento de pre-procesar la informaci√≥n. En este caso, oversampling influy√≥ fuertemente en el overfitting de los clasificadores, siendo contraproducente en el proceso experimental y su an√°lisis en los primeros pasos del experimento.  \n",
        "\n",
        "Un estudio m√°s profundo de la materia permitir√≠a experimentar con herramientas m√°s complejas y prevenir caer en los problemas mencionados anteriormente, por lo que se apunta a seguir trabajando en esto."
      ]
    }
  ]
}