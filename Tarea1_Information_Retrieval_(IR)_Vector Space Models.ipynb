{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Tarea 1 - CC6205 Natural Language Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbrMqMPyncTC"
      },
      "source": [
        "# **Tarea 1 - CC6205 Natural Language Processing 📚**\n",
        "\n",
        "**Integrantes: Vicente Ardiles S. - Rodrigo I. Oportot**\n",
        "\n",
        "**Fecha límite de entrega 📆:** Lunes 12 de Abril.\n",
        "\n",
        "**Tiempo estimado de dedicación: 6hrs**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1HFX-9PpxF9"
      },
      "source": [
        "` ` \n",
        "\n",
        "\n",
        "Bienvenid@s a la primera tarea en el curso de Natural Language Processing (NLP). Esta tarea tiene como objetivo evaluar los contenidos teóricos de las primeras semanas de clases, enfocado principalmente en ***Information Retrieval (IR)*** y ***Vector Space Models***. Si aún no han visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "La tarea consta de una parte teórica que busca evaluar conceptos vistos en clases. Seguido por una parte práctica con el fín de introducirlos a la programación en Python enfocada en NLP. \n",
        "\n",
        "` ` \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Instrucciones:**\n",
        "- La tarea se realiza en grupos de **máximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación. \n",
        "- Está **PROHIBIDO** usar cualquier librería que implemente los algoritmos pedidos (Spacy, scikit, etc). Sólo se podrán utilizar las librerías importadas al inicio de la sección de práctica.\n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a través del canal de Discord del curso. \n",
        "\n",
        "\n",
        "\n",
        "Ahora sí, empecemos! 😄🚀\n",
        "\n",
        "` ` \n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "\n",
        "Slides:\n",
        "    \n",
        "- [Introducción al curso](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-introduction.pdf)\n",
        "- [Vector Space Model / Information Retrieval](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-IR.pdf)    \n",
        "\n",
        "Videos: \n",
        "\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: CC6205 - Procesamiento de Lenguaje Natural: Introducción parte I](https://www.youtube.com/watch?v=HEKTNOttGvU)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: CC6205 - Procesamiento de Lenguaje Natural: Introducción parte II](https://www.youtube.com/watch?v=P8cwnI-f-Kg)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Vector Space Model and Information Retrieval parte 1](https://youtu.be/FXIVClF370w)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Vector Space Model and Information Retrieval parte 2](https://youtu.be/f8nG1EMmPZk)\n",
        "\n",
        "` ` \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJshpe1yrKJr"
      },
      "source": [
        "## **1 - Preguntas teóricas 📕 (2 puntos).** ##\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBEDKXBPoA7w"
      },
      "source": [
        "Las siguientes celdas contienen preguntas acerca del contenido visto en clases y en el material del curso.  Contestar cada pregunta en su celda correspondiente y **no extenderse más de 5 lineas** . 🙏\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNJPR1kMrw9R"
      },
      "source": [
        "**Pregunta 1 (0.5 puntos): ¿Por qué el análisis del lenguaje humano es una tarea compleja? Mencione dos razones.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlTBYHEptdde"
      },
      "source": [
        "` ` \n",
        "\n",
        "    Respuesta: \n",
        "    Una razón es su ambigüedad, uno puede agregar o quitar una palabra de una oración y cambiar totalmente el contexto y sentido de ella, sumado a que el lenguaje evoluciona con el tiempo, pudiendo no tener el mismo significado según la época. Por un lado más técnico, está la morfología de las palabras, no se pudiendo inferir una relación semántica entre palabras por las letras que la componen (casa y caza, por ejemplo).\n",
        "\n",
        "\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv8yjG94r6FC"
      },
      "source": [
        "**Pregunta 2 (0.5 puntos): ¿Cuál es la diferencia entre Natural Language Processing (NLP) y Computational Linguistics?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmC3uYHtctH"
      },
      "source": [
        "` ` \n",
        "\n",
        "    Respuesta:\n",
        "    PLN se encarga de la metodología y diseños para resolver problemas (tasks) concretos que involucren lenguaje natural, con algoritmos y representaciones que lo trabajen. Computational Linguistics es el estudio de los procesos computacionales asociados al entendimiento del lenguaje natural, tratando de entender el lenguaje desde un punto de vista científico/lingüista/computacional.\n",
        "    \n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-0imOaFsRK5"
      },
      "source": [
        "**Pregunta 3 (0.5 puntos): ¿Qué es el Machine Learning supersivado? ¿Por qué es importante para el campo de NLP?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY6mfCE7tb1x"
      },
      "source": [
        "` ` \n",
        "\n",
        "    Respuesta:\n",
        "    Son técnicas de ML donde el aprendizaje de los algoritmos que resolverán las tasks son estudiados e intervenidas a medida que se desarrollan, construyendo las reglas del lenguaje estudiado a través de un conjunto de datos ya conocidos (entrenamiento), etiquetados previamente. Destaca en NLP dada la dificultad de formalizar el lenguaje natural, siendo una herramienta muy útil para abordar tal problema.\n",
        "\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVMXilrYsiSZ"
      },
      "source": [
        "**Pregunta 4 (0.5 puntos): ¿Cuáles son las diferencias entre usar Deep learning y Machine Learning clásico (empirismo) para un problema de NLP? Ejemplifique con alguna task.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QXWMzpTtBZL"
      },
      "source": [
        "` ` \n",
        "\n",
        "    Respuesta:\n",
        "    El empirismo se caracterizaba por explotar manualmente el corpus a estudiar, luego usando modelos de ML supervisado y conceptos estadísticos. DL reemplaza lo anterior con algoritmos que aprenden sobre la marcha las reglas del lenguaje procesado. Ejemplo: NER con empirismo trabaja los datos crudos extrayendo atributos para procesarlos con ML supervisado, en cambio, en DL los algoritmos hacen la mayor parte del trabajo sin ese pre-procesamiento.\n",
        "    \n",
        "\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fpsz2pQt8x5"
      },
      "source": [
        "## **2 - Preguntas prácticas 💻 (4 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB92kQXspvbR"
      },
      "source": [
        "Esta segunda sección incluye ejercicios de programación 🤙. Leer atentamente las instrucciones entregadas a continuación para facilitar el proceso de revisión de sus trabajos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PosWgWgRxHKp"
      },
      "source": [
        "` ` \n",
        "\n",
        "**Instrucciones:**\n",
        "\n",
        "\n",
        "\n",
        "- Escribe tu código entre las lineas de comentarios **### Aquí inicia tu código ###** y **### Aquí termina tu código ###**.\n",
        "- Cuando el ejercicio incluya un bloque llamado ***Test***, comprueba que el resultado de la ejecución coincida con el resultado esperado.\n",
        "- Recuerde siempre mantener buenas prácticas de código.\n",
        "- Está permitido sólo utilizar las librerías importadas antes del Ejercicio 1.\n",
        "- **Recordar** que: *Documento = Oración. Dataset = Corpus. Vocabulario = Typos*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBrmFHXhqUww"
      },
      "source": [
        "**Ejemplo:** Implemente una función **`hello_world()`** que imprima en pantalla `\"Hello World\"`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu7cIsawyJHx"
      },
      "source": [
        "def hello_world():\n",
        "  ### Aquí inicia tu código ###\n",
        "  print(\"Hello World\")\n",
        "  ### Aquí termina tu código ###"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz6klw12lwbW"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac-WMk2dyQbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858824c6-4bf6-4786-9a7b-9343dcb49141"
      },
      "source": [
        "hello_world()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIoiAMxtyUjQ"
      },
      "source": [
        "***Resultado esperado***: \n",
        "<table>\n",
        "    <tr> \n",
        "        <td> Hello World </td> \n",
        "    </tr>\n",
        "</table> \n",
        "\n",
        "``\n",
        "``\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlPyrPXiH0l4"
      },
      "source": [
        "Estas son las librerías permitidas. Si quieren utilizar alguna librería adicional, pueden realizar la consulta a través de Discord. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtP6Emjo1kF0"
      },
      "source": [
        "import math\n",
        "import codecs\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSN4bBoY2Td4"
      },
      "source": [
        "` ` \n",
        "\n",
        "**Ejercicio 1 - *Tokenización* (0.5 puntos).** \n",
        "` `  \n",
        "` ` \n",
        "\n",
        "En el primer ejercicio veremos la dificultad 😨 de tokenizar textos no estructurados, destacando la importancia de tener librerías que realicen este trabajo. \n",
        "\n",
        "El archivo adjunto al enunciado de la tarea contiene una interconsulta médica real en Chile, estos textos son frecuentemente utilizados para obtener información epidemiológica y realizar estudios estadísticos a partir de ellos. Es por esto que poder analizar el texto de manera correcta es fundamental. \n",
        "\n",
        "\n",
        "Ejecute el código a continuación para cargar el ejemplo. Recuerde realizar la modificación al directorio en caso que el archivo no se encuentre en el mismo directorio del Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnNUYlWo21g4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893fa78b-ae9f-4cae-d384-be879a45ef2d"
      },
      "source": [
        "text = codecs.open('/content/sample_data/interconsulta.txt', 'r', 'UTF-8').read()\n",
        "print(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interconsulta n°2 del mes.-05/10/2019\r\n",
            "- ENFERMEDAD RENAL  CRÓNICA ETAPA 3. \r\n",
            "- Fundamento Clínico APS: PACIENTE  DE 61 AÑOS, \tCON ANTECEDENTES DE HRTA DE LARGA DATA.ACUDE A CONTROL PSCV CON EXAMNES, DONDE DESTACA CREATININA DE 1,43,\r\n",
            " CON UNA VFG CALCULADA DE 53 ML/MIN/1.73M2. CON UNA CREATININA HACE MAS DE 3 MESES DE 1.34.\r\n",
            "                   \r\n",
            "Enfermedad renal cronica Etapa 3(SOLICITO EVALUACION DEL CASO).\r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIaOYzMk3v1X"
      },
      "source": [
        "Implementen una función **`get_tokens()`** que reciba un texto y entregue una lista con sus tokens. Son libres de elegir la forma de tokenizar mientras no utilicen librerías con tokenizadores ya implementados. Pueden utilizar la librería **re** importada para trabajar símbolos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dl42-hgIhqB"
      },
      "source": [
        "Ejemplo de uso:\n",
        "\n",
        "`get_tokens('Este es un ejemplo de prueba.')` \n",
        "\n",
        "Nos entregaría:\n",
        "\n",
        "`['Este', 'es', 'un', 'ejemplo', 'de', 'prueba', '.']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF1RcIwq4G2x"
      },
      "source": [
        "def get_tokens(text):\n",
        "  ### Inicio del código ###\n",
        "  \n",
        "  output = []\n",
        "  regex = []\n",
        "\n",
        "  \"\"\"\n",
        "  Con expresiones regulares, trataremos de tokenizar:\n",
        "  - los numeros con parte decimal\n",
        "  - enteros sin separar sus digitos\n",
        "  - palabras\n",
        "  - simbolos como . , ? - entre otros\n",
        "  \"\"\"\n",
        "\n",
        "  ## decimal o entero\n",
        "  numbers = \"\\d+\\.?,?\\d+\"\n",
        "  regex.append(numbers)\n",
        "\n",
        "  word = \"[0-9]*[A-Za-zñáéíóúÁÉÍÓÚ]+[0-9]*\"\n",
        "  regex.append(word)\n",
        "  \n",
        "  symbol = \"[.,\\?\\-_\\(\\)°/]+\"\n",
        "  regex.append(symbol)\n",
        "\n",
        "  ## usar re.findall(pattern, string, flags=0)\n",
        "  ## primero, por cada regex\n",
        "  for item1 in regex:\n",
        "    found_words = re.findall(item1, text, flags=0)\n",
        "\n",
        "    ## se colocan las expresiones que hicieron match en el output\n",
        "    for item2 in found_words:\n",
        "      output.append(item2)\n",
        "  \n",
        "  return output\n",
        "  ### Fín del código ###"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqlSpefv4_EH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6309d1a-9605-484c-cfd4-4da9f475da18"
      },
      "source": [
        "tokens = get_tokens(text)\n",
        "tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['05',\n",
              " '10',\n",
              " '2019',\n",
              " '61',\n",
              " '1,43',\n",
              " '53',\n",
              " '1.73',\n",
              " '1.34',\n",
              " 'Interconsulta',\n",
              " 'n',\n",
              " 'del',\n",
              " 'mes',\n",
              " 'ENFERMEDAD',\n",
              " 'RENAL',\n",
              " 'CRÓNICA',\n",
              " 'ETAPA',\n",
              " 'Fundamento',\n",
              " 'Clínico',\n",
              " 'APS',\n",
              " 'PACIENTE',\n",
              " 'DE',\n",
              " 'A',\n",
              " 'OS',\n",
              " 'CON',\n",
              " 'ANTECEDENTES',\n",
              " 'DE',\n",
              " 'HRTA',\n",
              " 'DE',\n",
              " 'LARGA',\n",
              " 'DATA',\n",
              " 'ACUDE',\n",
              " 'A',\n",
              " 'CONTROL',\n",
              " 'PSCV',\n",
              " 'CON',\n",
              " 'EXAMNES',\n",
              " 'DONDE',\n",
              " 'DESTACA',\n",
              " 'CREATININA',\n",
              " 'DE',\n",
              " 'CON',\n",
              " 'UNA',\n",
              " 'VFG',\n",
              " 'CALCULADA',\n",
              " 'DE',\n",
              " 'ML',\n",
              " 'MIN',\n",
              " '73M2',\n",
              " 'CON',\n",
              " 'UNA',\n",
              " 'CREATININA',\n",
              " 'HACE',\n",
              " 'MAS',\n",
              " 'DE',\n",
              " 'MESES',\n",
              " 'DE',\n",
              " 'Enfermedad',\n",
              " 'renal',\n",
              " 'cronica',\n",
              " 'Etapa',\n",
              " 'SOLICITO',\n",
              " 'EVALUACION',\n",
              " 'DEL',\n",
              " 'CASO',\n",
              " '°',\n",
              " '.-',\n",
              " '/',\n",
              " '/',\n",
              " '-',\n",
              " '.',\n",
              " '-',\n",
              " ',',\n",
              " '.',\n",
              " ',',\n",
              " ',',\n",
              " ',',\n",
              " '/',\n",
              " '/',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '(',\n",
              " ').']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPbgTvAW-stF"
      },
      "source": [
        "**Describa cuáles fueron sus supuestos para realizar la tokenización y compare sus tokens con los entregados por la librería nltk en el bloque de código de más abajo.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGQ7CJy3-3aH"
      },
      "source": [
        "    Describa aquí\n",
        "    Se supuso que pueden haber palabras con números al comienzo o al final de\n",
        "    ellas, como en el regex word del cuerpo de la función. También que las\n",
        "    fracciones siempre se escribirán como números con puntos o comas entre la\n",
        "    parte entera y decimal. Los símbolos especiales asociados son los escritos\n",
        "    en el regex symbol.\n",
        "\n",
        "    Al comparar los tokens con los de nltk, se puede notar que el algoritmo de\n",
        "    la libreria entrega los tokens en el orden en que aparecen en el texto,\n",
        "    contrario a lo que hace la función get_tokens() definida más arriba. Debe\n",
        "    tener más sentido sintáctico, pero para efectos de una lista desordenada,\n",
        "    es muy parecido. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYtmAXTr9KXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c561200-e879-4145-80bb-66329b4874f2"
      },
      "source": [
        "from nltk.tokenize import wordpunct_tokenize # Nltk no puede ser utilizada para los siguientes ejercicio, es sólo un ejemplo\n",
        "nltk_tokens = wordpunct_tokenize(text)\n",
        "nltk_tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Interconsulta',\n",
              " 'n',\n",
              " '°',\n",
              " '2',\n",
              " 'del',\n",
              " 'mes',\n",
              " '.-',\n",
              " '05',\n",
              " '/',\n",
              " '10',\n",
              " '/',\n",
              " '2019',\n",
              " '-',\n",
              " 'ENFERMEDAD',\n",
              " 'RENAL',\n",
              " 'CRÓNICA',\n",
              " 'ETAPA',\n",
              " '3',\n",
              " '.',\n",
              " '-',\n",
              " 'Fundamento',\n",
              " 'Clínico',\n",
              " 'APS',\n",
              " ':',\n",
              " 'PACIENTE',\n",
              " 'DE',\n",
              " '61',\n",
              " 'AÑOS',\n",
              " ',',\n",
              " 'CON',\n",
              " 'ANTECEDENTES',\n",
              " 'DE',\n",
              " 'HRTA',\n",
              " 'DE',\n",
              " 'LARGA',\n",
              " 'DATA',\n",
              " '.',\n",
              " 'ACUDE',\n",
              " 'A',\n",
              " 'CONTROL',\n",
              " 'PSCV',\n",
              " 'CON',\n",
              " 'EXAMNES',\n",
              " ',',\n",
              " 'DONDE',\n",
              " 'DESTACA',\n",
              " 'CREATININA',\n",
              " 'DE',\n",
              " '1',\n",
              " ',',\n",
              " '43',\n",
              " ',',\n",
              " 'CON',\n",
              " 'UNA',\n",
              " 'VFG',\n",
              " 'CALCULADA',\n",
              " 'DE',\n",
              " '53',\n",
              " 'ML',\n",
              " '/',\n",
              " 'MIN',\n",
              " '/',\n",
              " '1',\n",
              " '.',\n",
              " '73M2',\n",
              " '.',\n",
              " 'CON',\n",
              " 'UNA',\n",
              " 'CREATININA',\n",
              " 'HACE',\n",
              " 'MAS',\n",
              " 'DE',\n",
              " '3',\n",
              " 'MESES',\n",
              " 'DE',\n",
              " '1',\n",
              " '.',\n",
              " '34',\n",
              " '.',\n",
              " 'Enfermedad',\n",
              " 'renal',\n",
              " 'cronica',\n",
              " 'Etapa',\n",
              " '3',\n",
              " '(',\n",
              " 'SOLICITO',\n",
              " 'EVALUACION',\n",
              " 'DEL',\n",
              " 'CASO',\n",
              " ').']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5QKlXAZwN1L"
      },
      "source": [
        "` `  \n",
        "` ` \n",
        "\n",
        "**Ejercicio 2 - *Stopwords y Stemming* (1 punto).** \n",
        "` `  \n",
        "` ` \n",
        "\n",
        "Considere el siguiente corpus:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEp83zESwb2j"
      },
      "source": [
        "dataset = [\"I like human languages\", \"I like programming languages\", \"Spanish is my favorite language\"]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmjdlJWuyS2E"
      },
      "source": [
        "Diseñe una función **`get_vocab()`** que extraiga los typos de este corpus solamente tokenizando. Puede utilizar la función creada por ustedes en el  Ejercicio 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UudC-b6TzZgw"
      },
      "source": [
        "def get_vocab(dataset):\n",
        "  ### Aquí inicia tu código ###\n",
        "  \"\"\"\n",
        "  Primero vamos a tokenizar cada documento \n",
        "  y luego añadir cada token a la lista output\n",
        "  \"\"\"\n",
        "  output = []\n",
        "\n",
        "  for var in dataset:\n",
        "    tokenized = get_tokens(var)\n",
        "    \n",
        "    for token in tokenized:\n",
        "      ## revisamos que no se repitan\n",
        "      if(token not in output):\n",
        "        output.append(token)\n",
        "\n",
        "  return output\n",
        "\n",
        "  ### Aquí termina tu código ###"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-m32IoNmSwM"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNzPKiAx0Aa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65afaae-6407-45cc-f9cf-366b1af4df4d"
      },
      "source": [
        "vocab = get_vocab(dataset)\n",
        "vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'like',\n",
              " 'human',\n",
              " 'languages',\n",
              " 'programming',\n",
              " 'Spanish',\n",
              " 'is',\n",
              " 'my',\n",
              " 'favorite',\n",
              " 'language']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZLV2hWf9FN7"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "<table>\n",
        "    <tr> \n",
        "        <td>['favorite',\n",
        " 'Spanish',\n",
        " 'language',\n",
        " 'I',\n",
        " 'like',\n",
        " 'programming',\n",
        " 'languages',\n",
        " 'my',\n",
        " 'human',\n",
        " 'is'] </td> \n",
        "    </tr>\n",
        "</table> \n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3KB0fL2zk2v"
      },
      "source": [
        "Ahora diseñe reglas que usted estime convenientes tanto de **Stemming** como de **Stopwords**. Implemente una función que reciba una lista con los elementos del vocabulario, le aplique sus reglas y devuelva el vocabulario actualizado. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY7g67Ml0aby"
      },
      "source": [
        "    Explique sus reglas aquí\n",
        "\n",
        "    Primero, las stopwords se eliminan, comparándolas en base a una lista de stopwords genérica en inglés.\n",
        "\n",
        "    Segundo, se revisarán las terminaciones de los strings (los sufijos de las \n",
        "    palabras), por ejemplo, en español la palabra \"perros\" debería ser modificada\n",
        "    a \"perro\". Las palabras cuyos finales sean cortados pueden repetirse, por ejemplo \"perros\" y \"perritos\"\n",
        "    son cortadas a \"perro\", por lo que solamente se añade una vez al resultado final. \n",
        "\n",
        "    Se busca implementar una versión más sencilla del algoritmo de Porter, con el fin de no crear\n",
        "    más funciones auxiliares, quedando la simplificación del proceso dentro de pre_processing().\n",
        "    No se modelará cada palabra en base a la cadena \\[C] (VM)^n \\[V], sino que solamente se revisará \n",
        "    el fin de cada string junto con las reglas, para luego modificarlo en caso de que así lo indiquen.\n",
        "\n",
        "    Las reglas en cuestión están definidas dentro del código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrKDtM_H0XlE"
      },
      "source": [
        "def pre_processing(vocab): \n",
        "  ### Aquí inicia tu código ###\n",
        "  ## Lista de stopwords\n",
        "  stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \n",
        "               \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \n",
        "               \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \n",
        "               \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \n",
        "               \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \n",
        "               \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \n",
        "               \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \n",
        "               \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \n",
        "               \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \n",
        "               \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \n",
        "               \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \n",
        "               \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \n",
        "               \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
        "               \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \n",
        "               \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \n",
        "               \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \n",
        "               \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \n",
        "               \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \n",
        "               \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \n",
        "               \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \n",
        "               \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \n",
        "               \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \n",
        "               \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\", \n",
        "               \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \n",
        "               \"yourselves\"]\n",
        "\n",
        "  ## Se filtran las stopwords\n",
        "  for word in vocab:\n",
        "    ## se pasa a minuscula para comparar con la lista\n",
        "    if word.lower() in stopwords:\n",
        "      ## dado que cada palabra está una sola vez en vocab,\n",
        "      ## remove() es suficiente.\n",
        "      vocab.remove(word)  \n",
        "\n",
        "  output = []\n",
        "\n",
        "  ## Ahora se procesa la parte de las conjugaciones y sufijos, cortando los \n",
        "  ## finales de los strings para tratar de dejar solamente las raices.\n",
        "  for word in vocab:\n",
        "    if (word[-4:].lower() == \"sses\"):\n",
        "      word = word[0:-2]\n",
        "\n",
        "    if (word[-3:].lower() == \"ies\"):\n",
        "      word = word[0:-2]\n",
        "\n",
        "    if (word[-2:].lower() == \"ss\"):\n",
        "      word = word\n",
        "\n",
        "    if (word[-1:].lower() == \"s\"):\n",
        "      word = word[0:-1]\n",
        "\n",
        "    if (word[-3:].lower() == \"eed\"):\n",
        "      word = word[0:-1]\n",
        "\n",
        "    if (word[-2:].lower() == \"ed\"):\n",
        "      word = word[0:-2]\n",
        "\n",
        "    if (word[-3:].lower() == \"ing\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "\n",
        "    if (word[-1:].lower() == \"y\"):\n",
        "      word = word[0:-1] + \"i\"\n",
        "      \n",
        "    if (word[-7:].lower() == \"ational\"):\n",
        "      word = word[0:-7] + \"ate\"\n",
        "      \n",
        "    if (word[-6:].lower() == \"tional\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-4:].lower() == \"enci\"):\n",
        "      word = word[0:-1] + \"e\"\n",
        "      \n",
        "    if (word[-4:].lower() == \"anci\"):\n",
        "      word = word[0:-1] + \"e\"\n",
        "      \n",
        "    if (word[-4:].lower() == \"izer\"):\n",
        "      word = word[0:-1]\n",
        "      \n",
        "    if (word[-4:].lower() == \"abli\"):\n",
        "      word = word[0:-1] + \"e\"\n",
        "      \n",
        "    if (word[-4:].lower() == \"alli\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-4:].lower() == \"entli\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-3:].lower() == \"eli\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-5:].lower() == \"ousli\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-7:].lower() == \"ization\"):\n",
        "      word = word[0:-5] + \"e\"\n",
        "      \n",
        "    if (word[-5:].lower() == \"ation\"):\n",
        "      word = word[0:-3] + \"e\"\n",
        "      \n",
        "    if (word[-4:].lower() == \"ator\"):\n",
        "      word = word[0:-2] + \"e\"\n",
        "      \n",
        "    if (word[-5:].lower() == \"alism\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-7:].lower() == \"iveness\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-7:].lower() == \"fulness\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-7:].lower() == \"ousness\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-5:].lower() == \"aliti\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-5:].lower() == \"iviti\"):\n",
        "      word = word[0:-3] + \"e\"\n",
        "      \n",
        "    if (word[-6:].lower() == \"biliti\"):\n",
        "      word = word[0:-5] + \"le\"\n",
        "      \n",
        "\n",
        "    if (word[-5:].lower() == \"icate\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-5:].lower() == \"ative\"):\n",
        "      word = word[0:-5]\n",
        "      \n",
        "    if (word[-5:].lower() == \"alize\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-5:].lower() == \"iciti\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ical\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ful\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ness\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "\n",
        "    if (word[-2:].lower() == \"al\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ance\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ence\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-2:].lower() == \"er\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-2:].lower() == \"ic\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-4:].lower() == \"able\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ible\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ant\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-5:].lower() == \"ement\"):\n",
        "      word = word[0:-5]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ment\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ent\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ion\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-2:].lower() == \"ou\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ism\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ate\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"iti\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ous\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ive\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ize\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "\n",
        "    if (word[-1:].lower() == \"e\"):\n",
        "      word = word[0:-1]\n",
        "\n",
        "    output.append(word)\n",
        "      \n",
        "  return output\n",
        "  ### Aquí termina tu código ###"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onOSuS-_mL2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec446615-fa1c-4a3b-9276-8af2d0b94f9c"
      },
      "source": [
        "vocab = pre_processing(vocab)\n",
        "vocab"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lik', 'human', 'languag', 'programm', 'Spanish', 'mi', 'favorit', 'languag']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65IwHx11uA75"
      },
      "source": [
        "` `  \n",
        "` ` \n",
        "\n",
        "**Ejercicio 3 - *Bag of Words* 🐶🐈(0.5 puntos).** \n",
        "` `  \n",
        "` ` \n",
        "\n",
        "Considere el siguiente corpus, donde cada elemento del arreglo representa un documento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh-utMuozhsK"
      },
      "source": [
        "d0 = 'El perro se come la comida y después se duerme'\n",
        "d1 = 'El perro se despierta y después empieza a ladrar'\n",
        "d2 = 'El perro ladra y después se come la comida'\n",
        "d3 = 'El gato se come la comida y después se duerme'\n",
        "d4 = 'El gato se despierta y después empieza a maullar'\n",
        "d5 = 'El gato maulla y después se come la comida'\n",
        "dataset = [d0, d1, d2, d3, d4, d5]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH7ne8C6ltvE"
      },
      "source": [
        "El objetivo de este ejercicio es determinar cuáles de  los documentos entregados son los más similares entre sí. Para ello utilizaremos la técnica TF-IDF. \n",
        "\n",
        "Como los algoritmos de Machine Learning no comprenden el texto en lenguaje natural, estos documentos deben ser convertidos a vectores numéricos. La representación más simple vista en clases es el **Bag of Words**, método mediante el cuál se cuentan las apariciones de cada palabra en cada uno de los documentos entregados.\n",
        "\n",
        "Implemente la función **`bag_of_words()`**, que reciba como input un arreglo de documentos y devuelva un pandas dataframe con la representación Bag of Words de los documentos entregados. En esta representación las columnas son el vocabulario y las filas representa las apariciones de cada una de las palabras en los documentos. En otras palabras, cada fila representa el bow de un determinado documento.\n",
        "\n",
        "\n",
        "Por ejemplo para el siguiente dataset: \n",
        "\n",
        "```\n",
        "dataset = ['El perro ladra', 'El perro come']\n",
        "```\n",
        "\n",
        "Debiese entregarnos lo siguiente:\n",
        "\n",
        "\n",
        "|   | el | perro | ladra | come |\n",
        "|---|----|-------|------|-------|\n",
        "| 0 | 1  | 1     | 1    | 0     |\n",
        "| 1 | 1  | 1     | 0    | 1     |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXDMAyFmnq5j"
      },
      "source": [
        "def bag_of_words(dataset):\n",
        "  ### Aquí inicia tu código ###\n",
        "  # Arreglo con los conteo de cada documento\n",
        "  data = []\n",
        "  types = []\n",
        "  doc_tokens_list =[]\n",
        "\n",
        "  # Se colocan todos los tipos en \"type\"\n",
        "  for document in dataset:\n",
        "    doc_tokens = get_tokens(document)\n",
        "    doc_tokens_list.append(doc_tokens)\n",
        "    for token in doc_tokens:\n",
        "      sub_data = []\n",
        "      if token not in types:\n",
        "        types.append(token)\n",
        "\n",
        "  # Ya encontrados los tipos se va viendo cuantas veces se \n",
        "  # encuentran en los documentos\n",
        "  for doc_tokens in doc_tokens_list:\n",
        "    sub_data = [0] * len(types)\n",
        "    for token in doc_tokens:\n",
        "      sub_data[types.index(token)] += 1\n",
        "    data.append(sub_data)\n",
        "\n",
        "  return pd.DataFrame(data, columns=types, \n",
        "                      index=[\"d0\", \"d1\", \"d2\", \"d3\", \"d4\", \"d5\"])\n",
        "  ### Aquí termina tu código ###"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okfo-nEQmW1R"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Kk9GwCoDW8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "6d24c555-50b8-4cbe-ec13-2e4443dbfe71"
      },
      "source": [
        "dataset_bow = bag_of_words(dataset)\n",
        "dataset_bow"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>El</th>\n",
              "      <th>perro</th>\n",
              "      <th>se</th>\n",
              "      <th>come</th>\n",
              "      <th>la</th>\n",
              "      <th>comida</th>\n",
              "      <th>y</th>\n",
              "      <th>después</th>\n",
              "      <th>duerme</th>\n",
              "      <th>despierta</th>\n",
              "      <th>empieza</th>\n",
              "      <th>a</th>\n",
              "      <th>ladrar</th>\n",
              "      <th>ladra</th>\n",
              "      <th>gato</th>\n",
              "      <th>maullar</th>\n",
              "      <th>maulla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>d0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    El  perro  se  come  la  comida  ...  a  ladrar  ladra  gato  maullar  maulla\n",
              "d0   1      1   2     1   1       1  ...  0       0      0     0        0       0\n",
              "d1   1      1   1     0   0       0  ...  1       1      0     0        0       0\n",
              "d2   1      1   1     1   1       1  ...  0       0      1     0        0       0\n",
              "d3   1      0   2     1   1       1  ...  0       0      0     1        0       0\n",
              "d4   1      0   1     0   0       0  ...  1       0      0     1        1       0\n",
              "d5   1      0   1     1   1       1  ...  0       0      0     1        0       1\n",
              "\n",
              "[6 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeR5ADGz-MPa"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "|    | El | perro | se | come | la | comida | y | después | duerme | despierta | empieza | a | ladrar | ladra | gato | maullar | maulla |\n",
        "|----|---:|------:|---:|-----:|---:|-------:|--:|--------:|-------:|----------:|--------:|--:|-------:|------:|-----:|--------:|-------:|\n",
        "| d0 |  1 |     1 |  2 |    1 |  1 |      1 | 1 |       1 |      1 |         0 |       0 | 0 |      0 |     0 |    0 |       0 |      0 |\n",
        "| d1 |  1 |     1 |  1 |    0 |  0 |      0 | 1 |       1 |      0 |         1 |       1 | 1 |      1 |     0 |    0 |       0 |      0 |\n",
        "| d2 |  1 |     1 |  1 |    1 |  1 |      1 | 1 |       1 |      0 |         0 |       0 | 0 |      0 |     1 |    0 |       0 |      0 |\n",
        "| d3 |  1 |     0 |  2 |    1 |  1 |      1 | 1 |       1 |      1 |         0 |       0 | 0 |      0 |     0 |    1 |       0 |      0 |\n",
        "| d4 |  1 |     0 |  1 |    0 |  0 |      0 | 1 |       1 |      0 |         1 |       1 | 1 |      0 |     0 |    1 |       1 |      0 |\n",
        "| d5 |  1 |     0 |  1 |    1 |  1 |      1 | 1 |       1 |      0 |         0 |       0 | 0 |      0 |     0 |    1 |       0 |      1 |\n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4OXMz7opWcd"
      },
      "source": [
        "` `  \n",
        "` ` \n",
        "\n",
        "**Ejercicio 4 - *Calcular TF* (0.5 puntos):** Ahora debemos usar el dataframe del ejercicio anterior para calcular la matriz de TF normalizada por la máxima frecuencia ${max_i({\\text{tf}_{i,j})}}$, donde\n",
        "i corresponde al índice de las filas (bow) y j al de las columnas (palabras). Es decir, dividir cada bow en la cantidad de veces de la palabra que aparezca más veces en ese vector. \n",
        "\n",
        "\n",
        "$$\\text{nft}_{i,j} = \\frac{\\text{tf}_{i,j}}{max_i({\\text{tf}_{i,j})}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGMAGcYooi6B",
        "outputId": "1e8f0fd3-d084-4aea-e3d0-46bf3bbf9ebd"
      },
      "source": [
        "max_i = dataset_bow.max(axis=1)[0]\n",
        "max_i"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWE16xhBpswc"
      },
      "source": [
        "def calc_tf(dataset_bow):\n",
        "    df = dataset_bow\n",
        "    ### Aquí inicia tu código ###\n",
        "    # Se calcula el maximo de cada fila\n",
        "    max_i = df.max(axis=1)\n",
        "    # Se divide cada valor por el maximo de su fila\n",
        "    for i in range(df.shape[0]):\n",
        "      df.iloc[i] = df.iloc[i].apply(lambda x: x/max_i[i])\n",
        "    return df\n",
        "    ### Aquí termina tu código ###"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZQPZe3JmYqy"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ2h8jEYp4nZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2dbd0a6e-7756-4685-a24e-1b6bdda82ae4"
      },
      "source": [
        "tf = calc_tf(dataset_bow)\n",
        "tf"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>El</th>\n",
              "      <th>perro</th>\n",
              "      <th>se</th>\n",
              "      <th>come</th>\n",
              "      <th>la</th>\n",
              "      <th>comida</th>\n",
              "      <th>y</th>\n",
              "      <th>después</th>\n",
              "      <th>duerme</th>\n",
              "      <th>despierta</th>\n",
              "      <th>empieza</th>\n",
              "      <th>a</th>\n",
              "      <th>ladrar</th>\n",
              "      <th>ladra</th>\n",
              "      <th>gato</th>\n",
              "      <th>maullar</th>\n",
              "      <th>maulla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>d0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d3</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     El  perro   se  come   la  ...  ladrar  ladra  gato  maullar  maulla\n",
              "d0  0.5    0.5  1.0   0.5  0.5  ...     0.0    0.0   0.0      0.0     0.0\n",
              "d1  1.0    1.0  1.0   0.0  0.0  ...     1.0    0.0   0.0      0.0     0.0\n",
              "d2  1.0    1.0  1.0   1.0  1.0  ...     0.0    1.0   0.0      0.0     0.0\n",
              "d3  0.5    0.0  1.0   0.5  0.5  ...     0.0    0.0   0.5      0.0     0.0\n",
              "d4  1.0    0.0  1.0   0.0  0.0  ...     0.0    0.0   1.0      1.0     0.0\n",
              "d5  1.0    0.0  1.0   1.0  1.0  ...     0.0    0.0   1.0      0.0     1.0\n",
              "\n",
              "[6 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuNXxe3pis7x",
        "outputId": "dfb6a1a3-92f2-412e-ff83-2958c60f7b82"
      },
      "source": [
        "list(tf.columns)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['El',\n",
              " 'perro',\n",
              " 'se',\n",
              " 'come',\n",
              " 'la',\n",
              " 'comida',\n",
              " 'y',\n",
              " 'después',\n",
              " 'duerme',\n",
              " 'despierta',\n",
              " 'empieza',\n",
              " 'a',\n",
              " 'ladrar',\n",
              " 'ladra',\n",
              " 'gato',\n",
              " 'maullar',\n",
              " 'maulla']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOzdRwx9_UMM"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "|    |  El | perro |  se | come |  la | comida |   y | después | duerme | despierta | empieza |   a | ladrar | ladra | gato | maullar | maulla |\n",
        "|----|----:|------:|----:|-----:|----:|-------:|----:|--------:|-------:|----------:|--------:|----:|-------:|------:|-----:|--------:|-------:|\n",
        "| d0 | 0.5 |   0.5 | 1.0 |  0.5 | 0.5 |    0.5 | 0.5 |     0.5 |    0.5 |       0.0 |     0.0 | 0.0 |    0.0 |   0.0 |  0.0 |     0.0 |    0.0 |\n",
        "| d1 | 1.0 |   1.0 | 1.0 |  0.0 | 0.0 |    0.0 | 1.0 |     1.0 |    0.0 |       1.0 |     1.0 | 1.0 |    1.0 |   0.0 |  0.0 |     0.0 |    0.0 |\n",
        "| d2 | 1.0 |   1.0 | 1.0 |  1.0 | 1.0 |    1.0 | 1.0 |     1.0 |    0.0 |       0.0 |     0.0 | 0.0 |    0.0 |   1.0 |  0.0 |     0.0 |    0.0 |\n",
        "| d3 | 0.5 |   0.0 | 1.0 |  0.5 | 0.5 |    0.5 | 0.5 |     0.5 |    0.5 |       0.0 |     0.0 | 0.0 |    0.0 |   0.0 |  0.5 |     0.0 |    0.0 |\n",
        "| d4 | 1.0 |   0.0 | 1.0 |  0.0 | 0.0 |    0.0 | 1.0 |     1.0 |    0.0 |       1.0 |     1.0 | 1.0 |    0.0 |   0.0 |  1.0 |     1.0 |    0.0 |\n",
        "| d5 | 1.0 |   0.0 | 1.0 |  1.0 | 1.0 |    1.0 | 1.0 |     1.0 |    0.0 |       0.0 |     0.0 | 0.0 |    0.0 |   0.0 |  1.0 |     0.0 |    1.0 |\n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqgW4Ni4t0xC"
      },
      "source": [
        "` `  \n",
        "` ` \n",
        "\n",
        "**Ejercicio 5 - *Calcular IDF* (0.5 punto):**\n",
        "\n",
        "\n",
        "Implementar `calc_idf(dataset_bow)`. Este debe retornar un diccionario en donde las llaves sean las palabras y los valores sean el calculo de cada idf por palabra.\n",
        "\n",
        "Recordar que $idf_{t_i} = log_{10}\\frac{N}{n_i}$ con $N = $ número de documentos y $n_i = $ Número de documentos que contienen la palabra $t_i$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thhDY1-Ht6T5"
      },
      "source": [
        "def calc_idf(dataset_bow):\n",
        "    ### Aquí inicia tu código ###\n",
        "\n",
        "    output = {}\n",
        "\n",
        "    ## 6 documentos originalmente\n",
        "    N = 6\n",
        "    \n",
        "    ## Recordar que dataset_bow es una matriz que indica\n",
        "    ## la cantidad de veces que cada palabra se repite en\n",
        "    ## cada documento. Cada columna tiene la palabra y las\n",
        "    ## filas representan la cantidad de veces que la palabra\n",
        "    ## aparece en cada documento.\n",
        "\n",
        "    ## Se itera sobre las columnas\n",
        "    for col_name in dataset_bow.columns[0:]:\n",
        "\n",
        "      ## Se crea un subset con las filas de valores > 0\n",
        "      docs = dataset_bow.loc[dataset_bow[col_name] > 0, col_name]\n",
        "      \n",
        "      ## Se revisa el tamaño del objeto\n",
        "      n_i = docs.shape[0]\n",
        "      frac = float(N/n_i)\n",
        "\n",
        "      idf_ti = np.log10(frac)\n",
        "\n",
        "      output[col_name] = idf_ti\n",
        "\n",
        "    return output\n",
        "    ### Aquí termina tu código ###"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR_j3pYemcAc"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro-OMGpduC0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e07dbc6-ebfb-442f-f9f5-d665b53c3b4c"
      },
      "source": [
        "idf = calc_idf(dataset_bow)\n",
        "idf"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'El': 0.0,\n",
              " 'a': 0.47712125471966244,\n",
              " 'come': 0.17609125905568124,\n",
              " 'comida': 0.17609125905568124,\n",
              " 'despierta': 0.47712125471966244,\n",
              " 'después': 0.0,\n",
              " 'duerme': 0.47712125471966244,\n",
              " 'empieza': 0.47712125471966244,\n",
              " 'gato': 0.3010299956639812,\n",
              " 'la': 0.17609125905568124,\n",
              " 'ladra': 0.7781512503836436,\n",
              " 'ladrar': 0.7781512503836436,\n",
              " 'maulla': 0.7781512503836436,\n",
              " 'maullar': 0.7781512503836436,\n",
              " 'perro': 0.3010299956639812,\n",
              " 'se': 0.0,\n",
              " 'y': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwjMAUUJ_i17"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "<table>\n",
        "    <tr> \n",
        "        <td> {'El': 0.0,\n",
        " 'a': 0.47712125471966244,\n",
        " 'come': 0.17609125905568124,\n",
        " 'comida': 0.17609125905568124,\n",
        " 'despierta': 0.47712125471966244,\n",
        " 'después': 0.0,\n",
        " 'duerme': 0.47712125471966244,\n",
        " 'empieza': 0.47712125471966244,\n",
        " 'gato': 0.3010299956639812,\n",
        " 'la': 0.17609125905568124,\n",
        " 'ladra': 0.7781512503836436,\n",
        " 'ladrar': 0.7781512503836436,\n",
        " 'maulla': 0.7781512503836436,\n",
        " 'maullar': 0.7781512503836436,\n",
        " 'perro': 0.3010299956639812,\n",
        " 'se': 0.0,\n",
        " 'y': 0.0} </td> \n",
        "    </tr>\n",
        "</table> \n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzKAzJtSJ7gx"
      },
      "source": [
        "Puede notar el bajo puntaje otorgado a las palabras que más se repiten! 😮"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D17lm6l9uJPo"
      },
      "source": [
        "**Ejercicio 6 - *Calcular TF-IDF & concluir similitud de documentos.* (1 punto)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc7FTQ19Kcwo"
      },
      "source": [
        "Programe la función `calc_tf_idf(tf, idf)` que entrega el dataframe TF-IDF asociado al dataset que estamos analizando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9knMl0KguMwo"
      },
      "source": [
        "def calc_tf_idf(tf, idf):\n",
        "    ### Aquí inicia tu código ###\n",
        "    # Hacemos una copia para modificar\n",
        "    df = tf\n",
        "    columns = list(df.columns)\n",
        "    # A cada columna se aplica la funcion TF-IDF\n",
        "    for column in columns:\n",
        "      df[column] = df[column].apply(lambda x: x*idf.get(column))\n",
        "    return df\n",
        "    ### Aquí termina tu código ### "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRzIr1nQmepp"
      },
      "source": [
        "***Test.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8z6jaq2uPEo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c712fa02-c858-488c-a66f-65eb62f81e00"
      },
      "source": [
        "tf_idf = calc_tf_idf(tf, idf)\n",
        "tf_idf"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>El</th>\n",
              "      <th>perro</th>\n",
              "      <th>se</th>\n",
              "      <th>come</th>\n",
              "      <th>la</th>\n",
              "      <th>comida</th>\n",
              "      <th>y</th>\n",
              "      <th>después</th>\n",
              "      <th>duerme</th>\n",
              "      <th>despierta</th>\n",
              "      <th>empieza</th>\n",
              "      <th>a</th>\n",
              "      <th>ladrar</th>\n",
              "      <th>ladra</th>\n",
              "      <th>gato</th>\n",
              "      <th>maullar</th>\n",
              "      <th>maulla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>d0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150515</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.238561</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.238561</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.150515</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.778151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     El     perro   se      come  ...     ladra      gato   maullar    maulla\n",
              "d0  0.0  0.150515  0.0  0.088046  ...  0.000000  0.000000  0.000000  0.000000\n",
              "d1  0.0  0.301030  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000\n",
              "d2  0.0  0.301030  0.0  0.176091  ...  0.778151  0.000000  0.000000  0.000000\n",
              "d3  0.0  0.000000  0.0  0.088046  ...  0.000000  0.150515  0.000000  0.000000\n",
              "d4  0.0  0.000000  0.0  0.000000  ...  0.000000  0.301030  0.778151  0.000000\n",
              "d5  0.0  0.000000  0.0  0.176091  ...  0.000000  0.301030  0.000000  0.778151\n",
              "\n",
              "[6 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBG2qfwv_6HK"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "|    |  El |    perro |  se |     come |       la |   comida |   y | después |   duerme | despierta |  empieza |        a |   ladrar |    ladra |     gato |  maullar |   maulla |\n",
        "|----|----:|---------:|----:|---------:|---------:|---------:|----:|--------:|---------:|----------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|\n",
        "| d0 | 0.0 | 0.150515 | 0.0 | 0.088046 | 0.088046 | 0.088046 | 0.0 |     0.0 | 0.238561 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 |\n",
        "| d1 | 0.0 | 0.301030 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 |     0.0 | 0.000000 |  0.477121 | 0.477121 | 0.477121 | 0.778151 | 0.000000 | 0.000000 | 0.000000 | 0.000000 |\n",
        "| d2 | 0.0 | 0.301030 | 0.0 | 0.176091 | 0.176091 | 0.176091 | 0.0 |     0.0 | 0.000000 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.778151 | 0.000000 | 0.000000 | 0.000000 |\n",
        "| d3 | 0.0 | 0.000000 | 0.0 | 0.088046 | 0.088046 | 0.088046 | 0.0 |     0.0 | 0.238561 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.150515 | 0.000000 | 0.000000 |\n",
        "| d4 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 |     0.0 | 0.000000 |  0.477121 | 0.477121 | 0.477121 | 0.000000 | 0.000000 | 0.301030 | 0.778151 | 0.000000 |\n",
        "| d5 | 0.0 | 0.000000 | 0.0 | 0.176091 | 0.176091 | 0.176091 | 0.0 |     0.0 | 0.000000 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.301030 | 0.000000 | 0.778151 |\n",
        "\n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmVlbpzMp5NU"
      },
      "source": [
        "Ahora que tenemos el dataframe de TF-IDF, nos queda calcular la similitud coseno entre todos los vectores. Notar que la matriz resultante será una matriz simétrica. Implemente la función *cosine_similarity(v1, v2)* que recibe dos vectores (v1 y v2) y calcula la similitud coseno entre ambos vectores. Concluya cuáles son los dos documentos más similares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgUtgBkQAae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20082845-7311-41e9-ec52-6eca8d29c5c4"
      },
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "  ### Aquí inicia tu código ###\n",
        "  # Se calcula el producto punto\n",
        "  a = np.dot(v1,v2)\n",
        "  # Se calcula el modulo de cada vector y se multiplica\n",
        "  b = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "  # Finalemente se divide ambos resultados anteriores\n",
        "  return a/b\n",
        "  ### Aquí termina tu código ### \n",
        "\n",
        "similarity_matrix = np.zeros((6,6))\n",
        "for i, v1 in enumerate(tf_idf.index.values):\n",
        "  for j, v2 in enumerate(tf_idf.index.values):\n",
        "      similarity = cosine_similarity(tf_idf.loc[v1].values, tf_idf.loc[v2].values)\n",
        "      similarity_matrix[i][j] = similarity\n",
        "print(similarity_matrix)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.12032418 0.3223436  0.77967014 0.         0.1632828 ]\n",
            " [0.12032418 1.         0.08686457 0.         0.4952126  0.        ]\n",
            " [0.3223436  0.08686457 1.         0.1632828  0.         0.11787732]\n",
            " [0.77967014 0.         0.1632828  1.         0.12032418 0.3223436 ]\n",
            " [0.         0.4952126  0.         0.12032418 1.         0.08686457]\n",
            " [0.1632828  0.         0.11787732 0.3223436  0.08686457 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FlltqIqRxf"
      },
      "source": [
        "    Escriba su respuesta aquí\n",
        "\n",
        "    El documento d0 y el documento d3 son los mas similares con una simulitud de\n",
        "    0.78 aproximadamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUAc1zX0Lg16"
      },
      "source": [
        "![gato](https://www.elagoradiario.com/wp-content/uploads/2020/05/Gato-mascarilla.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1A95IaXLHaB"
      },
      "source": [
        "**Cualquier recomendación que nos quisieran dar para una futura tarea es bienvenid@!**"
      ]
    }
  ]
}