{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Tarea 1 - CC6205 Natural Language Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbrMqMPyncTC"
      },
      "source": [
        "# **Tarea 1 - CC6205 Natural Language Processing 游닄**\n",
        "\n",
        "**Integrantes: Vicente Ardiles S. - Rodrigo I. Oportot**\n",
        "\n",
        "**Fecha l칤mite de entrega 游늱:** Lunes 12 de Abril.\n",
        "\n",
        "**Tiempo estimado de dedicaci칩n: 6hrs**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1HFX-9PpxF9"
      },
      "source": [
        "` ` \n",
        "\n",
        "\n",
        "Bienvenid@s a la primera tarea en el curso de Natural Language Processing (NLP). Esta tarea tiene como objetivo evaluar los contenidos te칩ricos de las primeras semanas de clases, enfocado principalmente en ***Information Retrieval (IR)*** y ***Vector Space Models***. Si a칰n no han visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "La tarea consta de una parte te칩rica que busca evaluar conceptos vistos en clases. Seguido por una parte pr치ctica con el f칤n de introducirlos a la programaci칩n en Python enfocada en NLP. \n",
        "\n",
        "` ` \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Instrucciones:**\n",
        "- La tarea se realiza en grupos de **m치ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav칠s de u-cursos a m치s tardar el d칤a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisi칩n tu c칩digo ser치 ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci칩n. \n",
        "- Est치 **PROHIBIDO** usar cualquier librer칤a que implemente los algoritmos pedidos (Spacy, scikit, etc). S칩lo se podr치n utilizar las librer칤as importadas al inicio de la secci칩n de pr치ctica.\n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav칠s del canal de Discord del curso. \n",
        "\n",
        "\n",
        "\n",
        "Ahora s칤, empecemos! 游땏游\n",
        "\n",
        "` ` \n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "\n",
        "Slides:\n",
        "    \n",
        "- [Introducci칩n al curso](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-introduction.pdf)\n",
        "- [Vector Space Model / Information Retrieval](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-IR.pdf)    \n",
        "\n",
        "Videos: \n",
        "\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: CC6205 - Procesamiento de Lenguaje Natural: Introducci칩n parte I](https://www.youtube.com/watch?v=HEKTNOttGvU)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: CC6205 - Procesamiento de Lenguaje Natural: Introducci칩n parte II](https://www.youtube.com/watch?v=P8cwnI-f-Kg)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Vector Space Model and Information Retrieval parte 1](https://youtu.be/FXIVClF370w)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Vector Space Model and Information Retrieval parte 2](https://youtu.be/f8nG1EMmPZk)\n",
        "\n",
        "` ` \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJshpe1yrKJr"
      },
      "source": [
        "## **1 - Preguntas te칩ricas 游늿 (2 puntos).** ##\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBEDKXBPoA7w"
      },
      "source": [
        "Las siguientes celdas contienen preguntas acerca del contenido visto en clases y en el material del curso.  Contestar cada pregunta en su celda correspondiente y **no extenderse m치s de 5 lineas** . 游똂\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNJPR1kMrw9R"
      },
      "source": [
        "**Pregunta 1 (0.5 puntos): 쯇or qu칠 el an치lisis del lenguaje humano es una tarea compleja? Mencione dos razones.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlTBYHEptdde"
      },
      "source": [
        "` ` \n",
        "\n",
        "    Respuesta: \n",
        "    Una raz칩n es su ambig칲edad, uno puede agregar o quitar una palabra de una oraci칩n y cambiar totalmente el contexto y sentido de ella, sumado a que el lenguaje evoluciona con el tiempo, pudiendo no tener el mismo significado seg칰n la 칠poca. Por un lado m치s t칠cnico, est치 la morfolog칤a de las palabras, no se pudiendo inferir una relaci칩n sem치ntica entre palabras por las letras que la componen (casa y caza, por ejemplo).\n",
        "\n",
        "\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv8yjG94r6FC"
      },
      "source": [
        "**Pregunta 2 (0.5 puntos): 쮺u치l es la diferencia entre Natural Language Processing (NLP) y Computational Linguistics?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmC3uYHtctH"
      },
      "source": [
        "` ` \n",
        "\n",
        "    Respuesta:\n",
        "    PLN se encarga de la metodolog칤a y dise침os para resolver problemas (tasks) concretos que involucren lenguaje natural, con algoritmos y representaciones que lo trabajen. Computational Linguistics es el estudio de los procesos computacionales asociados al entendimiento del lenguaje natural, tratando de entender el lenguaje desde un punto de vista cient칤fico/ling칲ista/computacional.\n",
        "    \n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-0imOaFsRK5"
      },
      "source": [
        "**Pregunta 3 (0.5 puntos): 쯈u칠 es el Machine Learning supersivado? 쯇or qu칠 es importante para el campo de NLP?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY6mfCE7tb1x"
      },
      "source": [
        "` ` \n",
        "\n",
        "    Respuesta:\n",
        "    Son t칠cnicas de ML donde el aprendizaje de los algoritmos que resolver치n las tasks son estudiados e intervenidas a medida que se desarrollan, construyendo las reglas del lenguaje estudiado a trav칠s de un conjunto de datos ya conocidos (entrenamiento), etiquetados previamente. Destaca en NLP dada la dificultad de formalizar el lenguaje natural, siendo una herramienta muy 칰til para abordar tal problema.\n",
        "\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVMXilrYsiSZ"
      },
      "source": [
        "**Pregunta 4 (0.5 puntos): 쮺u치les son las diferencias entre usar Deep learning y Machine Learning cl치sico (empirismo) para un problema de NLP? Ejemplifique con alguna task.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QXWMzpTtBZL"
      },
      "source": [
        "` ` \n",
        "\n",
        "    Respuesta:\n",
        "    El empirismo se caracterizaba por explotar manualmente el corpus a estudiar, luego usando modelos de ML supervisado y conceptos estad칤sticos. DL reemplaza lo anterior con algoritmos que aprenden sobre la marcha las reglas del lenguaje procesado. Ejemplo: NER con empirismo trabaja los datos crudos extrayendo atributos para procesarlos con ML supervisado, en cambio, en DL los algoritmos hacen la mayor parte del trabajo sin ese pre-procesamiento.\n",
        "    \n",
        "\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fpsz2pQt8x5"
      },
      "source": [
        "## **2 - Preguntas pr치cticas 游눹 (4 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB92kQXspvbR"
      },
      "source": [
        "Esta segunda secci칩n incluye ejercicios de programaci칩n 游뱇. Leer atentamente las instrucciones entregadas a continuaci칩n para facilitar el proceso de revisi칩n de sus trabajos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PosWgWgRxHKp"
      },
      "source": [
        "` ` \n",
        "\n",
        "**Instrucciones:**\n",
        "\n",
        "\n",
        "\n",
        "- Escribe tu c칩digo entre las lineas de comentarios **### Aqu칤 inicia tu c칩digo ###** y **### Aqu칤 termina tu c칩digo ###**.\n",
        "- Cuando el ejercicio incluya un bloque llamado ***Test***, comprueba que el resultado de la ejecuci칩n coincida con el resultado esperado.\n",
        "- Recuerde siempre mantener buenas pr치cticas de c칩digo.\n",
        "- Est치 permitido s칩lo utilizar las librer칤as importadas antes del Ejercicio 1.\n",
        "- **Recordar** que: *Documento = Oraci칩n. Dataset = Corpus. Vocabulario = Typos*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBrmFHXhqUww"
      },
      "source": [
        "**Ejemplo:** Implemente una funci칩n **`hello_world()`** que imprima en pantalla `\"Hello World\"`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu7cIsawyJHx"
      },
      "source": [
        "def hello_world():\n",
        "  ### Aqu칤 inicia tu c칩digo ###\n",
        "  print(\"Hello World\")\n",
        "  ### Aqu칤 termina tu c칩digo ###"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz6klw12lwbW"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac-WMk2dyQbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858824c6-4bf6-4786-9a7b-9343dcb49141"
      },
      "source": [
        "hello_world()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIoiAMxtyUjQ"
      },
      "source": [
        "***Resultado esperado***: \n",
        "<table>\n",
        "    <tr> \n",
        "        <td> Hello World </td> \n",
        "    </tr>\n",
        "</table> \n",
        "\n",
        "``\n",
        "``\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlPyrPXiH0l4"
      },
      "source": [
        "Estas son las librer칤as permitidas. Si quieren utilizar alguna librer칤a adicional, pueden realizar la consulta a trav칠s de Discord. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtP6Emjo1kF0"
      },
      "source": [
        "import math\n",
        "import codecs\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSN4bBoY2Td4"
      },
      "source": [
        "` ` \n",
        "\n",
        "**Ejercicio 1 - *Tokenizaci칩n* (0.5 puntos).** \n",
        "` `  \n",
        "` ` \n",
        "\n",
        "En el primer ejercicio veremos la dificultad 游땯 de tokenizar textos no estructurados, destacando la importancia de tener librer칤as que realicen este trabajo. \n",
        "\n",
        "El archivo adjunto al enunciado de la tarea contiene una interconsulta m칠dica real en Chile, estos textos son frecuentemente utilizados para obtener informaci칩n epidemiol칩gica y realizar estudios estad칤sticos a partir de ellos. Es por esto que poder analizar el texto de manera correcta es fundamental. \n",
        "\n",
        "\n",
        "Ejecute el c칩digo a continuaci칩n para cargar el ejemplo. Recuerde realizar la modificaci칩n al directorio en caso que el archivo no se encuentre en el mismo directorio del Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnNUYlWo21g4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893fa78b-ae9f-4cae-d384-be879a45ef2d"
      },
      "source": [
        "text = codecs.open('/content/sample_data/interconsulta.txt', 'r', 'UTF-8').read()\n",
        "print(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interconsulta n춿2 del mes.-05/10/2019\r\n",
            "- ENFERMEDAD RENAL  CR칍NICA ETAPA 3. \r\n",
            "- Fundamento Cl칤nico APS: PACIENTE  DE 61 A칌OS, \tCON ANTECEDENTES DE HRTA DE LARGA DATA.ACUDE A CONTROL PSCV CON EXAMNES, DONDE DESTACA CREATININA DE 1,43,\r\n",
            " CON UNA VFG CALCULADA DE 53 ML/MIN/1.73M2. CON UNA CREATININA HACE MAS DE 3 MESES DE 1.34.\r\n",
            "                   \r\n",
            "Enfermedad renal cronica Etapa 3(SOLICITO EVALUACION DEL CASO).\r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIaOYzMk3v1X"
      },
      "source": [
        "Implementen una funci칩n **`get_tokens()`** que reciba un texto y entregue una lista con sus tokens. Son libres de elegir la forma de tokenizar mientras no utilicen librer칤as con tokenizadores ya implementados. Pueden utilizar la librer칤a **re** importada para trabajar s칤mbolos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dl42-hgIhqB"
      },
      "source": [
        "Ejemplo de uso:\n",
        "\n",
        "`get_tokens('Este es un ejemplo de prueba.')` \n",
        "\n",
        "Nos entregar칤a:\n",
        "\n",
        "`['Este', 'es', 'un', 'ejemplo', 'de', 'prueba', '.']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF1RcIwq4G2x"
      },
      "source": [
        "def get_tokens(text):\n",
        "  ### Inicio del c칩digo ###\n",
        "  \n",
        "  output = []\n",
        "  regex = []\n",
        "\n",
        "  \"\"\"\n",
        "  Con expresiones regulares, trataremos de tokenizar:\n",
        "  - los numeros con parte decimal\n",
        "  - enteros sin separar sus digitos\n",
        "  - palabras\n",
        "  - simbolos como . , ? - entre otros\n",
        "  \"\"\"\n",
        "\n",
        "  ## decimal o entero\n",
        "  numbers = \"\\d+\\.?,?\\d+\"\n",
        "  regex.append(numbers)\n",
        "\n",
        "  word = \"[0-9]*[A-Za-z침치칠칤칩칰츼칄칈칍칔]+[0-9]*\"\n",
        "  regex.append(word)\n",
        "  \n",
        "  symbol = \"[.,\\?\\-_\\(\\)춿/]+\"\n",
        "  regex.append(symbol)\n",
        "\n",
        "  ## usar re.findall(pattern, string, flags=0)\n",
        "  ## primero, por cada regex\n",
        "  for item1 in regex:\n",
        "    found_words = re.findall(item1, text, flags=0)\n",
        "\n",
        "    ## se colocan las expresiones que hicieron match en el output\n",
        "    for item2 in found_words:\n",
        "      output.append(item2)\n",
        "  \n",
        "  return output\n",
        "  ### F칤n del c칩digo ###"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqlSpefv4_EH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6309d1a-9605-484c-cfd4-4da9f475da18"
      },
      "source": [
        "tokens = get_tokens(text)\n",
        "tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['05',\n",
              " '10',\n",
              " '2019',\n",
              " '61',\n",
              " '1,43',\n",
              " '53',\n",
              " '1.73',\n",
              " '1.34',\n",
              " 'Interconsulta',\n",
              " 'n',\n",
              " 'del',\n",
              " 'mes',\n",
              " 'ENFERMEDAD',\n",
              " 'RENAL',\n",
              " 'CR칍NICA',\n",
              " 'ETAPA',\n",
              " 'Fundamento',\n",
              " 'Cl칤nico',\n",
              " 'APS',\n",
              " 'PACIENTE',\n",
              " 'DE',\n",
              " 'A',\n",
              " 'OS',\n",
              " 'CON',\n",
              " 'ANTECEDENTES',\n",
              " 'DE',\n",
              " 'HRTA',\n",
              " 'DE',\n",
              " 'LARGA',\n",
              " 'DATA',\n",
              " 'ACUDE',\n",
              " 'A',\n",
              " 'CONTROL',\n",
              " 'PSCV',\n",
              " 'CON',\n",
              " 'EXAMNES',\n",
              " 'DONDE',\n",
              " 'DESTACA',\n",
              " 'CREATININA',\n",
              " 'DE',\n",
              " 'CON',\n",
              " 'UNA',\n",
              " 'VFG',\n",
              " 'CALCULADA',\n",
              " 'DE',\n",
              " 'ML',\n",
              " 'MIN',\n",
              " '73M2',\n",
              " 'CON',\n",
              " 'UNA',\n",
              " 'CREATININA',\n",
              " 'HACE',\n",
              " 'MAS',\n",
              " 'DE',\n",
              " 'MESES',\n",
              " 'DE',\n",
              " 'Enfermedad',\n",
              " 'renal',\n",
              " 'cronica',\n",
              " 'Etapa',\n",
              " 'SOLICITO',\n",
              " 'EVALUACION',\n",
              " 'DEL',\n",
              " 'CASO',\n",
              " '춿',\n",
              " '.-',\n",
              " '/',\n",
              " '/',\n",
              " '-',\n",
              " '.',\n",
              " '-',\n",
              " ',',\n",
              " '.',\n",
              " ',',\n",
              " ',',\n",
              " ',',\n",
              " '/',\n",
              " '/',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '(',\n",
              " ').']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPbgTvAW-stF"
      },
      "source": [
        "**Describa cu치les fueron sus supuestos para realizar la tokenizaci칩n y compare sus tokens con los entregados por la librer칤a nltk en el bloque de c칩digo de m치s abajo.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGQ7CJy3-3aH"
      },
      "source": [
        "    Describa aqu칤\n",
        "    Se supuso que pueden haber palabras con n칰meros al comienzo o al final de\n",
        "    ellas, como en el regex word del cuerpo de la funci칩n. Tambi칠n que las\n",
        "    fracciones siempre se escribir치n como n칰meros con puntos o comas entre la\n",
        "    parte entera y decimal. Los s칤mbolos especiales asociados son los escritos\n",
        "    en el regex symbol.\n",
        "\n",
        "    Al comparar los tokens con los de nltk, se puede notar que el algoritmo de\n",
        "    la libreria entrega los tokens en el orden en que aparecen en el texto,\n",
        "    contrario a lo que hace la funci칩n get_tokens() definida m치s arriba. Debe\n",
        "    tener m치s sentido sint치ctico, pero para efectos de una lista desordenada,\n",
        "    es muy parecido. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYtmAXTr9KXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c561200-e879-4145-80bb-66329b4874f2"
      },
      "source": [
        "from nltk.tokenize import wordpunct_tokenize # Nltk no puede ser utilizada para los siguientes ejercicio, es s칩lo un ejemplo\n",
        "nltk_tokens = wordpunct_tokenize(text)\n",
        "nltk_tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Interconsulta',\n",
              " 'n',\n",
              " '춿',\n",
              " '2',\n",
              " 'del',\n",
              " 'mes',\n",
              " '.-',\n",
              " '05',\n",
              " '/',\n",
              " '10',\n",
              " '/',\n",
              " '2019',\n",
              " '-',\n",
              " 'ENFERMEDAD',\n",
              " 'RENAL',\n",
              " 'CR칍NICA',\n",
              " 'ETAPA',\n",
              " '3',\n",
              " '.',\n",
              " '-',\n",
              " 'Fundamento',\n",
              " 'Cl칤nico',\n",
              " 'APS',\n",
              " ':',\n",
              " 'PACIENTE',\n",
              " 'DE',\n",
              " '61',\n",
              " 'A칌OS',\n",
              " ',',\n",
              " 'CON',\n",
              " 'ANTECEDENTES',\n",
              " 'DE',\n",
              " 'HRTA',\n",
              " 'DE',\n",
              " 'LARGA',\n",
              " 'DATA',\n",
              " '.',\n",
              " 'ACUDE',\n",
              " 'A',\n",
              " 'CONTROL',\n",
              " 'PSCV',\n",
              " 'CON',\n",
              " 'EXAMNES',\n",
              " ',',\n",
              " 'DONDE',\n",
              " 'DESTACA',\n",
              " 'CREATININA',\n",
              " 'DE',\n",
              " '1',\n",
              " ',',\n",
              " '43',\n",
              " ',',\n",
              " 'CON',\n",
              " 'UNA',\n",
              " 'VFG',\n",
              " 'CALCULADA',\n",
              " 'DE',\n",
              " '53',\n",
              " 'ML',\n",
              " '/',\n",
              " 'MIN',\n",
              " '/',\n",
              " '1',\n",
              " '.',\n",
              " '73M2',\n",
              " '.',\n",
              " 'CON',\n",
              " 'UNA',\n",
              " 'CREATININA',\n",
              " 'HACE',\n",
              " 'MAS',\n",
              " 'DE',\n",
              " '3',\n",
              " 'MESES',\n",
              " 'DE',\n",
              " '1',\n",
              " '.',\n",
              " '34',\n",
              " '.',\n",
              " 'Enfermedad',\n",
              " 'renal',\n",
              " 'cronica',\n",
              " 'Etapa',\n",
              " '3',\n",
              " '(',\n",
              " 'SOLICITO',\n",
              " 'EVALUACION',\n",
              " 'DEL',\n",
              " 'CASO',\n",
              " ').']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5QKlXAZwN1L"
      },
      "source": [
        "` `  \n",
        "` ` \n",
        "\n",
        "**Ejercicio 2 - *Stopwords y Stemming* (1 punto).** \n",
        "` `  \n",
        "` ` \n",
        "\n",
        "Considere el siguiente corpus:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEp83zESwb2j"
      },
      "source": [
        "dataset = [\"I like human languages\", \"I like programming languages\", \"Spanish is my favorite language\"]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmjdlJWuyS2E"
      },
      "source": [
        "Dise침e una funci칩n **`get_vocab()`** que extraiga los typos de este corpus solamente tokenizando. Puede utilizar la funci칩n creada por ustedes en el  Ejercicio 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UudC-b6TzZgw"
      },
      "source": [
        "def get_vocab(dataset):\n",
        "  ### Aqu칤 inicia tu c칩digo ###\n",
        "  \"\"\"\n",
        "  Primero vamos a tokenizar cada documento \n",
        "  y luego a침adir cada token a la lista output\n",
        "  \"\"\"\n",
        "  output = []\n",
        "\n",
        "  for var in dataset:\n",
        "    tokenized = get_tokens(var)\n",
        "    \n",
        "    for token in tokenized:\n",
        "      ## revisamos que no se repitan\n",
        "      if(token not in output):\n",
        "        output.append(token)\n",
        "\n",
        "  return output\n",
        "\n",
        "  ### Aqu칤 termina tu c칩digo ###"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-m32IoNmSwM"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNzPKiAx0Aa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65afaae-6407-45cc-f9cf-366b1af4df4d"
      },
      "source": [
        "vocab = get_vocab(dataset)\n",
        "vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'like',\n",
              " 'human',\n",
              " 'languages',\n",
              " 'programming',\n",
              " 'Spanish',\n",
              " 'is',\n",
              " 'my',\n",
              " 'favorite',\n",
              " 'language']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZLV2hWf9FN7"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "<table>\n",
        "    <tr> \n",
        "        <td>['favorite',\n",
        " 'Spanish',\n",
        " 'language',\n",
        " 'I',\n",
        " 'like',\n",
        " 'programming',\n",
        " 'languages',\n",
        " 'my',\n",
        " 'human',\n",
        " 'is'] </td> \n",
        "    </tr>\n",
        "</table> \n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3KB0fL2zk2v"
      },
      "source": [
        "Ahora dise침e reglas que usted estime convenientes tanto de **Stemming** como de **Stopwords**. Implemente una funci칩n que reciba una lista con los elementos del vocabulario, le aplique sus reglas y devuelva el vocabulario actualizado. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY7g67Ml0aby"
      },
      "source": [
        "    Explique sus reglas aqu칤\n",
        "\n",
        "    Primero, las stopwords se eliminan, compar치ndolas en base a una lista de stopwords gen칠rica en ingl칠s.\n",
        "\n",
        "    Segundo, se revisar치n las terminaciones de los strings (los sufijos de las \n",
        "    palabras), por ejemplo, en espa침ol la palabra \"perros\" deber칤a ser modificada\n",
        "    a \"perro\". Las palabras cuyos finales sean cortados pueden repetirse, por ejemplo \"perros\" y \"perritos\"\n",
        "    son cortadas a \"perro\", por lo que solamente se a침ade una vez al resultado final. \n",
        "\n",
        "    Se busca implementar una versi칩n m치s sencilla del algoritmo de Porter, con el fin de no crear\n",
        "    m치s funciones auxiliares, quedando la simplificaci칩n del proceso dentro de pre_processing().\n",
        "    No se modelar치 cada palabra en base a la cadena \\[C] (VM)^n \\[V], sino que solamente se revisar치 \n",
        "    el fin de cada string junto con las reglas, para luego modificarlo en caso de que as칤 lo indiquen.\n",
        "\n",
        "    Las reglas en cuesti칩n est치n definidas dentro del c칩digo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrKDtM_H0XlE"
      },
      "source": [
        "def pre_processing(vocab): \n",
        "  ### Aqu칤 inicia tu c칩digo ###\n",
        "  ## Lista de stopwords\n",
        "  stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \n",
        "               \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \n",
        "               \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \n",
        "               \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \n",
        "               \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \n",
        "               \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \n",
        "               \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \n",
        "               \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \n",
        "               \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \n",
        "               \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \n",
        "               \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \n",
        "               \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \n",
        "               \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
        "               \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \n",
        "               \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \n",
        "               \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \n",
        "               \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \n",
        "               \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \n",
        "               \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \n",
        "               \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \n",
        "               \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \n",
        "               \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \n",
        "               \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\", \n",
        "               \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \n",
        "               \"yourselves\"]\n",
        "\n",
        "  ## Se filtran las stopwords\n",
        "  for word in vocab:\n",
        "    ## se pasa a minuscula para comparar con la lista\n",
        "    if word.lower() in stopwords:\n",
        "      ## dado que cada palabra est치 una sola vez en vocab,\n",
        "      ## remove() es suficiente.\n",
        "      vocab.remove(word)  \n",
        "\n",
        "  output = []\n",
        "\n",
        "  ## Ahora se procesa la parte de las conjugaciones y sufijos, cortando los \n",
        "  ## finales de los strings para tratar de dejar solamente las raices.\n",
        "  for word in vocab:\n",
        "    if (word[-4:].lower() == \"sses\"):\n",
        "      word = word[0:-2]\n",
        "\n",
        "    if (word[-3:].lower() == \"ies\"):\n",
        "      word = word[0:-2]\n",
        "\n",
        "    if (word[-2:].lower() == \"ss\"):\n",
        "      word = word\n",
        "\n",
        "    if (word[-1:].lower() == \"s\"):\n",
        "      word = word[0:-1]\n",
        "\n",
        "    if (word[-3:].lower() == \"eed\"):\n",
        "      word = word[0:-1]\n",
        "\n",
        "    if (word[-2:].lower() == \"ed\"):\n",
        "      word = word[0:-2]\n",
        "\n",
        "    if (word[-3:].lower() == \"ing\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "\n",
        "    if (word[-1:].lower() == \"y\"):\n",
        "      word = word[0:-1] + \"i\"\n",
        "      \n",
        "    if (word[-7:].lower() == \"ational\"):\n",
        "      word = word[0:-7] + \"ate\"\n",
        "      \n",
        "    if (word[-6:].lower() == \"tional\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-4:].lower() == \"enci\"):\n",
        "      word = word[0:-1] + \"e\"\n",
        "      \n",
        "    if (word[-4:].lower() == \"anci\"):\n",
        "      word = word[0:-1] + \"e\"\n",
        "      \n",
        "    if (word[-4:].lower() == \"izer\"):\n",
        "      word = word[0:-1]\n",
        "      \n",
        "    if (word[-4:].lower() == \"abli\"):\n",
        "      word = word[0:-1] + \"e\"\n",
        "      \n",
        "    if (word[-4:].lower() == \"alli\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-4:].lower() == \"entli\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-3:].lower() == \"eli\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-5:].lower() == \"ousli\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-7:].lower() == \"ization\"):\n",
        "      word = word[0:-5] + \"e\"\n",
        "      \n",
        "    if (word[-5:].lower() == \"ation\"):\n",
        "      word = word[0:-3] + \"e\"\n",
        "      \n",
        "    if (word[-4:].lower() == \"ator\"):\n",
        "      word = word[0:-2] + \"e\"\n",
        "      \n",
        "    if (word[-5:].lower() == \"alism\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-7:].lower() == \"iveness\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-7:].lower() == \"fulness\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-7:].lower() == \"ousness\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-5:].lower() == \"aliti\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-5:].lower() == \"iviti\"):\n",
        "      word = word[0:-3] + \"e\"\n",
        "      \n",
        "    if (word[-6:].lower() == \"biliti\"):\n",
        "      word = word[0:-5] + \"le\"\n",
        "      \n",
        "\n",
        "    if (word[-5:].lower() == \"icate\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-5:].lower() == \"ative\"):\n",
        "      word = word[0:-5]\n",
        "      \n",
        "    if (word[-5:].lower() == \"alize\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-5:].lower() == \"iciti\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ical\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ful\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ness\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "\n",
        "    if (word[-2:].lower() == \"al\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ance\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ence\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-2:].lower() == \"er\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-2:].lower() == \"ic\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-4:].lower() == \"able\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ible\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ant\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-5:].lower() == \"ement\"):\n",
        "      word = word[0:-5]\n",
        "      \n",
        "    if (word[-4:].lower() == \"ment\"):\n",
        "      word = word[0:-4]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ent\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ion\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-2:].lower() == \"ou\"):\n",
        "      word = word[0:-2]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ism\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ate\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"iti\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ous\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ive\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "    if (word[-3:].lower() == \"ize\"):\n",
        "      word = word[0:-3]\n",
        "      \n",
        "\n",
        "    if (word[-1:].lower() == \"e\"):\n",
        "      word = word[0:-1]\n",
        "\n",
        "    output.append(word)\n",
        "      \n",
        "  return output\n",
        "  ### Aqu칤 termina tu c칩digo ###"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onOSuS-_mL2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec446615-fa1c-4a3b-9276-8af2d0b94f9c"
      },
      "source": [
        "vocab = pre_processing(vocab)\n",
        "vocab"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lik', 'human', 'languag', 'programm', 'Spanish', 'mi', 'favorit', 'languag']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65IwHx11uA75"
      },
      "source": [
        "` `  \n",
        "` ` \n",
        "\n",
        "**Ejercicio 3 - *Bag of Words* 游냤游낻(0.5 puntos).** \n",
        "` `  \n",
        "` ` \n",
        "\n",
        "Considere el siguiente corpus, donde cada elemento del arreglo representa un documento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh-utMuozhsK"
      },
      "source": [
        "d0 = 'El perro se come la comida y despu칠s se duerme'\n",
        "d1 = 'El perro se despierta y despu칠s empieza a ladrar'\n",
        "d2 = 'El perro ladra y despu칠s se come la comida'\n",
        "d3 = 'El gato se come la comida y despu칠s se duerme'\n",
        "d4 = 'El gato se despierta y despu칠s empieza a maullar'\n",
        "d5 = 'El gato maulla y despu칠s se come la comida'\n",
        "dataset = [d0, d1, d2, d3, d4, d5]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH7ne8C6ltvE"
      },
      "source": [
        "El objetivo de este ejercicio es determinar cu치les de  los documentos entregados son los m치s similares entre s칤. Para ello utilizaremos la t칠cnica TF-IDF. \n",
        "\n",
        "Como los algoritmos de Machine Learning no comprenden el texto en lenguaje natural, estos documentos deben ser convertidos a vectores num칠ricos. La representaci칩n m치s simple vista en clases es el **Bag of Words**, m칠todo mediante el cu치l se cuentan las apariciones de cada palabra en cada uno de los documentos entregados.\n",
        "\n",
        "Implemente la funci칩n **`bag_of_words()`**, que reciba como input un arreglo de documentos y devuelva un pandas dataframe con la representaci칩n Bag of Words de los documentos entregados. En esta representaci칩n las columnas son el vocabulario y las filas representa las apariciones de cada una de las palabras en los documentos. En otras palabras, cada fila representa el bow de un determinado documento.\n",
        "\n",
        "\n",
        "Por ejemplo para el siguiente dataset: \n",
        "\n",
        "```\n",
        "dataset = ['El perro ladra', 'El perro come']\n",
        "```\n",
        "\n",
        "Debiese entregarnos lo siguiente:\n",
        "\n",
        "\n",
        "|   | el | perro | ladra | come |\n",
        "|---|----|-------|------|-------|\n",
        "| 0 | 1  | 1     | 1    | 0     |\n",
        "| 1 | 1  | 1     | 0    | 1     |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXDMAyFmnq5j"
      },
      "source": [
        "def bag_of_words(dataset):\n",
        "  ### Aqu칤 inicia tu c칩digo ###\n",
        "  # Arreglo con los conteo de cada documento\n",
        "  data = []\n",
        "  types = []\n",
        "  doc_tokens_list =[]\n",
        "\n",
        "  # Se colocan todos los tipos en \"type\"\n",
        "  for document in dataset:\n",
        "    doc_tokens = get_tokens(document)\n",
        "    doc_tokens_list.append(doc_tokens)\n",
        "    for token in doc_tokens:\n",
        "      sub_data = []\n",
        "      if token not in types:\n",
        "        types.append(token)\n",
        "\n",
        "  # Ya encontrados los tipos se va viendo cuantas veces se \n",
        "  # encuentran en los documentos\n",
        "  for doc_tokens in doc_tokens_list:\n",
        "    sub_data = [0] * len(types)\n",
        "    for token in doc_tokens:\n",
        "      sub_data[types.index(token)] += 1\n",
        "    data.append(sub_data)\n",
        "\n",
        "  return pd.DataFrame(data, columns=types, \n",
        "                      index=[\"d0\", \"d1\", \"d2\", \"d3\", \"d4\", \"d5\"])\n",
        "  ### Aqu칤 termina tu c칩digo ###"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okfo-nEQmW1R"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Kk9GwCoDW8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "6d24c555-50b8-4cbe-ec13-2e4443dbfe71"
      },
      "source": [
        "dataset_bow = bag_of_words(dataset)\n",
        "dataset_bow"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>El</th>\n",
              "      <th>perro</th>\n",
              "      <th>se</th>\n",
              "      <th>come</th>\n",
              "      <th>la</th>\n",
              "      <th>comida</th>\n",
              "      <th>y</th>\n",
              "      <th>despu칠s</th>\n",
              "      <th>duerme</th>\n",
              "      <th>despierta</th>\n",
              "      <th>empieza</th>\n",
              "      <th>a</th>\n",
              "      <th>ladrar</th>\n",
              "      <th>ladra</th>\n",
              "      <th>gato</th>\n",
              "      <th>maullar</th>\n",
              "      <th>maulla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>d0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    El  perro  se  come  la  comida  ...  a  ladrar  ladra  gato  maullar  maulla\n",
              "d0   1      1   2     1   1       1  ...  0       0      0     0        0       0\n",
              "d1   1      1   1     0   0       0  ...  1       1      0     0        0       0\n",
              "d2   1      1   1     1   1       1  ...  0       0      1     0        0       0\n",
              "d3   1      0   2     1   1       1  ...  0       0      0     1        0       0\n",
              "d4   1      0   1     0   0       0  ...  1       0      0     1        1       0\n",
              "d5   1      0   1     1   1       1  ...  0       0      0     1        0       1\n",
              "\n",
              "[6 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeR5ADGz-MPa"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "|    | El | perro | se | come | la | comida | y | despu칠s | duerme | despierta | empieza | a | ladrar | ladra | gato | maullar | maulla |\n",
        "|----|---:|------:|---:|-----:|---:|-------:|--:|--------:|-------:|----------:|--------:|--:|-------:|------:|-----:|--------:|-------:|\n",
        "| d0 |  1 |     1 |  2 |    1 |  1 |      1 | 1 |       1 |      1 |         0 |       0 | 0 |      0 |     0 |    0 |       0 |      0 |\n",
        "| d1 |  1 |     1 |  1 |    0 |  0 |      0 | 1 |       1 |      0 |         1 |       1 | 1 |      1 |     0 |    0 |       0 |      0 |\n",
        "| d2 |  1 |     1 |  1 |    1 |  1 |      1 | 1 |       1 |      0 |         0 |       0 | 0 |      0 |     1 |    0 |       0 |      0 |\n",
        "| d3 |  1 |     0 |  2 |    1 |  1 |      1 | 1 |       1 |      1 |         0 |       0 | 0 |      0 |     0 |    1 |       0 |      0 |\n",
        "| d4 |  1 |     0 |  1 |    0 |  0 |      0 | 1 |       1 |      0 |         1 |       1 | 1 |      0 |     0 |    1 |       1 |      0 |\n",
        "| d5 |  1 |     0 |  1 |    1 |  1 |      1 | 1 |       1 |      0 |         0 |       0 | 0 |      0 |     0 |    1 |       0 |      1 |\n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4OXMz7opWcd"
      },
      "source": [
        "` `  \n",
        "` ` \n",
        "\n",
        "**Ejercicio 4 - *Calcular TF* (0.5 puntos):** Ahora debemos usar el dataframe del ejercicio anterior para calcular la matriz de TF normalizada por la m치xima frecuencia ${max_i({\\text{tf}_{i,j})}}$, donde\n",
        "i corresponde al 칤ndice de las filas (bow) y j al de las columnas (palabras). Es decir, dividir cada bow en la cantidad de veces de la palabra que aparezca m치s veces en ese vector. \n",
        "\n",
        "\n",
        "$$\\text{nft}_{i,j} = \\frac{\\text{tf}_{i,j}}{max_i({\\text{tf}_{i,j})}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGMAGcYooi6B",
        "outputId": "1e8f0fd3-d084-4aea-e3d0-46bf3bbf9ebd"
      },
      "source": [
        "max_i = dataset_bow.max(axis=1)[0]\n",
        "max_i"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWE16xhBpswc"
      },
      "source": [
        "def calc_tf(dataset_bow):\n",
        "    df = dataset_bow\n",
        "    ### Aqu칤 inicia tu c칩digo ###\n",
        "    # Se calcula el maximo de cada fila\n",
        "    max_i = df.max(axis=1)\n",
        "    # Se divide cada valor por el maximo de su fila\n",
        "    for i in range(df.shape[0]):\n",
        "      df.iloc[i] = df.iloc[i].apply(lambda x: x/max_i[i])\n",
        "    return df\n",
        "    ### Aqu칤 termina tu c칩digo ###"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZQPZe3JmYqy"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ2h8jEYp4nZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2dbd0a6e-7756-4685-a24e-1b6bdda82ae4"
      },
      "source": [
        "tf = calc_tf(dataset_bow)\n",
        "tf"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>El</th>\n",
              "      <th>perro</th>\n",
              "      <th>se</th>\n",
              "      <th>come</th>\n",
              "      <th>la</th>\n",
              "      <th>comida</th>\n",
              "      <th>y</th>\n",
              "      <th>despu칠s</th>\n",
              "      <th>duerme</th>\n",
              "      <th>despierta</th>\n",
              "      <th>empieza</th>\n",
              "      <th>a</th>\n",
              "      <th>ladrar</th>\n",
              "      <th>ladra</th>\n",
              "      <th>gato</th>\n",
              "      <th>maullar</th>\n",
              "      <th>maulla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>d0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d3</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     El  perro   se  come   la  ...  ladrar  ladra  gato  maullar  maulla\n",
              "d0  0.5    0.5  1.0   0.5  0.5  ...     0.0    0.0   0.0      0.0     0.0\n",
              "d1  1.0    1.0  1.0   0.0  0.0  ...     1.0    0.0   0.0      0.0     0.0\n",
              "d2  1.0    1.0  1.0   1.0  1.0  ...     0.0    1.0   0.0      0.0     0.0\n",
              "d3  0.5    0.0  1.0   0.5  0.5  ...     0.0    0.0   0.5      0.0     0.0\n",
              "d4  1.0    0.0  1.0   0.0  0.0  ...     0.0    0.0   1.0      1.0     0.0\n",
              "d5  1.0    0.0  1.0   1.0  1.0  ...     0.0    0.0   1.0      0.0     1.0\n",
              "\n",
              "[6 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuNXxe3pis7x",
        "outputId": "dfb6a1a3-92f2-412e-ff83-2958c60f7b82"
      },
      "source": [
        "list(tf.columns)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['El',\n",
              " 'perro',\n",
              " 'se',\n",
              " 'come',\n",
              " 'la',\n",
              " 'comida',\n",
              " 'y',\n",
              " 'despu칠s',\n",
              " 'duerme',\n",
              " 'despierta',\n",
              " 'empieza',\n",
              " 'a',\n",
              " 'ladrar',\n",
              " 'ladra',\n",
              " 'gato',\n",
              " 'maullar',\n",
              " 'maulla']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOzdRwx9_UMM"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "|    |  El | perro |  se | come |  la | comida |   y | despu칠s | duerme | despierta | empieza |   a | ladrar | ladra | gato | maullar | maulla |\n",
        "|----|----:|------:|----:|-----:|----:|-------:|----:|--------:|-------:|----------:|--------:|----:|-------:|------:|-----:|--------:|-------:|\n",
        "| d0 | 0.5 |   0.5 | 1.0 |  0.5 | 0.5 |    0.5 | 0.5 |     0.5 |    0.5 |       0.0 |     0.0 | 0.0 |    0.0 |   0.0 |  0.0 |     0.0 |    0.0 |\n",
        "| d1 | 1.0 |   1.0 | 1.0 |  0.0 | 0.0 |    0.0 | 1.0 |     1.0 |    0.0 |       1.0 |     1.0 | 1.0 |    1.0 |   0.0 |  0.0 |     0.0 |    0.0 |\n",
        "| d2 | 1.0 |   1.0 | 1.0 |  1.0 | 1.0 |    1.0 | 1.0 |     1.0 |    0.0 |       0.0 |     0.0 | 0.0 |    0.0 |   1.0 |  0.0 |     0.0 |    0.0 |\n",
        "| d3 | 0.5 |   0.0 | 1.0 |  0.5 | 0.5 |    0.5 | 0.5 |     0.5 |    0.5 |       0.0 |     0.0 | 0.0 |    0.0 |   0.0 |  0.5 |     0.0 |    0.0 |\n",
        "| d4 | 1.0 |   0.0 | 1.0 |  0.0 | 0.0 |    0.0 | 1.0 |     1.0 |    0.0 |       1.0 |     1.0 | 1.0 |    0.0 |   0.0 |  1.0 |     1.0 |    0.0 |\n",
        "| d5 | 1.0 |   0.0 | 1.0 |  1.0 | 1.0 |    1.0 | 1.0 |     1.0 |    0.0 |       0.0 |     0.0 | 0.0 |    0.0 |   0.0 |  1.0 |     0.0 |    1.0 |\n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqgW4Ni4t0xC"
      },
      "source": [
        "` `  \n",
        "` ` \n",
        "\n",
        "**Ejercicio 5 - *Calcular IDF* (0.5 punto):**\n",
        "\n",
        "\n",
        "Implementar `calc_idf(dataset_bow)`. Este debe retornar un diccionario en donde las llaves sean las palabras y los valores sean el calculo de cada idf por palabra.\n",
        "\n",
        "Recordar que $idf_{t_i} = log_{10}\\frac{N}{n_i}$ con $N = $ n칰mero de documentos y $n_i = $ N칰mero de documentos que contienen la palabra $t_i$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thhDY1-Ht6T5"
      },
      "source": [
        "def calc_idf(dataset_bow):\n",
        "    ### Aqu칤 inicia tu c칩digo ###\n",
        "\n",
        "    output = {}\n",
        "\n",
        "    ## 6 documentos originalmente\n",
        "    N = 6\n",
        "    \n",
        "    ## Recordar que dataset_bow es una matriz que indica\n",
        "    ## la cantidad de veces que cada palabra se repite en\n",
        "    ## cada documento. Cada columna tiene la palabra y las\n",
        "    ## filas representan la cantidad de veces que la palabra\n",
        "    ## aparece en cada documento.\n",
        "\n",
        "    ## Se itera sobre las columnas\n",
        "    for col_name in dataset_bow.columns[0:]:\n",
        "\n",
        "      ## Se crea un subset con las filas de valores > 0\n",
        "      docs = dataset_bow.loc[dataset_bow[col_name] > 0, col_name]\n",
        "      \n",
        "      ## Se revisa el tama침o del objeto\n",
        "      n_i = docs.shape[0]\n",
        "      frac = float(N/n_i)\n",
        "\n",
        "      idf_ti = np.log10(frac)\n",
        "\n",
        "      output[col_name] = idf_ti\n",
        "\n",
        "    return output\n",
        "    ### Aqu칤 termina tu c칩digo ###"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR_j3pYemcAc"
      },
      "source": [
        "***Test:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro-OMGpduC0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e07dbc6-ebfb-442f-f9f5-d665b53c3b4c"
      },
      "source": [
        "idf = calc_idf(dataset_bow)\n",
        "idf"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'El': 0.0,\n",
              " 'a': 0.47712125471966244,\n",
              " 'come': 0.17609125905568124,\n",
              " 'comida': 0.17609125905568124,\n",
              " 'despierta': 0.47712125471966244,\n",
              " 'despu칠s': 0.0,\n",
              " 'duerme': 0.47712125471966244,\n",
              " 'empieza': 0.47712125471966244,\n",
              " 'gato': 0.3010299956639812,\n",
              " 'la': 0.17609125905568124,\n",
              " 'ladra': 0.7781512503836436,\n",
              " 'ladrar': 0.7781512503836436,\n",
              " 'maulla': 0.7781512503836436,\n",
              " 'maullar': 0.7781512503836436,\n",
              " 'perro': 0.3010299956639812,\n",
              " 'se': 0.0,\n",
              " 'y': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwjMAUUJ_i17"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "<table>\n",
        "    <tr> \n",
        "        <td> {'El': 0.0,\n",
        " 'a': 0.47712125471966244,\n",
        " 'come': 0.17609125905568124,\n",
        " 'comida': 0.17609125905568124,\n",
        " 'despierta': 0.47712125471966244,\n",
        " 'despu칠s': 0.0,\n",
        " 'duerme': 0.47712125471966244,\n",
        " 'empieza': 0.47712125471966244,\n",
        " 'gato': 0.3010299956639812,\n",
        " 'la': 0.17609125905568124,\n",
        " 'ladra': 0.7781512503836436,\n",
        " 'ladrar': 0.7781512503836436,\n",
        " 'maulla': 0.7781512503836436,\n",
        " 'maullar': 0.7781512503836436,\n",
        " 'perro': 0.3010299956639812,\n",
        " 'se': 0.0,\n",
        " 'y': 0.0} </td> \n",
        "    </tr>\n",
        "</table> \n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzKAzJtSJ7gx"
      },
      "source": [
        "Puede notar el bajo puntaje otorgado a las palabras que m치s se repiten! 游땵"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D17lm6l9uJPo"
      },
      "source": [
        "**Ejercicio 6 - *Calcular TF-IDF & concluir similitud de documentos.* (1 punto)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc7FTQ19Kcwo"
      },
      "source": [
        "Programe la funci칩n `calc_tf_idf(tf, idf)` que entrega el dataframe TF-IDF asociado al dataset que estamos analizando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9knMl0KguMwo"
      },
      "source": [
        "def calc_tf_idf(tf, idf):\n",
        "    ### Aqu칤 inicia tu c칩digo ###\n",
        "    # Hacemos una copia para modificar\n",
        "    df = tf\n",
        "    columns = list(df.columns)\n",
        "    # A cada columna se aplica la funcion TF-IDF\n",
        "    for column in columns:\n",
        "      df[column] = df[column].apply(lambda x: x*idf.get(column))\n",
        "    return df\n",
        "    ### Aqu칤 termina tu c칩digo ### "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRzIr1nQmepp"
      },
      "source": [
        "***Test.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8z6jaq2uPEo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c712fa02-c858-488c-a66f-65eb62f81e00"
      },
      "source": [
        "tf_idf = calc_tf_idf(tf, idf)\n",
        "tf_idf"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>El</th>\n",
              "      <th>perro</th>\n",
              "      <th>se</th>\n",
              "      <th>come</th>\n",
              "      <th>la</th>\n",
              "      <th>comida</th>\n",
              "      <th>y</th>\n",
              "      <th>despu칠s</th>\n",
              "      <th>duerme</th>\n",
              "      <th>despierta</th>\n",
              "      <th>empieza</th>\n",
              "      <th>a</th>\n",
              "      <th>ladrar</th>\n",
              "      <th>ladra</th>\n",
              "      <th>gato</th>\n",
              "      <th>maullar</th>\n",
              "      <th>maulla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>d0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150515</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.238561</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.088046</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.238561</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.150515</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.301030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.778151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     El     perro   se      come  ...     ladra      gato   maullar    maulla\n",
              "d0  0.0  0.150515  0.0  0.088046  ...  0.000000  0.000000  0.000000  0.000000\n",
              "d1  0.0  0.301030  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000\n",
              "d2  0.0  0.301030  0.0  0.176091  ...  0.778151  0.000000  0.000000  0.000000\n",
              "d3  0.0  0.000000  0.0  0.088046  ...  0.000000  0.150515  0.000000  0.000000\n",
              "d4  0.0  0.000000  0.0  0.000000  ...  0.000000  0.301030  0.778151  0.000000\n",
              "d5  0.0  0.000000  0.0  0.176091  ...  0.000000  0.301030  0.000000  0.778151\n",
              "\n",
              "[6 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBG2qfwv_6HK"
      },
      "source": [
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "|    |  El |    perro |  se |     come |       la |   comida |   y | despu칠s |   duerme | despierta |  empieza |        a |   ladrar |    ladra |     gato |  maullar |   maulla |\n",
        "|----|----:|---------:|----:|---------:|---------:|---------:|----:|--------:|---------:|----------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|\n",
        "| d0 | 0.0 | 0.150515 | 0.0 | 0.088046 | 0.088046 | 0.088046 | 0.0 |     0.0 | 0.238561 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 |\n",
        "| d1 | 0.0 | 0.301030 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 |     0.0 | 0.000000 |  0.477121 | 0.477121 | 0.477121 | 0.778151 | 0.000000 | 0.000000 | 0.000000 | 0.000000 |\n",
        "| d2 | 0.0 | 0.301030 | 0.0 | 0.176091 | 0.176091 | 0.176091 | 0.0 |     0.0 | 0.000000 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.778151 | 0.000000 | 0.000000 | 0.000000 |\n",
        "| d3 | 0.0 | 0.000000 | 0.0 | 0.088046 | 0.088046 | 0.088046 | 0.0 |     0.0 | 0.238561 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.150515 | 0.000000 | 0.000000 |\n",
        "| d4 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 |     0.0 | 0.000000 |  0.477121 | 0.477121 | 0.477121 | 0.000000 | 0.000000 | 0.301030 | 0.778151 | 0.000000 |\n",
        "| d5 | 0.0 | 0.000000 | 0.0 | 0.176091 | 0.176091 | 0.176091 | 0.0 |     0.0 | 0.000000 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.301030 | 0.000000 | 0.778151 |\n",
        "\n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmVlbpzMp5NU"
      },
      "source": [
        "Ahora que tenemos el dataframe de TF-IDF, nos queda calcular la similitud coseno entre todos los vectores. Notar que la matriz resultante ser치 una matriz sim칠trica. Implemente la funci칩n *cosine_similarity(v1, v2)* que recibe dos vectores (v1 y v2) y calcula la similitud coseno entre ambos vectores. Concluya cu치les son los dos documentos m치s similares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgUtgBkQAae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20082845-7311-41e9-ec52-6eca8d29c5c4"
      },
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "  ### Aqu칤 inicia tu c칩digo ###\n",
        "  # Se calcula el producto punto\n",
        "  a = np.dot(v1,v2)\n",
        "  # Se calcula el modulo de cada vector y se multiplica\n",
        "  b = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "  # Finalemente se divide ambos resultados anteriores\n",
        "  return a/b\n",
        "  ### Aqu칤 termina tu c칩digo ### \n",
        "\n",
        "similarity_matrix = np.zeros((6,6))\n",
        "for i, v1 in enumerate(tf_idf.index.values):\n",
        "  for j, v2 in enumerate(tf_idf.index.values):\n",
        "      similarity = cosine_similarity(tf_idf.loc[v1].values, tf_idf.loc[v2].values)\n",
        "      similarity_matrix[i][j] = similarity\n",
        "print(similarity_matrix)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.12032418 0.3223436  0.77967014 0.         0.1632828 ]\n",
            " [0.12032418 1.         0.08686457 0.         0.4952126  0.        ]\n",
            " [0.3223436  0.08686457 1.         0.1632828  0.         0.11787732]\n",
            " [0.77967014 0.         0.1632828  1.         0.12032418 0.3223436 ]\n",
            " [0.         0.4952126  0.         0.12032418 1.         0.08686457]\n",
            " [0.1632828  0.         0.11787732 0.3223436  0.08686457 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FlltqIqRxf"
      },
      "source": [
        "    Escriba su respuesta aqu칤\n",
        "\n",
        "    El documento d0 y el documento d3 son los mas similares con una simulitud de\n",
        "    0.78 aproximadamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUAc1zX0Lg16"
      },
      "source": [
        "![gato](https://www.elagoradiario.com/wp-content/uploads/2020/05/Gato-mascarilla.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1A95IaXLHaB"
      },
      "source": [
        "**Cualquier recomendaci칩n que nos quisieran dar para una futura tarea es bienvenid@!**"
      ]
    }
  ]
}